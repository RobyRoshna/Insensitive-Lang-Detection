{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1X-_qh-lFamOLgxc81uS27YThGnivdz3A",
      "authorship_tag": "ABX9TyPPPRe/VswDMP4gbmdMFZFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobyRoshna/Insensitive-Lang-Detection/blob/main/BERTtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT training for Identifying Insensitive Language about Disabled People Using Semantic Analysis and Machine Learning\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "1. [Imports](#imports)\n",
        "2. [Data Splitting: Train, Test, and Validation (80:10:10)](#split-to-train-test-and-validation)\n",
        "3. [Tokenizer](#tokenizer)\n",
        "4. [Dataset Creation for BERT](#dataset-Creation-for-BERT)\n",
        "5. [Max Sentence Length for Tokenizer](#max-Sentence-Length)\n",
        "6. [PyTorch Dataloader for Future Extension](#pytorch-dataloader-for-future-extension)\n",
        "7. [Dataset Check](#dataset-check)\n",
        "8. [BERT Base Model](#bert-base-model)\n"
      ],
      "metadata": {
        "id": "J5TwaJxAzLN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Imports** <a name=\"imports\"></a>"
      ],
      "metadata": {
        "id": "RVjGNSqGtcho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "login(token='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')\n",
        "import wandb\n",
        "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
        "import pandas as pd\n",
        "import torch # Only if extending or customizing the trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ],
      "metadata": {
        "id": "89Kb85TtrYom"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Split to Train, Test, and Validation (80:10:10)** <a name=\"split-to-train-test-and-validation\"></a>"
      ],
      "metadata": {
        "id": "XkmEfqqgrYTU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_3pOckwqEyr",
        "outputId": "3d728671-cc51-46d3-f941-3f0954e2424d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 870, Validation size: 109, Test size: 109\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The annotated dataset\n",
        "file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/Abstract_annotations.xlsx'  # Update with your path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# cleaning data\n",
        "data = data[['Sentence', 'Manual_Annotation']]\n",
        "data = data.dropna()\n",
        "\n",
        "# 1 for insensitive and 0 for notInsensitive\n",
        "data['Manual_Annotation'] = data['Manual_Annotation'].apply(lambda x: 1 if x.lower() == 'insensitive' else 0)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['Manual_Annotation'])\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Manual_Annotation'])\n",
        "\n",
        "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Tokenizer** <a name=\"tokenizer\"></a>"
      ],
      "metadata": {
        "id": "DTmQaqxJ4JWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize data\n",
        "def tokenize_data(data, tokenizer, max_length=109):\n",
        "    return tokenizer(\n",
        "        list(data['Sentence']),  # Tokenize sentences\n",
        "        padding=True,            # Pad shorter sentences\n",
        "        truncation=True,         # Truncate longer sentences\n",
        "        max_length=max_length,   # Max token length\n",
        "        return_tensors='pt'      # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "train_labels = list(train_data['Manual_Annotation'])\n",
        "val_labels = list(val_data['Manual_Annotation'])\n",
        "test_labels = list(test_data['Manual_Annotation'])\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenize_data(train_data, tokenizer)\n",
        "val_encodings = tokenize_data(val_data, tokenizer)\n",
        "test_encodings = tokenize_data(test_data, tokenizer)\n"
      ],
      "metadata": {
        "id": "p9tvFGJi4Hwm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Dataset Creation for BERT** <a name=\"dataset-creation-for-bert\"></a>"
      ],
      "metadata": {
        "id": "91jA6NoV4ymq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for Tokenized Data\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            encodings: Dictionary containing tokenized input IDs, attention masks, etc.\n",
        "            labels: List of labels corresponding to the sentences (e.g., 0 for NotInsensitive, 1 for Insensitive).\n",
        "        \"\"\"\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the tokenized inputs and the corresponding label for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the data sample.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the tokenized inputs (input IDs, attention masks, etc.)\n",
        "            and the label for the specified index.\n",
        "        \"\"\"\n",
        "        # Convert tokenized data for the index to PyTorch tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])  # Add the corresponding label\n",
        "        return item\n",
        "# Create datasets for train, validation, and test sets\n",
        "train_dataset = SentenceDataset(train_encodings, train_labels)\n",
        "val_dataset = SentenceDataset(val_encodings, val_labels)\n",
        "test_dataset = SentenceDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "5OalPCYK4zJf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Max Sentence Length for Tokenizer** <a name=\"max-Sentence-Length\"></a>"
      ],
      "metadata": {
        "id": "FVCLKee9xFxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_lengths = [len(tokenizer.tokenize(sent)) for sent in train_data['Sentence']]\n",
        "print(f\"Max length: {max(sentence_lengths)}\")\n",
        "print(f\"Average length: {sum(sentence_lengths)/len(sentence_lengths)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBplqkCeejHL",
        "outputId": "6bc977ae-c6f2-4a5b-8833-d9d6584ea4e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length: 109\n",
            "Average length: 32.96206896551724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. PyTorch Dataloader for Future Extension** <a name=\"pytorch-dataloader-for-future-extension\"></a>"
      ],
      "metadata": {
        "id": "ge44_pt0EXjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n"
      ],
      "metadata": {
        "id": "mdJ-EUXHEYOP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Dataset Check** <a name=\"dataset-check\"></a>"
      ],
      "metadata": {
        "id": "tCakRDO1xiRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Examples from the training dataset\n",
        "for i in range(5):\n",
        "    item = train_dataset[i]\n",
        "    print(\"Input IDs:\", item['input_ids'])\n",
        "    print(\"Attention Mask:\", item['attention_mask'])\n",
        "    print(\"Label:\", item['labels'])  # 0 for Not Insensitive, 1 for Insensitive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qc2mosXdE5I",
        "outputId": "e9e096e5-60a7-4c89-c990-4fad7805f784"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([  101,  2122,  2913,  1998,  3141,  3906,  6592,  4022,  2005,  2925,\n",
            "        27758,  2015,  1997,  1996,  2291,  2000,  5770,  6397,  1998, 17453,\n",
            "        18234,  5198,  1999,  4547,  1010,  2658,  1010,  1998, 10517, 18046,\n",
            "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label: tensor(1)\n",
            "Input IDs: tensor([  101,  1996,  4118,  2003,  5415,  1011,  2881,  2000,  9585,  2025,\n",
            "         2069,  1996, 17453, 18234,  2111,  2021,  2036,  1996,  2512,  1011,\n",
            "        18234,  2111,  2000,  5959,  2478,  9123,  3617,  4949,  8417,  2362,\n",
            "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label: tensor(1)\n",
            "Input IDs: tensor([  101,  2256,  8993,  6086,  1037,  2342,  2005,  3247,  5906,  2000,\n",
            "         2393, 19294,  2015,  2191,  3350,  1011,  2241,  6567,  2055,  3293,\n",
            "         2399,  1025,  1045,  1012,  1041,  1012,  2000,  2393,  2068,  5454,\n",
            "         2399,  2008,  2674,  5219,  3289,  1998,  5776, 17879,  1012,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label: tensor(0)\n",
            "Input IDs: tensor([  101,  2023,  2147,  5577,  5876,  2008,  7461,  9164,  2076,  7954,\n",
            "         7547,  2011,  2796, 17453, 18234,  5381,  1010,  1998,  1996,  2536,\n",
            "         9942,  2109,  2000, 10210, 28731,  7860,  4320,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label: tensor(1)\n",
            "Input IDs: tensor([  101,  2053, 11287,  2024,  2109,  1011,  1011,  2069,  3722,  1010,\n",
            "         3733,  1011,  2000,  1011,  3342, 18327,  1012,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Label: tensor(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. BERT Base Model** <a name=\"bert-base-model\"></a>"
      ],
      "metadata": {
        "id": "soYMpuWxExCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT for binary classification\n",
        "modelBbase = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQAlI3mxExgX",
        "outputId": "424ccc50-629c-4b3e-f97b-d32009e768e7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics Function"
      ],
      "metadata": {
        "id": "f3XUhkycglAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute metrics\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    preds = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "Dk3oUnkugl5Y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert-base Training"
      ],
      "metadata": {
        "id": "1pdzGCEuyXfg"
      }
    },
    {
      "source": [
        "# Safe close any previous WandB session\n",
        "wandb.finish()\n",
        "\n",
        "# Initialize WandB with a specific run name\n",
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_base\")\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=[\"wandb\"],  # WandB is used for logging\n",
        "    run_name=\"Bert_base\"  # the run name for this Trainer\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=modelBbase,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
        "wandb.log(results)\n",
        "\n",
        "wandb.finish()  # Close the evaluation session\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yWI1i-HcNyef",
        "outputId": "f1348aa8-47bc-4399-bc23-97adbc367a08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▅█</td></tr><tr><td>train/global_step</td><td>▁▅█</td></tr><tr><td>train/grad_norm</td><td>█▁▄</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.54545</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>11.64348</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.3679</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Bert_large</strong> at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/dj3jcpbd' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/dj3jcpbd</a><br> View project at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250102_222856-dj3jcpbd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250102_223307-p19y8p1b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/p19y8p1b' target=\"_blank\">Bert_base</a></strong> to <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/p19y8p1b' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/p19y8p1b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [165/165 01:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.190100</td>\n",
              "      <td>0.169704</td>\n",
              "      <td>0.944954</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.907407</td>\n",
              "      <td>0.942308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.073600</td>\n",
              "      <td>0.099539</td>\n",
              "      <td>0.972477</td>\n",
              "      <td>0.947368</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.972973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.067100</td>\n",
              "      <td>0.056631</td>\n",
              "      <td>0.990826</td>\n",
              "      <td>0.981818</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁▅█</td></tr><tr><td>eval/f1</td><td>▁▅█</td></tr><tr><td>eval/loss</td><td>█▄▁</td></tr><tr><td>eval/precision</td><td>█▁█</td></tr><tr><td>eval/recall</td><td>▁██</td></tr><tr><td>eval/runtime</td><td>▅▁█</td></tr><tr><td>eval/samples_per_second</td><td>▃█▁</td></tr><tr><td>eval/steps_per_second</td><td>▃█▁</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>test_runtime</td><td>▁</td></tr><tr><td>test_samples_per_second</td><td>▁</td></tr><tr><td>test_steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▄▃▃▂▂▇▂▃▃▁▁▁▁█▁▄</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▃▂▂▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>eval/accuracy</td><td>0.99083</td></tr><tr><td>eval/f1</td><td>0.99083</td></tr><tr><td>eval/loss</td><td>0.05663</td></tr><tr><td>eval/precision</td><td>0.98182</td></tr><tr><td>eval/recall</td><td>1</td></tr><tr><td>eval/runtime</td><td>0.6</td></tr><tr><td>eval/samples_per_second</td><td>181.681</td></tr><tr><td>eval/steps_per_second</td><td>11.668</td></tr><tr><td>test/accuracy</td><td>0.95413</td></tr><tr><td>test/f1</td><td>0.95238</td></tr><tr><td>test/loss</td><td>0.18113</td></tr><tr><td>test/precision</td><td>0.98039</td></tr><tr><td>test/recall</td><td>0.92593</td></tr><tr><td>test/runtime</td><td>0.4861</td></tr><tr><td>test/samples_per_second</td><td>224.252</td></tr><tr><td>test/steps_per_second</td><td>14.401</td></tr><tr><td>test_accuracy</td><td>0.95413</td></tr><tr><td>test_f1</td><td>0.95238</td></tr><tr><td>test_loss</td><td>0.18113</td></tr><tr><td>test_precision</td><td>0.98039</td></tr><tr><td>test_recall</td><td>0.92593</td></tr><tr><td>test_runtime</td><td>0.4861</td></tr><tr><td>test_samples_per_second</td><td>224.252</td></tr><tr><td>test_steps_per_second</td><td>14.401</td></tr><tr><td>total_flos</td><td>146196219022200.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>165</td></tr><tr><td>train/grad_norm</td><td>4.39783</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0671</td></tr><tr><td>train_loss</td><td>0.18364</td></tr><tr><td>train_runtime</td><td>117.661</td></tr><tr><td>train_samples_per_second</td><td>22.182</td></tr><tr><td>train_steps_per_second</td><td>1.402</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Bert_base</strong> at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/p19y8p1b' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/p19y8p1b</a><br> View project at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250102_223307-p19y8p1b/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and tokenizer\n",
        "save_directory = \"/content/drive/MyDrive/Honours MiscData(Roshna)/bert_base_model\"\n",
        "trainer.save_model(save_directory)  # Save model\n",
        "tokenizer.save_pretrained(save_directory)  # Save tokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI852U20HrJA",
        "outputId": "1b7da30f-93c3-44fb-f47a-1f195890a2b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Honours MiscData(Roshna)/bert_base_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Honours MiscData(Roshna)/bert_base_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Honours MiscData(Roshna)/bert_base_model/vocab.txt',\n",
              " '/content/drive/MyDrive/Honours MiscData(Roshna)/bert_base_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. BERT Large Model** <a name=\"bert-large-model\"></a>"
      ],
      "metadata": {
        "id": "A_PDQY7JAcp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safe close any previous WandB session\n",
        "wandb.finish()\n",
        "\n",
        "# Initialize WandB with a specific run name for BERT Large\n",
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_large\")\n",
        "\n",
        "# Load pre-trained BERT Large for sequence classification\n",
        "modelBlarge = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-large-uncased',  # BERT Large pre-trained model\n",
        "    num_labels=2  # Binary classification: Insensitive and Not-Insensitive\n",
        ")\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_large',\n",
        "    num_train_epochs=3,  # Adjust if more epochs are required\n",
        "    per_device_train_batch_size=16,  # Batch size\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',  # Evaluate at the end of each epoch\n",
        "    save_strategy='epoch',  # Save checkpoints at each epoch\n",
        "    logging_dir='./logs_large',  # Logging directory\n",
        "    logging_steps=10,\n",
        "    report_to=[\"wandb\"],  # WandB logging\n",
        "    run_name=\"Bert_large\"  # Run name\n",
        ")\n",
        "\n",
        "# Trainer setup for BERT Large\n",
        "trainer = Trainer(\n",
        "    model=modelBlarge,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Your tokenized training dataset\n",
        "    eval_dataset=val_dataset,  # Your tokenized validation dataset\n",
        "    compute_metrics=compute_metrics,  # Custom metrics function\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate on the test dataset\n",
        "results = trainer.evaluate(test_dataset, metric_key_prefix=\"test\")\n",
        "\n",
        "# Log the results to WandB\n",
        "wandb.log(results)\n",
        "\n",
        "# Close the WandB session\n",
        "wandb.finish()\n",
        "trainer.save_model(\"/content/drive/MyDrive/Honours MiscData(Roshna)/bert_large_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Honours MiscData(Roshna)/bert_large_model\")\n",
        "# Print evaluation results\n",
        "print(\"Test Results:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "415wMM-HAdHn",
        "outputId": "74844d90-0234-4b28-dedc-672bea8f9342"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250102_223607-bxznz4o2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/bxznz4o2' target=\"_blank\">Bert_large</a></strong> to <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/bxznz4o2' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/bxznz4o2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [165/165 04:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.256100</td>\n",
              "      <td>0.143217</td>\n",
              "      <td>0.963303</td>\n",
              "      <td>0.980769</td>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.962264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.114900</td>\n",
              "      <td>0.016767</td>\n",
              "      <td>0.990826</td>\n",
              "      <td>0.981818</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.990826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.067200</td>\n",
              "      <td>0.055827</td>\n",
              "      <td>0.981651</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.981818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-20-9eedf51cf1d1>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval/accuracy</td><td>▁█▆</td></tr><tr><td>eval/f1</td><td>▁█▆</td></tr><tr><td>eval/loss</td><td>█▁▃</td></tr><tr><td>eval/precision</td><td>██▁</td></tr><tr><td>eval/recall</td><td>▁██</td></tr><tr><td>eval/runtime</td><td>█▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃█</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>test_runtime</td><td>▁</td></tr><tr><td>test_samples_per_second</td><td>▁</td></tr><tr><td>test_steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▅▂▃▂▂▃▁▂▄▂▁▁▂▃▄█</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▃▂▂▂▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>eval/accuracy</td><td>0.98165</td></tr><tr><td>eval/f1</td><td>0.98182</td></tr><tr><td>eval/loss</td><td>0.05583</td></tr><tr><td>eval/precision</td><td>0.96429</td></tr><tr><td>eval/recall</td><td>1</td></tr><tr><td>eval/runtime</td><td>1.7704</td></tr><tr><td>eval/samples_per_second</td><td>61.567</td></tr><tr><td>eval/steps_per_second</td><td>3.954</td></tr><tr><td>test/accuracy</td><td>0.93578</td></tr><tr><td>test/f1</td><td>0.93458</td></tr><tr><td>test/loss</td><td>0.25178</td></tr><tr><td>test/precision</td><td>0.9434</td></tr><tr><td>test/recall</td><td>0.92593</td></tr><tr><td>test/runtime</td><td>1.5447</td></tr><tr><td>test/samples_per_second</td><td>70.563</td></tr><tr><td>test/steps_per_second</td><td>4.532</td></tr><tr><td>test_accuracy</td><td>0.93578</td></tr><tr><td>test_f1</td><td>0.93458</td></tr><tr><td>test_loss</td><td>0.25178</td></tr><tr><td>test_precision</td><td>0.9434</td></tr><tr><td>test_recall</td><td>0.92593</td></tr><tr><td>test_runtime</td><td>1.5447</td></tr><tr><td>test_samples_per_second</td><td>70.563</td></tr><tr><td>test_steps_per_second</td><td>4.532</td></tr><tr><td>total_flos</td><td>517822565533560.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>165</td></tr><tr><td>train/grad_norm</td><td>37.37182</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0672</td></tr><tr><td>train_loss</td><td>0.21658</td></tr><tr><td>train_runtime</td><td>295.5188</td></tr><tr><td>train_samples_per_second</td><td>8.832</td></tr><tr><td>train_steps_per_second</td><td>0.558</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Bert_large</strong> at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/bxznz4o2' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton/runs/bxznz4o2</a><br> View project at: <a href='https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton' target=\"_blank\">https://wandb.ai/Roshna/Insensitive%20Lang%20Detecton</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250102_223607-bxznz4o2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results: {'test_loss': 0.25178229808807373, 'test_accuracy': 0.9357798165137615, 'test_precision': 0.9433962264150944, 'test_recall': 0.9259259259259259, 'test_f1': 0.9345794392523364, 'test_runtime': 1.5447, 'test_samples_per_second': 70.563, 'test_steps_per_second': 4.532, 'epoch': 3.0}\n"
          ]
        }
      ]
    }
  ]
}