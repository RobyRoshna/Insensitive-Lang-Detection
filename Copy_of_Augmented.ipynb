{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1tCkM7NFn0AJSg2U_rozAPdIAcp7eZeoQ",
      "authorship_tag": "ABX9TyPowh7zucMY3snqJb7e3C9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobyRoshna/Insensitive-Lang-Detection/blob/Augmentation/Copy_of_Augmented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Imports** <a name=\"imports\"></a>"
      ],
      "metadata": {
        "id": "A-RJYklFel6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr7QMQ6EzzdD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, set_seed\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import pickle\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: random_state for splits need to set separately for this study its 42 throughout\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "yfT8aDKyz7Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset and metrics class\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMjCj1pce1dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for Tokenized Data\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: The number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the tokenized inputs and the corresponding label for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the data sample.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the tokenized inputs (input IDs, attention masks, etc.)\n",
        "            and the label for the specified index.\n",
        "        \"\"\"\n",
        "        # Convert tokenized data for the index to PyTorch tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Ensure the label key is 'labels'\n",
        "        item['labels'] = torch.tensor(self.labels[idx])  # Add the corresponding label\n",
        "        return item\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    preds = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "oB9vz0-rz-kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure GPT on unaug"
      ],
      "metadata": {
        "id": "H7D-YbPC6BWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "test_dataexp = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\")\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTbert_base_model\"\n",
        "modelAug = BertForSequenceClassification.from_pretrained(model_path)\n",
        "modelAug.eval()\n",
        "\n",
        "print(\"\\nLoaded Model Path:\", model_path)\n",
        "\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detection\", entity=\"Roshna\", name=\"Bert_baseOripredGPT\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_dataexp['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = modelAug(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "test_predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "test_true_labels = test_dataexp['Manual_Annotation'].values\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "class_report = classification_report(test_true_labels, test_predictions, output_dict=True)\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n",
        "\n",
        "# Extract Precision, Recall, and F1-score - macro average gives equal importance to both classes in binary classification\n",
        "precision = class_report[\"macro avg\"][\"precision\"]\n",
        "recall = class_report[\"macro avg\"][\"recall\"]\n",
        "f1 = class_report[\"macro avg\"][\"f1-score\"]\n",
        "\n",
        "wandb.log({\n",
        "    \"test_accuracy\": accuracy,\n",
        "    \"test_precision\": precision,\n",
        "    \"test_recall\": recall,\n",
        "    \"test_f1\": f1\n",
        "})\n",
        "\n",
        "\n",
        "print(\"\\nTest Set Evaluation\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                              columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                              index=[\"Actual NotInsensitive\", \"Actual Insensitive\"])\n",
        "\n",
        "print(\"\\nConfusion Matrix - Test Set:\")\n",
        "print(conf_matrix_df)\n",
        "\n",
        "wandb.log({\"confusion_matrix\": wandb.Table(dataframe=conf_matrix_df)})\n",
        "\n",
        "# Analyze false positives & false negatives\n",
        "false_positives = test_dataexp[(test_true_labels == 0) & (test_predictions == 1)].copy()\n",
        "false_negatives = test_dataexp[(test_true_labels == 1) & (test_predictions == 0)].copy()\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "print(false_positives[['Sentence', 'Manual_Annotation']])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "print(false_negatives[['Sentence', 'Manual_Annotation']])\n",
        "\n",
        "# Confidence scores\n",
        "test_dataexp['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_dataexp['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "\n",
        "# Assign confidence values to misclassified examples\n",
        "false_positives['Confidence_Positive'] = test_dataexp.loc[false_positives.index, 'Confidence_Positive'].values\n",
        "false_negatives['Confidence_Negative'] = test_dataexp.loc[false_negatives.index, 'Confidence_Negative'].values\n",
        "\n",
        "print(\"\\nFalse Positives with Confidence:\")\n",
        "print(false_positives[['Sentence', 'Manual_Annotation', 'Confidence_Positive']])\n",
        "\n",
        "print(\"\\nFalse Negatives with Confidence:\")\n",
        "print(false_negatives[['Sentence', 'Manual_Annotation', 'Confidence_Negative']])\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"NotInsensitive\", \"Insensitive\"],\n",
        "            yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"OriginalTestset.png\")\n",
        "plt.show()\n",
        "\n",
        "# Finish WandB logging\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "UFcVOZU06AGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Pure generated Data** <a name=\"gendata\"></a>"
      ],
      "metadata": {
        "id": "26RZTq07BU86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The augmentated dataset\n",
        "file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_generated.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# cleaning data\n",
        "data = data[['Sentence', 'Label','Term']]\n",
        "data = data.dropna()\n",
        "\n",
        "# 1 for insensitive and 0 for notInsensitive\n",
        "data['Label'] = data['Label'].apply(lambda x: 1 if x.lower() == 'insensitive' else 0)\n",
        "\n",
        "#train, validation, and test sets\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, stratify=data['Label'],random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['Label'],random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n",
        "# Save splits\n",
        "train_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_data.csv\", index=False)\n",
        "val_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTval_data.csv\", index=False)\n",
        "test_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtest_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "55Ly2oQr0Chm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_data.csv\")\n",
        "val_data=pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTval_data.csv\")\n",
        "test_data=pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtest_data.csv\")\n",
        "print(f\"Duplicates in train data: {train_data.duplicated(subset=['Sentence', 'Label']).sum()}\")\n",
        "print(f\"Duplicates in validation data: {val_data.duplicated(subset=['Sentence', 'Label']).sum()}\")\n",
        "print(f\"Duplicates in test data: {test_data.duplicated(subset=['Sentence', 'Label']).sum()}\")\n"
      ],
      "metadata": {
        "id": "SLbGykPeNaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "kgk0fbhCfGhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to tokenize data\n",
        "def tokenize_data(data, tokenizer, max_length=109):\n",
        "    return tokenizer(\n",
        "        list(data['Sentence']),\n",
        "        padding=True,\n",
        "        truncation=True,         # Truncate longer sentences\n",
        "        max_length=max_length,   # Max token length\n",
        "        return_tensors='pt'      # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "train_labelsChatGPT = list(train_data['Label'])\n",
        "val_labelsChatGPT = list(val_data['Label'])\n",
        "test_labelsChatGPT = list(test_data['Label'])\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodingsChatGPT = tokenize_data(train_data, tokenizer)\n",
        "val_encodingsChatGPT = tokenize_data(val_data, tokenizer)\n",
        "test_encodingsChatGPT = tokenize_data(test_data, tokenizer)\n",
        "\n",
        "\n",
        "# Save tokenized data\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_encodings_input_ids.npy\", train_encodingsChatGPT['input_ids'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_encodings_attention_mask.npy\", train_encodingsChatGPT['attention_mask'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_labels.npy\", np.array(train_labelsChatGPT))\n"
      ],
      "metadata": {
        "id": "sFmU4l071L01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create datasets for train, validation, and test sets\n",
        "train_datasetChatGPT = SentenceDataset(train_encodingsChatGPT, train_labelsChatGPT)\n",
        "val_datasetChatGPT = SentenceDataset(val_encodingsChatGPT, val_labelsChatGPT)\n",
        "test_datasetChatGPT = SentenceDataset(test_encodingsChatGPT, test_labelsChatGPT)\n",
        "\n",
        "\n",
        "# Save the train, val, and test datasets\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtrain_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_datasetChatGPT, f)\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTval_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(val_datasetChatGPT, f)\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtest_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_datasetChatGPT, f)\n"
      ],
      "metadata": {
        "id": "NDj74_7k3BqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Examples from the training dataset\n",
        "for i in range(5):\n",
        "    item = train_datasetChatGPT[i]\n",
        "    print(\"Input IDs:\", item['input_ids'])\n",
        "    print(\"Attention Mask:\", item['attention_mask'])\n",
        "    print(\"Label:\", item['labels'])  # 0 for Not Insensitive, 1 for Insensitive\n"
      ],
      "metadata": {
        "id": "2heLTVFe3hgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "kJEYAP-kfP-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT for binary classification\n",
        "modelBbaseChatGPT = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ],
      "metadata": {
        "id": "45uv74i13rzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(modelBbaseChatGPT.config)\n"
      ],
      "metadata": {
        "id": "OkXmkSGce534"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wandb.finish()\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_baseChatGPT\")\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=[\"wandb\"],  # WandB is used for logging\n",
        "    run_name=\"Bert_baseChatGPT\"  # the run name for this Trainer\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=modelBbaseChatGPT,\n",
        "    args=training_args,\n",
        "    train_dataset=train_datasetChatGPT,\n",
        "    eval_dataset=val_datasetChatGPT,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate(test_datasetChatGPT, metric_key_prefix=\"test\")\n",
        "wandb.log(results)\n",
        "\n",
        "wandb.finish()  # Close the evaluation session\n"
      ],
      "metadata": {
        "id": "Yhc9dGO-36RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and tokenizer\n",
        "save_directory = \"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTbert_base_model\"\n",
        "trainer.save_model(save_directory)  # Save model\n",
        "tokenizer.save_pretrained(save_directory)  # Save tokenizer"
      ],
      "metadata": {
        "id": "DO6qZyj-CS3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Results"
      ],
      "metadata": {
        "id": "eV2SojZ6fbD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_baseGPT\")\n",
        "\n",
        "# Extract predictions and true labels\n",
        "test_predictions = test_results.predictions.argmax(axis=1)  # Get predicted classes\n",
        "test_true_labels = test_results.label_ids\n",
        "\n",
        "# Evaluate the predictions using classification_report\n",
        "print(\"Classification Report on Test Data:\")\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_true_labels, test_predictions, target_names=[\"notInsensitive\", \"Insensitive\"]))\n",
        "\n",
        "# Save predictions along with the test data\n",
        "test_data['Predicted_Label'] = test_predictions\n",
        "test_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetest_predictions.csv\", index=False)\n",
        "print(\"Predictions saved to '/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetest_predictions.csv'\")\n",
        "\n",
        "# Get predictions on the training data using the same method\n",
        "train_results = trainer.predict(train_datasetChatGPT)\n",
        "\n",
        "# Extract training predictions and labels\n",
        "train_predictions = train_results.predictions.argmax(axis=1)\n",
        "train_true_labels = train_results.label_ids\n",
        "\n",
        "# Evaluate the predictions on the training data\n",
        "print(\"Classification Report on Training Data:\")\n",
        "print(classification_report(train_true_labels, train_predictions, target_names=[\"notInsensitive\", \"Insensitive\"]))\n",
        "\n",
        "# Save training predictions\n",
        "train_data['Predicted_Label'] = train_predictions\n",
        "train_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetrain_predictions.csv\", index=False)\n",
        "print(\"Training predictions saved to '/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetrain_predictions.csv'\")\n"
      ],
      "metadata": {
        "id": "tzdOkwEXBwaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load test predictions and true labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetest_predictions.csv\")\n",
        "true_labels = test_data['Label']\n",
        "predicted_labels = test_data['Predicted_Label']\n",
        "ChatGPT_train_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_basetrain_predictions.csv\")\n",
        "ChatGPT_train_true_labels = ChatGPT_train_data['Label']\n",
        "ChatGPT_train_predicted_labels = ChatGPT_train_data['Predicted_Label']\n",
        "\n",
        "# **1. Compare Train, Validation, and Test Metrics**\n",
        "train_metrics = {\n",
        "    \"accuracy\": accuracy_score(ChatGPT_train_true_labels, ChatGPT_train_predicted_labels),\n",
        "    \"classification_report\": classification_report(\n",
        "        ChatGPT_train_true_labels, ChatGPT_train_predicted_labels, target_names=[\"NotInsensitive\", \"Insensitive\"]\n",
        "    )}\n",
        "\n",
        "val_metrics = {\"accuracy\": 0.97674, \"f1\": 0.97717, \"precision\": 0.96396, \"recall\": 0.99074}\n",
        "test_metrics = {\n",
        "    \"accuracy\": accuracy_score(true_labels, predicted_labels),\n",
        "    \"classification_report\": classification_report(true_labels, predicted_labels, target_names=[\"NotInsensitive\", \"Insensitive\"]),\n",
        "}\n",
        "\n",
        "\n",
        "# Training Metrics\n",
        "train_df = pd.DataFrame({\n",
        "    \"Metric\": [\"Accuracy\"],\n",
        "    \"Value\": [train_metrics[\"accuracy\"]],\n",
        "})\n",
        "print(\"\\nTraining Metrics:\")\n",
        "print(train_df)\n",
        "print(\"\\nClassification Report:\\n\", train_metrics[\"classification_report\"])\n",
        "\n",
        "# Validation Metrics\n",
        "val_df = pd.DataFrame(val_metrics.items(), columns=[\"Metric\", \"Value\"])\n",
        "print(\"\\nValidation Metrics:\")\n",
        "print(val_df)\n",
        "\n",
        "# Test Metrics\n",
        "test_df = pd.DataFrame({\n",
        "    \"Metric\": [\"Accuracy\"],\n",
        "    \"Value\": [test_metrics[\"accuracy\"]],\n",
        "})\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(test_df)\n",
        "print(\"\\nClassification Report:\\n\", test_metrics[\"classification_report\"])\n",
        "\n",
        "\n",
        "epochs = [1, 2, 3]\n",
        "train_losses = [0.212700, 0.050800, 0.007500]  # Training losses for epochs 1, 2, and 3 check training results above\n",
        "val_losses = [0.064114, 0.074954, 0.106145]\n",
        "\n",
        "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Trends\")\n",
        "plt.legend()\n",
        "plt.savefig(\"PureGenloss_trends.png\")\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"NotInsensitive\", \"Insensitive\"], yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "plt.title(\"Confusion Matrix - Test Set\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"PureGenconfusion_matrix.png\")\n",
        "plt.show()\n",
        "\n",
        "# Analyze false positives and false negatives\n",
        "false_positives = test_data[(true_labels == 0) & (predicted_labels == 1)]\n",
        "false_negatives = test_data[(true_labels == 1) & (predicted_labels == 0)]\n",
        "\n",
        "print(\"False Positives:\")\n",
        "print(false_positives[['Sentence', 'Term']])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "print(false_negatives[['Sentence', 'Term']])\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTbert_base_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_data['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# model outputs\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)  # Get probabilities\n",
        "\n",
        "\n",
        "test_data['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_data['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "\n",
        "false_positives['Confidence_Positive'] = test_data.loc[false_positives.index, 'Confidence_Positive']\n",
        "false_negatives['Confidence_Negative'] = test_data.loc[false_negatives.index, 'Confidence_Negative']\n",
        "\n",
        "print(\"False Positives with Confidence:\")\n",
        "print(false_positives[['Sentence', 'Term', 'Confidence_Positive']])\n",
        "\n",
        "print(\"\\nFalse Negatives with Confidence:\")\n",
        "print(false_negatives[['Sentence', 'Term', 'Confidence_Negative']])\n"
      ],
      "metadata": {
        "id": "TzBOt30hD1hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Augmented Data Original + GPT** <a name=\"augdata\"></a>"
      ],
      "metadata": {
        "id": "Wd32q07SBkUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation Process - removing test set before augmentation"
      ],
      "metadata": {
        "id": "0b_CTgn5gWxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "file1 = \"/content/drive/MyDrive/Honours MiscData(Roshna)/deduplicated_annotations.xlsx\"\n",
        "file2 = \"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.xlsx\"\n",
        "output_file1 = \"/content/drive/MyDrive/Honours MiscData(Roshna)/AbstractswithoutTestset.xlsx\"\n",
        "\n",
        "df1 = pd.read_excel(file1)\n",
        "df2 = pd.read_excel(file2)\n",
        "\n",
        "# Ensure the 'Sentence' column exists in both files\n",
        "if 'Sentence' not in df1.columns or 'Sentence' not in df2.columns:\n",
        "    raise ValueError(\"One of the files does not contain a 'Sentence' column.\")\n",
        "\n",
        "# Convert 'Sentence' columns to strings for consistency\n",
        "df1['Sentence'] = df1['Sentence'].astype(str)\n",
        "df2['Sentence'] = df2['Sentence'].astype(str)\n",
        "\n",
        "# Identify sentences in File1 that are also in File2\n",
        "common_sentences = df1['Sentence'][df1['Sentence'].isin(df2['Sentence'])]\n",
        "\n",
        "# Remove sentences in File1 that are also in File2\n",
        "df1_no_common = df1[~df1['Sentence'].isin(df2['Sentence'])]\n",
        "\n",
        "# Count the number of sentences removed\n",
        "removed_count = len(common_sentences)\n",
        "\n",
        "# Save the modified dataframe to a new file\n",
        "df1_no_common.to_excel(output_file1, index=False)\n",
        "\n",
        "# Print confirmation and count of removed sentences\n",
        "print(f\"New file with sentences removed from File1 saved to: {output_file1}\")\n",
        "print(f\"Number of sentences removed from File1: {removed_count}\")\n",
        "\n",
        "# Optional: Print the first few rows of the modified file\n",
        "print(\"\\nFirst 5 rows of File1 without common sentences:\")\n",
        "print(df1_no_common.head())\n"
      ],
      "metadata": {
        "id": "KdfTFvKVWcHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V3 augmented"
      ],
      "metadata": {
        "id": "w8OjgkE5dJgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "original_file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/AbstractswithoutTestset.xlsx'\n",
        "generated_file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPT_generated.xlsx'\n",
        "\n",
        "original_data = pd.read_excel(original_file_path)\n",
        "generated_data = pd.read_excel(generated_file_path)\n",
        "\n",
        "original_data = original_data.rename(columns={\"Matched_Terms\": \"Term\", \"Manual_Annotation\": \"Label\"})\n",
        "generated_data = generated_data.rename(columns={\"Sentence\": \"Sentence\", \"Term\": \"Term\", \"Label\": \"Label\"})\n",
        "\n",
        "original_data['Label'] = original_data['Label'].apply(lambda x: 1 if x.lower() == \"insensitive\" else 0)\n",
        "generated_data['Label'] = generated_data['Label'].apply(lambda x: 1 if x.lower() == \"insensitive\" else 0)\n",
        "\n",
        "original_data['Source'] = 'Original'\n",
        "generated_data['Source'] = 'ChatGPT'\n",
        "\n",
        "# Ensure generated data has all required columns\n",
        "if 'Prompt' not in generated_data.columns:\n",
        "    generated_data['Prompt'] = None\n",
        "\n",
        "# Initialize augmented dataset\n",
        "augmented_data = []\n",
        "\n",
        "# Get unique terms from the lexicon\n",
        "terms = list(set(original_data['Term'].dropna().unique()) | set(generated_data['Term'].dropna().unique()))\n",
        "\n",
        "# Define the required number of sentences per term and label\n",
        "required_count = 25\n",
        "\n",
        "# Augment data for each term\n",
        "for term in terms:\n",
        "    term_data = []\n",
        "\n",
        "    # Filter original and generated datasets for this term\n",
        "    original_term_data = original_data[original_data['Term'] == term]\n",
        "    generated_term_data = generated_data[generated_data['Term'] == term]\n",
        "\n",
        "    # Select insensitive sentences\n",
        "    insensitive_original = original_term_data[original_term_data['Label'] == 1].sample(\n",
        "        n=min(required_count, len(original_term_data[original_term_data['Label'] == 1])),\n",
        "        random_state=42\n",
        "    )\n",
        "    insensitive_generated = generated_term_data[generated_term_data['Label'] == 1].sample(\n",
        "        n=max(0, required_count - len(insensitive_original)),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Select notInsensitive sentences\n",
        "    not_insensitive_original = original_term_data[original_term_data['Label'] == 0].sample(\n",
        "        n=min(required_count, len(original_term_data[original_term_data['Label'] == 0])),\n",
        "        random_state=42\n",
        "    )\n",
        "    not_insensitive_generated = generated_term_data[generated_term_data['Label'] == 0].sample(\n",
        "        n=max(0, required_count - len(not_insensitive_original)),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Combine selected data\n",
        "    term_data.extend(insensitive_original.to_dict(orient='records'))\n",
        "    term_data.extend(insensitive_generated.to_dict(orient='records'))\n",
        "    term_data.extend(not_insensitive_original.to_dict(orient='records'))\n",
        "    term_data.extend(not_insensitive_generated.to_dict(orient='records'))\n",
        "    augmented_data.extend(term_data)\n",
        "\n",
        "augmented_df = pd.DataFrame(augmented_data)\n",
        "\n",
        "#only the required columns\n",
        "required_columns = ['Title', 'Sentence', 'Term', 'Label', 'Source_File', 'Source', 'Prompt']\n",
        "augmented_df = augmented_df[required_columns]\n",
        "\n",
        "# Save the augmented dataset\n",
        "output_file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/25Augmented_annotationsV2.csv'\n",
        "augmented_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Augmented dataset saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "PvJNBL_4coDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentated data Analysis"
      ],
      "metadata": {
        "id": "PHEX07bHcV5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# data\n",
        "path = '/content/drive/MyDrive/Honours MiscData(Roshna)/25Augmented_annotationsV2.csv'\n",
        "data = pd.read_csv(path)\n",
        "term_summary = (\n",
        "    data.groupby(['Term', 'Source', 'Label'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# reverse alphabetical order\n",
        "terms = sorted(term_summary['Term'].unique(), reverse=True)\n",
        "\n",
        "insensitive_original = []\n",
        "insensitive_gpt = []\n",
        "not_insensitive_original = []\n",
        "not_insensitive_gpt = []\n",
        "\n",
        "for term in terms:\n",
        "    term_data = term_summary[term_summary['Term'] == term]\n",
        "\n",
        "    # Filling missing values 0\n",
        "    original = term_data[term_data['Source'] == 'Original']\n",
        "    gpt = term_data[term_data['Source'] == 'ChatGPT']\n",
        "\n",
        "    insensitive_original.append(original[1].values[0] if not original.empty else 0)\n",
        "    not_insensitive_original.append(original[0].values[0] if not original.empty else 0)\n",
        "    insensitive_gpt.append(gpt[1].values[0] if not gpt.empty else 0)\n",
        "    not_insensitive_gpt.append(gpt[0].values[0] if not gpt.empty else 0)\n",
        "\n",
        "\n",
        "y = [i * 3.5 for i in range(len(terms))]  #spacing between terms\n",
        "height = 1.2\n",
        "\n",
        "plt.figure(figsize=(15, 25))\n",
        "plt.barh(\n",
        "    [p - height / 2 for p in y],\n",
        "    insensitive_original,\n",
        "    height=height,\n",
        "    label=\"Insensitive (Source)\",\n",
        "    color='#e69f00',\n",
        "    edgecolor='black'\n",
        ")\n",
        "plt.barh(\n",
        "    [p - height / 2 for p in y],\n",
        "    insensitive_gpt,\n",
        "    height=height,\n",
        "    left=insensitive_original,\n",
        "    label=\"Insensitive (Synthetic)\",\n",
        "    color='#003f5c',\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.barh(\n",
        "    [p + height / 2 for p in y],\n",
        "    not_insensitive_original,\n",
        "    height=height,\n",
        "    label=\"Not Insensitive (Source)\",\n",
        "    color='#d1495b',\n",
        "    edgecolor='black',\n",
        "    alpha=0.9\n",
        ")\n",
        "plt.barh(\n",
        "    [p + height / 2 for p in y],\n",
        "    not_insensitive_gpt,\n",
        "    height=height,\n",
        "    left=not_insensitive_original,\n",
        "    label=\"Not Insensitive (Synthetic)\",\n",
        "    color='#00876c',\n",
        "    edgecolor='black',\n",
        "    alpha=0.9\n",
        ")\n",
        "\n",
        "plt.xlim(0, max(insensitive_original + insensitive_gpt + not_insensitive_original + not_insensitive_gpt))\n",
        "plt.xlabel(\"Number of Sentences\", fontsize=14)\n",
        "plt.ylabel(\"Terms\", fontsize=14)\n",
        "plt.yticks(ticks=y, labels=terms, fontsize=12)  # terms on the y-axis\n",
        "\n",
        "max_x = plt.xlim()[1]\n",
        "for x in range(0, 25 + 1, 5):\n",
        "    plt.axvline(x, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.legend(\n",
        "    fontsize= 11,\n",
        "    ncol=4,\n",
        "    loc='upper left'\n",
        ")\n",
        "# Remove top and right border (spines)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.savefig('distribution_by_term_horizontaln.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "adjGJaFQdono"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# data\n",
        "path = '/content/drive/MyDrive/Honours MiscData(Roshna)/25Augmented_annotationsV2.csv'\n",
        "data = pd.read_csv(path)\n",
        "term_summary = (\n",
        "    data.groupby(['Term', 'Source', 'Label'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# reverse alphabetical order\n",
        "terms = sorted(term_summary['Term'].unique(), reverse=True)\n",
        "\n",
        "insensitive_original = []\n",
        "insensitive_gpt = []\n",
        "not_insensitive_original = []\n",
        "not_insensitive_gpt = []\n",
        "\n",
        "for term in terms:\n",
        "    term_data = term_summary[term_summary['Term'] == term]\n",
        "\n",
        "    # Filling missing values with 0\n",
        "    original = term_data[term_data['Source'] == 'Original']\n",
        "    gpt = term_data[term_data['Source'] == 'ChatGPT']\n",
        "\n",
        "    insensitive_original.append(original[1].values[0] if not original.empty else 0)\n",
        "    not_insensitive_original.append(original[0].values[0] if not original.empty else 0)\n",
        "    insensitive_gpt.append(gpt[1].values[0] if not gpt.empty else 0)\n",
        "    not_insensitive_gpt.append(gpt[0].values[0] if not gpt.empty else 0)\n",
        "\n",
        "# --- Synthetic Percentage Calculation ---\n",
        "total_synthetic = sum(insensitive_gpt) + sum(not_insensitive_gpt)\n",
        "total_original = sum(insensitive_original) + sum(not_insensitive_original)\n",
        "total_all = total_synthetic + total_original\n",
        "\n",
        "synthetic_percentage = (total_synthetic / total_all) * 100\n",
        "print(f\"Synthetic sentences: {total_synthetic} ({synthetic_percentage:.2f}% of total)\")\n",
        "print(f\"Original sentences: {total_original} ({100 - synthetic_percentage:.2f}% of total)\")\n",
        "# ----------------------------------------\n",
        "\n",
        "# Plotting\n",
        "y = [i * 3.5 for i in range(len(terms))]  # spacing between terms\n",
        "height = 1.2\n",
        "\n",
        "plt.figure(figsize=(15, 25))\n",
        "plt.barh(\n",
        "    [p - height / 2 for p in y],\n",
        "    insensitive_original,\n",
        "    height=height,\n",
        "    label=\"Insensitive (Source)\",\n",
        "    color='#e69f00',\n",
        "    edgecolor='black'\n",
        ")\n",
        "plt.barh(\n",
        "    [p - height / 2 for p in y],\n",
        "    insensitive_gpt,\n",
        "    height=height,\n",
        "    left=insensitive_original,\n",
        "    label=\"Insensitive (Synthetic)\",\n",
        "    color='#003f5c',\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.barh(\n",
        "    [p + height / 2 for p in y],\n",
        "    not_insensitive_original,\n",
        "    height=height,\n",
        "    label=\"Not Insensitive (Source)\",\n",
        "    color='#d1495b',\n",
        "    edgecolor='black',\n",
        "    alpha=0.9\n",
        ")\n",
        "plt.barh(\n",
        "    [p + height / 2 for p in y],\n",
        "    not_insensitive_gpt,\n",
        "    height=height,\n",
        "    left=not_insensitive_original,\n",
        "    label=\"Not Insensitive (Synthetic)\",\n",
        "    color='#00876c',\n",
        "    edgecolor='black',\n",
        "    alpha=0.9\n",
        ")\n",
        "\n",
        "plt.xlim(0, max(insensitive_original + insensitive_gpt + not_insensitive_original + not_insensitive_gpt))\n",
        "plt.xlabel(\"Number of Sentences\", fontsize=14)\n",
        "plt.ylabel(\"Terms\", fontsize=14)\n",
        "plt.yticks(ticks=y, labels=terms, fontsize=12)\n",
        "\n",
        "max_x = plt.xlim()[1]\n",
        "for x in range(0, 25 + 1, 5):\n",
        "    plt.axvline(x, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.legend(\n",
        "    fontsize=11,\n",
        "    ncol=4,\n",
        "    loc='upper left'\n",
        ")\n",
        "\n",
        "# Remove top and right borders\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "\n",
        "# Save plot\n",
        "plt.savefig('distribution_by_term_horizontaln.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9EsQHg7GFGF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}