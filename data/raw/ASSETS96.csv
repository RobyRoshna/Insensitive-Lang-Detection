Title,DOI,Abstract,BibTeX
Beyond assistive technology: universal design goes to school,10.1145/228347.228348,"For  centuries,  print  technologies  have  been  the  dominant  means  for  learning  and  expression  in  our  schools.  However,  print  technologies  are  not  equally  accessible  to  all  students.  Those  with  sensory,  physical  and  learning  disabilities  face  particular  barriers  in  print.  Schools  have  attempted  to  overcome  these  barriers  with  a variety  of  adaptations  such  as  special  classrooms,  therapies,  and  assistive  technologies.  As  schools  move  to  a  more  inclusive  multimedia  platform,  new  opportunities  arise  to  eliminate  barriers  for  children  with  disabilities.  The  realization  of  these  opportunities  can  only  be  achieved  through  universal  design  of  educational  media  and  materials.  The  prospects  of  universal  design  in  education  will  be  the  focus  of  this  presentation.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'pages': '1', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'For  centuries,  print  technologies  have  been  the  dominant  means  for  learning  and  expression  in  our  schools.  However,  print  technologies  are  not  equally  accessible  to  all  students.  Those  with  sensory,  physical  and  learning  disabilities  face  particular  barriers  in  print.  Schools  have  attempted  to  overcome  these  barriers  with  a variety  of  adaptations  such  as  special  classrooms,  therapies,  and  assistive  technologies.  As  schools  move  to  a  more  inclusive  multimedia  platform,  new  opportunities  arise  to  eliminate  barriers  for  children  with  disabilities.  The  realization  of  these  opportunities  can  only  be  achieved  through  universal  design  of  educational  media  and  materials.  The  prospects  of  universal  design  in  education  will  be  the  focus  of  this  presentation.', 'doi': '10.1145/228347.228348', 'url': 'https://doi.org/10.1145/228347.228348', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Beyond assistive technology: universal design goes to school', 'author': 'Rose, David', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228348'}"
Touching and hearing GUI's: design issues for the PC-Access system,10.1145/228347.228349,"PC-Access is a system which combines both hardware and software in order  to provide multimodal feedback in a Microsoft Windows graphical interface and within its applications. We propose two versions of PC-Access: one which  offers sound feedback with an enhanced drawing tablet and another in which tactile stimuli are synthesized by a haptic pointing device. When using the  second version, the user will be able to perceive the interface objects (e.g, icons, menus, windows) as well as actions (e.g, moving, re-sizing). Thus, PC-Access offers auditory information (non-verbal sounds and voice synthesis), reinforced by the sense of touch which in turn helps to direct manipulation.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'numpages': '8', 'pages': '2–9', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'PC-Access is a system which combines both hardware and software in order  to provide multimodal feedback in a Microsoft Windows graphical interface and within its applications. We propose two versions of PC-Access: one which  offers sound feedback with an enhanced drawing tablet and another in which tactile stimuli are synthesized by a haptic pointing device. When using the  second version, the user will be able to perceive the interface objects (e.g, icons, menus, windows) as well as actions (e.g, moving, re-sizing). Thus, PC-Access offers auditory information (non-verbal sounds and voice synthesis), reinforced by the sense of touch which in turn helps to direct manipulation.', 'doi': '10.1145/228347.228349', 'url': 'https://doi.org/10.1145/228347.228349', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': ""Touching and hearing GUI's: design issues for the PC-Access system"", 'author': ""Ramstein, Christophe and Martial, Odile and Dufresne, Aude and Carignan, Michel and Chass\\'{e}, Patrick and Mabilleau, Philippe"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228349'}"
Enhancing scanning input with non-speech sounds,10.1145/228347.228350,"This  paper  proposes  the  addition  of  non-speech  sounds  to  aid  people  who  use  scanning  as  their  method  of  input.  Scanning  input  is  a  temporal  task;  users  have  to  press  a switch  when  a  cursor  is  over  the  required  target.  However,  it  is  usually  presented  as  a spatial  task  with  the  items  to  be  scanned  laid-out  in  a  grid.  Research  has  shown  that  for  temporal  tasks  the  auditory  modality  is  often  better  than  the  visual.  This  paper  investigates  this  by  adding  non-speech  sound  to  a  visual  scanning  system.  It  also  shows  how  our  natural  abilities  to  perceive  rhythms  can  be  supported  so  that  they  can  be  used  to  aid  the  scanning  process.  Structured  audio  messages  called  Earcons  were  used  for  the  sound  output.  The  results  from  a  preliminary  investigation  were  favourable,  indicating  that  the  idea  is  feasible  and  further  research  should  be  undertaken.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'scanning input, non-speech sound, multimodal interaction, earcons', 'numpages': '5', 'pages': '10–14', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'This  paper  proposes  the  addition  of  non-speech  sounds  to  aid  people  who  use  scanning  as  their  method  of  input.  Scanning  input  is  a  temporal  task;  users  have  to  press  a switch  when  a  cursor  is  over  the  required  target.  However,  it  is  usually  presented  as  a spatial  task  with  the  items  to  be  scanned  laid-out  in  a  grid.  Research  has  shown  that  for  temporal  tasks  the  auditory  modality  is  often  better  than  the  visual.  This  paper  investigates  this  by  adding  non-speech  sound  to  a  visual  scanning  system.  It  also  shows  how  our  natural  abilities  to  perceive  rhythms  can  be  supported  so  that  they  can  be  used  to  aid  the  scanning  process.  Structured  audio  messages  called  Earcons  were  used  for  the  sound  output.  The  results  from  a  preliminary  investigation  were  favourable,  indicating  that  the  idea  is  feasible  and  further  research  should  be  undertaken.', 'doi': '10.1145/228347.228350', 'url': 'https://doi.org/10.1145/228347.228350', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Enhancing scanning input with non-speech sounds', 'author': 'Brewster, Stephen A. and Raty, Veli-Pekka and Kortekangas, Atte', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228350'}"
A study of input device manipulation difficulties,10.1145/228347.228351,"People with a motor disability affecting their use of the keyboard and/or mouse often tend to make unintentional input errors. Little or no quantified data exists on physical errors in the use of standard computer input devices, particularly with respect to motor disabilities.Such information, if available, could be used to develop techniques for automatic recognition of specific difficulties. Once recognised, many can be reduced or eliminated by appropriate system and application configuration.This paper describes the pilot study for an experiment intended to gather detailed information about input errors made with keyboards and mice. This work is a step towards provision of dynamic, automatic support for the configuration of systems and applications to suit individual users.Some initial results from the pilot study are presented, including an assessment of the experiment design and a summary of some interesting characteristics of the data gathered so far.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'physical disability, mouse, keyboard, input logging, input devices, errors', 'numpages': '8', 'pages': '15–22', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'People with a motor disability affecting their use of the keyboard and/or mouse often tend to make unintentional input errors. Little or no quantified data exists on physical errors in the use of standard computer input devices, particularly with respect to motor disabilities.Such information, if available, could be used to develop techniques for automatic recognition of specific difficulties. Once recognised, many can be reduced or eliminated by appropriate system and application configuration.This paper describes the pilot study for an experiment intended to gather detailed information about input errors made with keyboards and mice. This work is a step towards provision of dynamic, automatic support for the configuration of systems and applications to suit individual users.Some initial results from the pilot study are presented, including an assessment of the experiment design and a summary of some interesting characteristics of the data gathered so far.', 'doi': '10.1145/228347.228351', 'url': 'https://doi.org/10.1145/228347.228351', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'A study of input device manipulation difficulties', 'author': 'Trewin, Shari', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228351'}"
V-Lynx: bringing the World Wide Web to sight impaired users,10.1145/228347.228352,"The  World  Wide  Web  (WWW)  project  merges  the  techniques  of  networked  information  and  hypertext  to make  an  easy  but  powerful  global  information  system.  A client  program  called  a  browser  is  used  to  access  documents  in  the  WWW  system  and  present  them  all  as  parts  of  a seamless  hypertext  information  space. However,  today's  browsers  are  primarily  graphically  or  text  oriented,  which  makes  the  whole  system  inaccessible  to  sight- impaired  users.  In  this  project  we  wanted  to  extend  an  existing  browser  with  voice  output.  This  extension  would  allow  the  sight- impaired  to  use,  at  least,  textual  data,  which,  at  present,  forms  the  bulk  ofinformation  available  over  the  Web.  Our  browser  should  be  able  to read  the  document  a line  or  paragraph  at a time,  read  only  the  first  sentence  in  a paragraph  for  quick  scanning  of  the  document,  convey  the  document  structure  (headings,  emphasized  text,lists,  hyperlinks),  and  allow  for  easy  navigation  while  inside  and  between  documents.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'web navigation, web browser, voice, hypertext, http protocol, audio, WWW, URL, Lynx', 'numpages': '4', 'pages': '23–26', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': ""The  World  Wide  Web  (WWW)  project  merges  the  techniques  of  networked  information  and  hypertext  to make  an  easy  but  powerful  global  information  system.  A client  program  called  a  browser  is  used  to  access  documents  in  the  WWW  system  and  present  them  all  as  parts  of  a seamless  hypertext  information  space. However,  today's  browsers  are  primarily  graphically  or  text  oriented,  which  makes  the  whole  system  inaccessible  to  sight- impaired  users.  In  this  project  we  wanted  to  extend  an  existing  browser  with  voice  output.  This  extension  would  allow  the  sight- impaired  to  use,  at  least,  textual  data,  which,  at  present,  forms  the  bulk  ofinformation  available  over  the  Web.  Our  browser  should  be  able  to read  the  document  a line  or  paragraph  at a time,  read  only  the  first  sentence  in  a paragraph  for  quick  scanning  of  the  document,  convey  the  document  structure  (headings,  emphasized  text,lists,  hyperlinks),  and  allow  for  easy  navigation  while  inside  and  between  documents."", 'doi': '10.1145/228347.228352', 'url': 'https://doi.org/10.1145/228347.228352', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'V-Lynx: bringing the World Wide Web to sight impaired users', 'author': 'Krell, Mitchell and Cubranic, Davor', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228352'}"
Computer generated 3-dimensional models of manual alphabet handshapes for the World Wide Web,10.1145/228347.228353,"A  teaching  tool  consisting  of  a collection  of  three  dimensional  computer  graphic  models  representing  American  Sign  Language  manual  alphabet  hand  shapes  in  various  locations  and  orientations  has  been  established.  These  computer  graphic  models  have  been  recorded  in  the  ""Virtual  Reality  Modeling  Language  (VRML)  [1]  for  display  with  World  Wide  Web  browsers  such  as  Netscape  or  Mosaic,  in  conjunction  with  VRML  browsers  such  as  WebSpace  or  WorldView"".","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'virtual reality, World Wide Web, VRML, ASL', 'numpages': '5', 'pages': '27–31', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'A  teaching  tool  consisting  of  a collection  of  three  dimensional  computer  graphic  models  representing  American  Sign  Language  manual  alphabet  hand  shapes  in  various  locations  and  orientations  has  been  established.  These  computer  graphic  models  have  been  recorded  in  the  ""Virtual  Reality  Modeling  Language  (VRML)  [1]  for  display  with  World  Wide  Web  browsers  such  as  Netscape  or  Mosaic,  in  conjunction  with  VRML  browsers  such  as  WebSpace  or  WorldView"".', 'doi': '10.1145/228347.228353', 'url': 'https://doi.org/10.1145/228347.228353', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Computer generated 3-dimensional models of manual alphabet handshapes for the World Wide Web', 'author': 'Geitz, Sarah and Hanson, Timothy and Maher, Stephen', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228353'}"
Emacspeak—direct speech access,10.1145/228347.228354,"Emacspeak  is  a full-fledged  speech  output  inter- face  to  Emacs,  and  is being  used  to  provide  direct  speech  access  to  a UNIX  workstation.  The kind  of  speech  access  provided  by  Emacspeak  is qual- itatively  different  from  what  conventional  screen- readers  provide  -emacspeak  makes  applications  speak- as  opposed  to  speaking  the  screen.Emacspeak  is the  first  full-fledged  speech  output  system  that  will  allow  someone  who  cannot  see  to  work  directly  on  a UNIX  system  (Until  now,  the  only  option  available  to visually  impaired  users  has  been  to use  a talking  PC  as  a terminal.)  Emacspeak  is built  on  top  of  Emacs.  Once  Emacs  is started,  the  user  gets  complete  spoken  feedback.I currently  use  Emacspeak  at  work  on  my  SUN  SparcStation  and  have  also  used  it on  a DECAL- FHA  workstation  under  Digital  UNIX  while  at Di- gital's  CRL 1 .  I also  use  Emacspeak  as  the  only  speech  output  system  on  my  laptop  running  Linux.  Emacspeak  is available  on  the  Internet:ftp://crl.dec.com/pub/digitallemacspeak/http://www.research.digital.com/CRL","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'direct speech access, access to UNIX workstations', 'numpages': '5', 'pages': '32–36', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': ""Emacspeak  is  a full-fledged  speech  output  inter- face  to  Emacs,  and  is being  used  to  provide  direct  speech  access  to  a UNIX  workstation.  The kind  of  speech  access  provided  by  Emacspeak  is qual- itatively  different  from  what  conventional  screen- readers  provide  -emacspeak  makes  applications  speak- as  opposed  to  speaking  the  screen.Emacspeak  is the  first  full-fledged  speech  output  system  that  will  allow  someone  who  cannot  see  to  work  directly  on  a UNIX  system  (Until  now,  the  only  option  available  to visually  impaired  users  has  been  to use  a talking  PC  as  a terminal.)  Emacspeak  is built  on  top  of  Emacs.  Once  Emacs  is started,  the  user  gets  complete  spoken  feedback.I currently  use  Emacspeak  at  work  on  my  SUN  SparcStation  and  have  also  used  it on  a DECAL- FHA  workstation  under  Digital  UNIX  while  at Di- gital's  CRL 1 .  I also  use  Emacspeak  as  the  only  speech  output  system  on  my  laptop  running  Linux.  Emacspeak  is available  on  the  Internet:ftp://crl.dec.com/pub/digitallemacspeak/http://www.research.digital.com/CRL"", 'doi': '10.1145/228347.228354', 'url': 'https://doi.org/10.1145/228347.228354', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Emacspeak—direct speech access', 'author': 'Raman, T. V.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228354'}"
Combining haptic and braille technologies: design issues and pilot study,10.1145/228347.228355,"This  article  describes  design  issues  for  a bi-dimensional  single  cell  braille  display,  called  Pantobraille,  combining  a standard  braille  cell  with  a force  feedback  device  developed  as  part  of  the  Cffl's  PC-Access  project.  The  Pantobraille,  with  a  10xl6cm  workspace,  allows  the  user  to  place  the  pointer  on  a graphical  interface,  to  perceive  forms  and  textures  using  the  sense  of  touch,  and  to  read  braille  text  on  a bi-dimensional  page.  In  order  to  determine  the  usability  of  such  a device  and  to  have  a better  understanding  of  the  issues  that  may  arise  when  manipulating  it  for  actual  interactive  tasks,  two  visually  handicapped  persons  were  asked  to  use  the  device  to  follow reading  patterns with  one  or  two  hands.  Reading  performance  and  comfort  with  the  Pantobraille  remain  inferior  to  standard  braille  displays  but  significant  improvments  were  observed  while  performing  the  complementary  pointing  and  reading  tasks  using both  hands.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'single cell braille display, haptic interface, force feedback device, braille display', 'numpages': '8', 'pages': '37–44', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': ""This  article  describes  design  issues  for  a bi-dimensional  single  cell  braille  display,  called  Pantobraille,  combining  a standard  braille  cell  with  a force  feedback  device  developed  as  part  of  the  Cffl's  PC-Access  project.  The  Pantobraille,  with  a  10xl6cm  workspace,  allows  the  user  to  place  the  pointer  on  a graphical  interface,  to  perceive  forms  and  textures  using  the  sense  of  touch,  and  to  read  braille  text  on  a bi-dimensional  page.  In  order  to  determine  the  usability  of  such  a device  and  to  have  a better  understanding  of  the  issues  that  may  arise  when  manipulating  it  for  actual  interactive  tasks,  two  visually  handicapped  persons  were  asked  to  use  the  device  to  follow reading  patterns with  one  or  two  hands.  Reading  performance  and  comfort  with  the  Pantobraille  remain  inferior  to  standard  braille  displays  but  significant  improvments  were  observed  while  performing  the  complementary  pointing  and  reading  tasks  using both  hands."", 'doi': '10.1145/228347.228355', 'url': 'https://doi.org/10.1145/228347.228355', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Combining haptic and braille technologies: design issues and pilot study', 'author': 'Ramstein, Christophe', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228355'}"
Interactive tactile display system: a support system for the visually disabled to recognize 3D objects,10.1145/228347.228356,"We have developed an interactive tactile display system for the visually disabled to actively recognize three-dimensional objects or environments. The display presents visual patterns by tactile pins arranged in two-dimensional format. The pin height can be set to several levels to increase the touch information and to display a three-dimensional surface shape. Also, each pin has a tact switch in the bottom for the user to make the system know the position by pushing it. This paper describes the hardware and software of the system.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'the visually disabled, tactile display, stereo vision, interactive interface, 3D', 'numpages': '6', 'pages': '45–50', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'We have developed an interactive tactile display system for the visually disabled to actively recognize three-dimensional objects or environments. The display presents visual patterns by tactile pins arranged in two-dimensional format. The pin height can be set to several levels to increase the touch information and to display a three-dimensional surface shape. Also, each pin has a tact switch in the bottom for the user to make the system know the position by pushing it. This paper describes the hardware and software of the system.', 'doi': '10.1145/228347.228356', 'url': 'https://doi.org/10.1145/228347.228356', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Interactive tactile display system: a support system for the visually disabled to recognize 3D objects', 'author': 'Kawai, Yoshihiro and Tomita, Fumiaki', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228356'}"
Audiograf: a diagram-reader for the blind,10.1145/228347.228357,"In  technical  reports  and  papers  interrelations  are  often  represented  as  diagrams.  With  the  aid  of  a touch  panel  and  auditory  display  AudioGraf  enables  blind  and  visually  impaired  people  to  read  such  diagrams.  The  diagram  is  displayed  on  the  touch  panel  where  a part  can  be  selected  with  a finger.  The  selected  part  will  be  auditorally  displayed.  If  the  finger  is  moved,  another  part  is  selected  and  auditorally  displayed.  This  way  the  whole  diagram  can  be  explored  in  an  audio-tactile  way.  A  model  of  this  audio-tactile  exploration  is  presented.  Based  on  this  model  it  is  explained  how  AudioGraf  supports  the  user.  Usability  tests  have  shown  that  simple  diagrams  can  be  read  by  blind  users  within  relative  short  time.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'usability, reading-aid, diagram, blind users, auditory user interfaces', 'numpages': '6', 'pages': '51–56', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'In  technical  reports  and  papers  interrelations  are  often  represented  as  diagrams.  With  the  aid  of  a touch  panel  and  auditory  display  AudioGraf  enables  blind  and  visually  impaired  people  to  read  such  diagrams.  The  diagram  is  displayed  on  the  touch  panel  where  a part  can  be  selected  with  a finger.  The  selected  part  will  be  auditorally  displayed.  If  the  finger  is  moved,  another  part  is  selected  and  auditorally  displayed.  This  way  the  whole  diagram  can  be  explored  in  an  audio-tactile  way.  A  model  of  this  audio-tactile  exploration  is  presented.  Based  on  this  model  it  is  explained  how  AudioGraf  supports  the  user.  Usability  tests  have  shown  that  simple  diagrams  can  be  read  by  blind  users  within  relative  short  time.', 'doi': '10.1145/228347.228357', 'url': 'https://doi.org/10.1145/228347.228357', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Audiograf: a diagram-reader for the blind', 'author': 'Kennel, Andrea R.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228357'}"
"EVA, an early vocalization analyzer: an empirical validity study of computer categorization",10.1145/228347.228358,"Previous  research  indicates  that  infant  vocalizations  are  effective  predictors  of  later  articulation  and  language  abilities  (Locke,  1989,  Menyuk,  Liebergott,  Shultz,  Chesnick  \&amp;  Ferrier,  1991,  Oller  \&amp;  Seibert  1988,  Jensen,  Boggild-Andersen,  Schmidt,  Ankerhus,  Hansen,  1988).  Intervention  to  encourage  babbling  activity  in  at-risk  infants  is frequently  undertaken.  Research  and  clinical  diagnosis  of  delayed  or  reduced  babbling  have  so  far  relied  on  time-consuming  and  unreliable  perceptual  analyses  of  recorded  infant  sounds.  While  acoustic  analysis  of  infant  sounds  has  provided  important  information  on  the  early  characteristics  of  infant  vocalizations  (Bauer,  1988,  Stark  1986)  this  information  has  still  to  be  used  to  carry  out  automatic,  real-time  analysis.We  are  developing  a program,  EVA,  for  the  Macintosh  computer  that  automatically  analyzes  digitized  recordings  of  infant  vocalizations.  We  describe  the  prototype  and  report  on  validity- testing  of  the  first  stage  of  development.  Our  human  judge  and  EVA  had  92.8\%  agreement  on  the  number  of  utterances  in  the  20  minutes  of  recordings,  commonly  identifying  411  utterances.  Their  categorizations  agreed  79.8\%  for  duration  and  87.3\%  for  frequency,  better  than  human  inter-judge  agreement  reported  in  the  literature.  The authors  hope  that  the  final  version  of  EVA  will  serve  as  a reliable  standard  for  the  analysis  and  evaluation  of  utterances  of  normal  and  at-risk  infants  with  a variety  of  etiologies.  The  acoustic  information  gained  from  such  analysis  will  allow  us  to  develop  a computer-based  system  to  encourage  early  vocalization.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'pre-speech vocalization, infants, early intervention, acoustic analysis', 'numpages': '7', 'pages': '57–63', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'Previous  research  indicates  that  infant  vocalizations  are  effective  predictors  of  later  articulation  and  language  abilities  (Locke,  1989,  Menyuk,  Liebergott,  Shultz,  Chesnick  \\&amp;  Ferrier,  1991,  Oller  \\&amp;  Seibert  1988,  Jensen,  Boggild-Andersen,  Schmidt,  Ankerhus,  Hansen,  1988).  Intervention  to  encourage  babbling  activity  in  at-risk  infants  is frequently  undertaken.  Research  and  clinical  diagnosis  of  delayed  or  reduced  babbling  have  so  far  relied  on  time-consuming  and  unreliable  perceptual  analyses  of  recorded  infant  sounds.  While  acoustic  analysis  of  infant  sounds  has  provided  important  information  on  the  early  characteristics  of  infant  vocalizations  (Bauer,  1988,  Stark  1986)  this  information  has  still  to  be  used  to  carry  out  automatic,  real-time  analysis.We  are  developing  a program,  EVA,  for  the  Macintosh  computer  that  automatically  analyzes  digitized  recordings  of  infant  vocalizations.  We  describe  the  prototype  and  report  on  validity- testing  of  the  first  stage  of  development.  Our  human  judge  and  EVA  had  92.8\\%  agreement  on  the  number  of  utterances  in  the  20  minutes  of  recordings,  commonly  identifying  411  utterances.  Their  categorizations  agreed  79.8\\%  for  duration  and  87.3\\%  for  frequency,  better  than  human  inter-judge  agreement  reported  in  the  literature.  The authors  hope  that  the  final  version  of  EVA  will  serve  as  a reliable  standard  for  the  analysis  and  evaluation  of  utterances  of  normal  and  at-risk  infants  with  a variety  of  etiologies.  The  acoustic  information  gained  from  such  analysis  will  allow  us  to  develop  a computer-based  system  to  encourage  early  vocalization.', 'doi': '10.1145/228347.228358', 'url': 'https://doi.org/10.1145/228347.228358', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'EVA, an early vocalization analyzer: an empirical validity study of computer categorization', 'author': 'Fell, Harriet J. and Ferrier, Linda J. and Mooraj, Zehra and Benson, Etienne and Schneider, Dale', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228358'}"
An approach to the evaluation of assistive technology,10.1145/228347.228359,"A valid criticism of may innovations in assistive technology is that they have not been evaluated. However. there are obstacles which make this form of technology difficult to evaluate according to conventional paradigms. The reasons behind this are discussed. A particular evaluation which endeavoured to circumvent those problems is described. The item evaluated was Matllta/k, a program to make mathematics accessible to blind people.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'mathematics, earcons, blind people, auditory interfaces', 'numpages': '8', 'pages': '64–71', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'A valid criticism of may innovations in assistive technology is that they have not been evaluated. However. there are obstacles which make this form of technology difficult to evaluate according to conventional paradigms. The reasons behind this are discussed. A particular evaluation which endeavoured to circumvent those problems is described. The item evaluated was Matllta/k, a program to make mathematics accessible to blind people.', 'doi': '10.1145/228347.228359', 'url': 'https://doi.org/10.1145/228347.228359', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'An approach to the evaluation of assistive technology', 'author': 'Stevens, Robert D. and Edwards, Alistair D. N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228359'}"
Designing interface toolkit with dynamic selectable modality,10.1145/228347.228360,"Incorporating flexibility to select desirable modality into user interface systems is needed for people with disabilities, since most modern applications use graphical user interfaces forcing fixed modality which is useful only to sighted users.However, the requirement of user interfaces with flexible and selectable modality is not a specific problem of disabled persons but a general problem of interfaces in the next generation, considering that environments, in which computers are used, are widening rapidly.This paper discusses about an architecture of user interface toolkit to support flexibility required by both users with disability and users in special environment, and proposes a model of semantic abstraction of user interaction, named abstract widgets. The experimental implementation of such toolkit, named Fruit system, is also described.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'graphical user interface, multi-modal interface, user interface management system', 'numpages': '8', 'pages': '72–79', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'Incorporating flexibility to select desirable modality into user interface systems is needed for people with disabilities, since most modern applications use graphical user interfaces forcing fixed modality which is useful only to sighted users.However, the requirement of user interfaces with flexible and selectable modality is not a specific problem of disabled persons but a general problem of interfaces in the next generation, considering that environments, in which computers are used, are widening rapidly.This paper discusses about an architecture of user interface toolkit to support flexibility required by both users with disability and users in special environment, and proposes a model of semantic abstraction of user interaction, named abstract widgets. The experimental implementation of such toolkit, named Fruit system, is also described.', 'doi': '10.1145/228347.228360', 'url': 'https://doi.org/10.1145/228347.228360', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Designing interface toolkit with dynamic selectable modality', 'author': 'Kawai, Shiro and Aida, Hitoshi and Saito, Tadao', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228360'}"
Multimodal input for computer access and augmentative communication,10.1145/228347.228361,"This paper describes the overall goals of a project that focuses on multimodal input for computer access and Augmentative and Alternative Communication (AAC) systems. In particular the project explores the integration of speech recognition with head-pointing. The first part of this project addresses the use of speech and head-pointing to replace the traditional keyboard and mouse. While either of these technologies can emulate both keyboard and mouse functions, it is hypothesized that the most advantageous use of each technology will come from integration such that each device's strength is utilized appropriately.To test this hypothesis, a series of experiments are planned. The first experiment compares (quantitatively and qualitatively) each technology in the context of text generation. The second experiment looks at typical pointing tasks (e.g., dragging) for each technology. The third experiment will look at the technologies in an integrated context. Because each of the technologies are themselves highly complex, significant time and effort has been devoted to pilot testing. Those results and the implications on our research methodology are presented in this paper.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'assistive technology, augmentative and alternative communication, computer access, head pointing, multimodal input, speech recognition', 'numpages': '6', 'pages': '80–85', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': ""This paper describes the overall goals of a project that focuses on multimodal input for computer access and Augmentative and Alternative Communication (AAC) systems. In particular the project explores the integration of speech recognition with head-pointing. The first part of this project addresses the use of speech and head-pointing to replace the traditional keyboard and mouse. While either of these technologies can emulate both keyboard and mouse functions, it is hypothesized that the most advantageous use of each technology will come from integration such that each device's strength is utilized appropriately.To test this hypothesis, a series of experiments are planned. The first experiment compares (quantitatively and qualitatively) each technology in the context of text generation. The second experiment looks at typical pointing tasks (e.g., dragging) for each technology. The third experiment will look at the technologies in an integrated context. Because each of the technologies are themselves highly complex, significant time and effort has been devoted to pilot testing. Those results and the implications on our research methodology are presented in this paper."", 'doi': '10.1145/228347.228361', 'url': 'https://doi.org/10.1145/228347.228361', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Multimodal input for computer access and augmentative communication', 'author': 'Smith, Alice and Dunaway, John and Demasco, Patrick and Peischl, Denise', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228361'}"
The keybowl: an ergonomically designed document processing device,10.1145/228347.228362,"This  paper  discloses  preliminary  findings  and  provides  a  discussion  of  a newly  designed  alphanumeric  input  device  called  the  Keybowl.  The  Keybowl  was  designed  and  developed  primarily  as  an  alternative  input  device  to  allow  users  of  various  upper  extremity  disabilities  to  effectively  type,  interact  with,  and  navigate  current  computer  interface  designs.  In  addition,  the  Keybowl's  unique  characteristics  of  adapting  to  the  user's  needs  may  provide  a solution  to  the  multi-million  dollar  a year  problem  of  carpal  tunnel  syndrome  (CTS)  as  it relates  to  typing.  The  Keybowl  totally  eliminates  finger  movement,  minimizes  wrist  movement,  and  uses  the  concept  of  concurrent  independent  inputs  (i.e.,  chording)  in  which  two  domes  are  moved  laterally  to  type.  Initial  results  indicated  that  users  of  the  Keybowl  typed  an  average  of  52\%  of  their  regular  QWERTY  flatboard  keying  speed  in  as  little  as  five  hours.  With  regard  to  ergonomic  advantage,  Keybowl  typists'  flexion/extension  wrist  movements  were  reduced  by  an  average  of  81.5\%  when  compared  to  typists  using  the  QWERTY  keyboard.  Movements  in  the  ulnar/radial  plane  were  reduced  by  an  average  of  48\%.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'carpal tunnel syndrome, cumulative trauma, handicap, keyboard, typing', 'numpages': '8', 'pages': '86–93', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': ""This  paper  discloses  preliminary  findings  and  provides  a  discussion  of  a newly  designed  alphanumeric  input  device  called  the  Keybowl.  The  Keybowl  was  designed  and  developed  primarily  as  an  alternative  input  device  to  allow  users  of  various  upper  extremity  disabilities  to  effectively  type,  interact  with,  and  navigate  current  computer  interface  designs.  In  addition,  the  Keybowl's  unique  characteristics  of  adapting  to  the  user's  needs  may  provide  a solution  to  the  multi-million  dollar  a year  problem  of  carpal  tunnel  syndrome  (CTS)  as  it relates  to  typing.  The  Keybowl  totally  eliminates  finger  movement,  minimizes  wrist  movement,  and  uses  the  concept  of  concurrent  independent  inputs  (i.e.,  chording)  in  which  two  domes  are  moved  laterally  to  type.  Initial  results  indicated  that  users  of  the  Keybowl  typed  an  average  of  52\\%  of  their  regular  QWERTY  flatboard  keying  speed  in  as  little  as  five  hours.  With  regard  to  ergonomic  advantage,  Keybowl  typists'  flexion/extension  wrist  movements  were  reduced  by  an  average  of  81.5\\%  when  compared  to  typists  using  the  QWERTY  keyboard.  Movements  in  the  ulnar/radial  plane  were  reduced  by  an  average  of  48\\%."", 'doi': '10.1145/228347.228362', 'url': 'https://doi.org/10.1145/228347.228362', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'The keybowl: an ergonomically designed document processing device', 'author': 'McAlindon, Peter J. and Stanney, Kay M.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228362'}"
Designing the World Wide Web for people with disabilities: a user centered design approach,10.1145/228347.228363,"The  emergence  of  the  World  Wide  Web  has  made  it  possible  for  individuals  with  appropriate  computer  and  telecommunications  equipment  to  interact  as  never  before.  An  explosion  of  next-generation  information  systems  are  flooding  the  commercial  market.  This  cyberspace  convergence  of  data,  computers,  networks,  and  multimedia  presents  exciting  challenges  to  interface  designers.  However,  this  ""new  technology  frontier""  has  also  created  enormous  roadblocks  and  barriers  for  people  with  disabilities.  This  panel  will  discuss  specific  issues,  suggest  potential  solutions  and  solicit  contributions  required  to  design  an  accessible  Web  interface  that  includes  people  with  disabilities.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'accessibility, blindness, deaf, disabilities, hypermedia, mobility, people with disabilities, software development, special needs, user interfaces, user requirements', 'numpages': '8', 'pages': '94–101', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'The  emergence  of  the  World  Wide  Web  has  made  it  possible  for  individuals  with  appropriate  computer  and  telecommunications  equipment  to  interact  as  never  before.  An  explosion  of  next-generation  information  systems  are  flooding  the  commercial  market.  This  cyberspace  convergence  of  data,  computers,  networks,  and  multimedia  presents  exciting  challenges  to  interface  designers.  However,  this  ""new  technology  frontier""  has  also  created  enormous  roadblocks  and  barriers  for  people  with  disabilities.  This  panel  will  discuss  specific  issues,  suggest  potential  solutions  and  solicit  contributions  required  to  design  an  accessible  Web  interface  that  includes  people  with  disabilities.', 'doi': '10.1145/228347.228363', 'url': 'https://doi.org/10.1145/228347.228363', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Designing the World Wide Web for people with disabilities: a user centered design approach', 'author': 'Laux, Lila F. and McNally, Peter R. and Paciello, Michael G. and Vanderheiden, Gregg C.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228363'}"
A gesture recognition architecture for sign language,10.1145/228347.228364,"This paper presents a gesture recognition architecture dedicated to Sign Languages. Sign Language gestures include five co-occurring parameters, which convey complementary independent information. Some signs belong to a predefined lexicon which can be learned by the recognition system, but some other signs may be created during the discourses, depending on the context. The proposed recognition system is able to recognise both kinds of signs, by using specific classification tools, and a virtual scene for context storage. It is based on a study of French Sign Language (LSF).","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'data glove, gesture interpretation, gesture recognition, sign language', 'numpages': '8', 'pages': '102–109', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'This paper presents a gesture recognition architecture dedicated to Sign Languages. Sign Language gestures include five co-occurring parameters, which convey complementary independent information. Some signs belong to a predefined lexicon which can be learned by the recognition system, but some other signs may be created during the discourses, depending on the context. The proposed recognition system is able to recognise both kinds of signs, by using specific classification tools, and a virtual scene for context storage. It is based on a study of French Sign Language (LSF).', 'doi': '10.1145/228347.228364', 'url': 'https://doi.org/10.1145/228347.228364', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'A gesture recognition architecture for sign language', 'author': 'Braffort, Annelies', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228364'}"
“Composability”: widening participation in music making for people with disabilities via music software and controller solutions,10.1145/228347.228365,"This  paper  discusses  ways  of  enabling  visually  impaired  and  physically  disabled  people  to  compose  and  perform  music.  The  usage  and  adaptation  of  existing  software-based  composition  systems  are  described,  in  the  context  of  education  work  undertaken  by  the  Drake  Music  Project  - a  charity  which  aims  to  facilitate  disabled  people  in  making  music  via  technology.  Some  of  the  problems  faced  arc  discussed,  and  a  custom  system  presented  which  aims  to  resolve  some  of  these  difficulties.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'MIDI, adaptive technology, composition, education, music, physical disability, visual impairment', 'numpages': '7', 'pages': '110–116', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'This  paper  discusses  ways  of  enabling  visually  impaired  and  physically  disabled  people  to  compose  and  perform  music.  The  usage  and  adaptation  of  existing  software-based  composition  systems  are  described,  in  the  context  of  education  work  undertaken  by  the  Drake  Music  Project  - a  charity  which  aims  to  facilitate  disabled  people  in  making  music  via  technology.  Some  of  the  problems  faced  arc  discussed,  and  a  custom  system  presented  which  aims  to  resolve  some  of  these  difficulties.', 'doi': '10.1145/228347.228365', 'url': 'https://doi.org/10.1145/228347.228365', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': '“Composability”: widening participation in music making for people with disabilities via music software and controller solutions', 'author': 'Anderson, Tim and Smith, Clare', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228365'}"
A generic direct-manipulation 3D-auditory environment for hierarchical navigation in non-visual interaction,10.1145/228347.228366,"Auditory  presentation  methods  may  significantly  enhance  the  interaction  quality  during  user-computer  dialogue.  The  impact  of  auditory  interaction  methods  is  important  in  the  context  of  non-visual  interaction,  where  audio  is the  primary  direct  perception  output  modality.  In  a few  cases,  3D-audio  output  techniques  have  been  employed  for  providing  interaction  for  blind  users.  Unfortunately,  such  developments  have  been  too  specialized  and  do  not  support  re-usability  of  the  implemented  approaches  and  techniques  in  different  contexts,  where  non-visual  interaction  needs  to  be  realized.  A  generic  re-usable  environment  has  been  implemented,  based  on  3D  audio,  3D  pointing,  hand  gestures  and  voice  input,  which  is  applicable  in  all  cases  that  interactive  hierarchically  structured  selections  from  sets  of  alternatives  must  be  handled.  This  environment  has  been  used  to  implement  the  hierarchical  navigation  dialogue  in  a multi- media  non-visual  toolkit  currently  under  development.  It  is  composed  of  a  set  of  modules  implementing  re-usable  functionality  with  which  interaction  for  non-visual  hierarchical  navigation  can  be  realized  within  any  non-visual  interaction  toolkit.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': '3D-audio, auditory interfaces, non-visual interaction, re-usability, toolkits', 'numpages': '7', 'pages': '117–123', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'Auditory  presentation  methods  may  significantly  enhance  the  interaction  quality  during  user-computer  dialogue.  The  impact  of  auditory  interaction  methods  is  important  in  the  context  of  non-visual  interaction,  where  audio  is the  primary  direct  perception  output  modality.  In  a few  cases,  3D-audio  output  techniques  have  been  employed  for  providing  interaction  for  blind  users.  Unfortunately,  such  developments  have  been  too  specialized  and  do  not  support  re-usability  of  the  implemented  approaches  and  techniques  in  different  contexts,  where  non-visual  interaction  needs  to  be  realized.  A  generic  re-usable  environment  has  been  implemented,  based  on  3D  audio,  3D  pointing,  hand  gestures  and  voice  input,  which  is  applicable  in  all  cases  that  interactive  hierarchically  structured  selections  from  sets  of  alternatives  must  be  handled.  This  environment  has  been  used  to  implement  the  hierarchical  navigation  dialogue  in  a multi- media  non-visual  toolkit  currently  under  development.  It  is  composed  of  a  set  of  modules  implementing  re-usable  functionality  with  which  interaction  for  non-visual  hierarchical  navigation  can  be  realized  within  any  non-visual  interaction  toolkit.', 'doi': '10.1145/228347.228366', 'url': 'https://doi.org/10.1145/228347.228366', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'A generic direct-manipulation 3D-auditory environment for hierarchical navigation in non-visual interaction', 'author': 'Savidis, Anthony and Stephanidis, Constantine and Korte, Andreas and Crispien, Kai and Fellbaum, Klaus', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228366'}"
Improving the usability of speech-based interfaces for blind users,10.1145/228347.228367,"Adaptations  using speech  synthesis  provide  a  basic  level  of  access  to  computer  systems  for  blind  users,  but  current  systems  pose  a number  of  usability  problems.  A  study  was  carried  out  in  order  to  assess  the  impact  of  certain  issues  on  the  usability  of  a  typical  speech  adaptation.  The  results  suggest  that  much  work  needs  to  be  done  on  the  design  of  speech  dialogues.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'numpages': '7', 'pages': '124–130', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'Adaptations  using speech  synthesis  provide  a  basic  level  of  access  to  computer  systems  for  blind  users,  but  current  systems  pose  a number  of  usability  problems.  A  study  was  carried  out  in  order  to  assess  the  impact  of  certain  issues  on  the  usability  of  a  typical  speech  adaptation.  The  results  suggest  that  much  work  needs  to  be  done  on  the  design  of  speech  dialogues.', 'doi': '10.1145/228347.228367', 'url': 'https://doi.org/10.1145/228347.228367', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Improving the usability of speech-based interfaces for blind users', 'author': 'Pitt, Ian J. and Edwards, Alistair D. N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228367'}"
TDraw: a computer-based tactile drawing tool for blind people,10.1145/228347.228368,"Considerations  about  blind  people?s  relation  to  pictures  of  real  world  objects  lead  to  the  conclusion  that  blind  and  sighted  people  have  very  similar  mental  models  of  the  3D  world.  Because  perception  works  completely  differently,  the  mapping  of  the  3D  world  to  a  2D  picture  differs  significantly.  A  tool  has  been  developed  to  allow  blind  people  to  draw  pictures  and  at  the  same  time  study  their  drawing  process.  A  first  evaluation  shows  interesting  results.  These  will  eventually  lead  to  a  design  of  a  rendering  tool  for  (tactile)  pictures  for  blind  people.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'TDraw, mental model, tactile drawing tool, tactile drawings, tactile rendering', 'numpages': '8', 'pages': '131–138', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'Considerations  about  blind  people?s  relation  to  pictures  of  real  world  objects  lead  to  the  conclusion  that  blind  and  sighted  people  have  very  similar  mental  models  of  the  3D  world.  Because  perception  works  completely  differently,  the  mapping  of  the  3D  world  to  a  2D  picture  differs  significantly.  A  tool  has  been  developed  to  allow  blind  people  to  draw  pictures  and  at  the  same  time  study  their  drawing  process.  A  first  evaluation  shows  interesting  results.  These  will  eventually  lead  to  a  design  of  a  rendering  tool  for  (tactile)  pictures  for  blind  people.', 'doi': '10.1145/228347.228368', 'url': 'https://doi.org/10.1145/228347.228368', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'TDraw: a computer-based tactile drawing tool for blind people', 'author': 'Kurze, Martin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228368'}"
Development of dialogue systems for a mobility aid for blind people: initial design and usability testing,10.1145/228347.228369,"This  paper  presents  a  new  travel  aid  to  increase  the  independent  mobility  of  blind  and  elderly  travellers.  This  aid  builds  on  the  technologies  of  geographical  information  systems  (GIS)  and  the  Global  Positioning  System  (GPS).  The  MoBIC  Travel  Aid  (MoTA)  consists  of  two  interrelated  components:  the  MoBIC  Pre-journey  System  (MoPS)  to  assist  users  in  planning  journeys  and  the  MoBIC  Outdoor  System  (MoODS)  to  execute  these  plans  by  providing  users  with  orientation  and  navigation  assistance  during  journeys.  The  MoBIC  travel  aid  is  complementary  to  primary  mobility  aids  such  as  the  long  cane  or  guide  dog.  Results  of  a study  of  user  requirements,  the  user  interface  designs,  and  the  first  field  trial,  currently  being  conducted  in  Berlin,  are  presented.","{'series': ""Assets '96"", 'location': 'Vancouver, British Columbia, Canada', 'keywords': 'GIS, GPS, mobility and navigation, user trials, visually disabled users', 'numpages': '6', 'pages': '139–144', 'booktitle': 'Proceedings of the Second Annual ACM Conference on Assistive Technologies', 'abstract': 'This  paper  presents  a  new  travel  aid  to  increase  the  independent  mobility  of  blind  and  elderly  travellers.  This  aid  builds  on  the  technologies  of  geographical  information  systems  (GIS)  and  the  Global  Positioning  System  (GPS).  The  MoBIC  Travel  Aid  (MoTA)  consists  of  two  interrelated  components:  the  MoBIC  Pre-journey  System  (MoPS)  to  assist  users  in  planning  journeys  and  the  MoBIC  Outdoor  System  (MoODS)  to  execute  these  plans  by  providing  users  with  orientation  and  navigation  assistance  during  journeys.  The  MoBIC  travel  aid  is  complementary  to  primary  mobility  aids  such  as  the  long  cane  or  guide  dog.  Results  of  a study  of  user  requirements,  the  user  interface  designs,  and  the  first  field  trial,  currently  being  conducted  in  Berlin,  are  presented.', 'doi': '10.1145/228347.228369', 'url': 'https://doi.org/10.1145/228347.228369', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '0897917766', 'year': '1996', 'title': 'Development of dialogue systems for a mobility aid for blind people: initial design and usability testing', 'author': 'Strothotte, Thomas and Fritz, Steffi and Michel, Rainer and Raab, Andreas and Petrie, Helen and Johnson, Valerie and Reichert, Lars and Schalt, Axel', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/228347.228369'}"
