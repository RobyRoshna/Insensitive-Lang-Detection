Title,DOI,Abstract,BibTeX
How Do People with Limited Movement Personalize Upper-Body Gestures? Considerations for the Design of Personalized and Accessible Gesture Interfaces,10.1145/3597638.3608430,"Always-on, upper-body input from sensors like accelerometers, infrared cameras, and electromyography hold promise to enable accessible gesture input for people with upper-body motor impairments. When these sensors are distributed across the person’s body, they can enable the use of varied body parts and gestures for device interaction. Personalized upper-body gestures that enable input from diverse body parts including the head, neck, shoulders, arms, hands and fingers and match the abilities of each user, could be useful for ensuring that gesture systems are accessible. In this work, we characterize the personalized gesture sets designed by 25 participants with upper-body motor impairments and develop design recommendations for upper-body personalized gesture interfaces. We found that the personalized gesture sets that participants designed were highly ability-specific. Even within a specific type of disability, there were significant differences in what muscles participants used to perform upper-body gestures, with some predominantly using shoulder and upper-arm muscles, and others solely using their finger muscles. Eight percent of gestures that participants designed were with their head, neck, and shoulders, rather than their hands and fingers, demonstrating the importance of tracking the whole upper-body. To combat fatigue, participants performed 51\% of gestures with their hands resting on or barely coming off of their armrest, highlighting the importance of using sensing mechanisms that are agnostic to the location and orientation of the body. Lastly, participants activated their muscles but did not visibly move during 10\% of the gestures, demonstrating the need for using sensors that can sense muscle activations without movement. Both inertial measurement unit (IMU) and electromyography (EMG) wearable sensors proved to be promising sensors to differentiate between personalized gestures. Personalized upper-body gesture interfaces that take advantage of each person’s abilities are critical for enabling accessible upper-body gestures for people with upper-body motor impairments.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, gestures, input, motor impairments', 'numpages': '15', 'articleno': '1', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Always-on, upper-body input from sensors like accelerometers, infrared cameras, and electromyography hold promise to enable accessible gesture input for people with upper-body motor impairments. When these sensors are distributed across the person’s body, they can enable the use of varied body parts and gestures for device interaction. Personalized upper-body gestures that enable input from diverse body parts including the head, neck, shoulders, arms, hands and fingers and match the abilities of each user, could be useful for ensuring that gesture systems are accessible. In this work, we characterize the personalized gesture sets designed by 25 participants with upper-body motor impairments and develop design recommendations for upper-body personalized gesture interfaces. We found that the personalized gesture sets that participants designed were highly ability-specific. Even within a specific type of disability, there were significant differences in what muscles participants used to perform upper-body gestures, with some predominantly using shoulder and upper-arm muscles, and others solely using their finger muscles. Eight percent of gestures that participants designed were with their head, neck, and shoulders, rather than their hands and fingers, demonstrating the importance of tracking the whole upper-body. To combat fatigue, participants performed 51\\% of gestures with their hands resting on or barely coming off of their armrest, highlighting the importance of using sensing mechanisms that are agnostic to the location and orientation of the body. Lastly, participants activated their muscles but did not visibly move during 10\\% of the gestures, demonstrating the need for using sensors that can sense muscle activations without movement. Both inertial measurement unit (IMU) and electromyography (EMG) wearable sensors proved to be promising sensors to differentiate between personalized gestures. Personalized upper-body gesture interfaces that take advantage of each person’s abilities are critical for enabling accessible upper-body gestures for people with upper-body motor impairments.', 'doi': '10.1145/3597638.3608430', 'url': 'https://doi.org/10.1145/3597638.3608430', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'How Do People with Limited Movement Personalize Upper-Body Gestures? Considerations for the Design of Personalized and Accessible Gesture Interfaces', 'author': 'Yamagami, Momona and Portnova-Fahreeva, Alexandra A and Kong, Junhan and Wobbrock, Jacob O. and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608430'}"
"Experiments with RouteNav, A Wayfinding App for Blind Travelers in a Transit Hub",10.1145/3597638.3608428,"RouteNav is an iOS app designed to support wayfinding for blind travelers in an indoor/outdoor transit hub. It doesn’t rely on external infrastructure (such as BLE beacons); instead, localization is obtained by fusing spatial information from inertial dead reckoning and GPS (when available) via particle filtering. Routes are expressed as sequences of “tiles”, where each tile may contain relevant points of interest. Redundant modalities are used to guide users to switching goalposts within tiles. In this paper, we describe the different components of RouteNav, and report on a user study with seven blind participants, who traversed three challenging routes in a transit hub while receiving input from the app.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'mobility, orientation, wayfinding', 'numpages': '15', 'articleno': '2', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'RouteNav is an iOS app designed to support wayfinding for blind travelers in an indoor/outdoor transit hub. It doesn’t rely on external infrastructure (such as BLE beacons); instead, localization is obtained by fusing spatial information from inertial dead reckoning and GPS (when available) via particle filtering. Routes are expressed as sequences of “tiles”, where each tile may contain relevant points of interest. Redundant modalities are used to guide users to switching goalposts within tiles. In this paper, we describe the different components of RouteNav, and report on a user study with seven blind participants, who traversed three challenging routes in a transit hub while receiving input from the app.', 'doi': '10.1145/3597638.3608428', 'url': 'https://doi.org/10.1145/3597638.3608428', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Experiments with RouteNav, A Wayfinding App for Blind Travelers in a Transit Hub', 'author': 'Ren, Peng and Lam, Jonathan and Manduchi, Roberto and Mirzaei, Fatemeh', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608428'}"
A Usability Study of Nomon: A Flexible Interface for Single-Switch Users,10.1145/3597638.3608415,"Many individuals with severe motor impairments communicate via a single switch—which might be activated by a blink, facial movement, or puff of air. These switches are commonly used as input to scanning systems that allow selection from a 2D grid of options. Nomon is an alternative interface that provides a more flexible layout, not confined to a grid. Previous work suggests that, even when options appear in a grid, Nomon may be faster and easier to use than scanning systems. However, previous work primarily tested Nomon with non–motor-impaired individuals, and evaluation with potential end-users was limited to a single motor-impaired participant. We provide a usability study following seven participants with motor impairments and compare their performance with Nomon against a row-column scanning system. Most participants were faster with Nomon in a picture selection task, while entry rates varied more in a text-entry task. However, we found participants had to click more times per selection using Nomon, motivating future research into mitigating this increased click load. All but one participant preferred using Nomon; most reported it felt faster and had better predictive text.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '17', 'articleno': '3', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Many individuals with severe motor impairments communicate via a single switch—which might be activated by a blink, facial movement, or puff of air. These switches are commonly used as input to scanning systems that allow selection from a 2D grid of options. Nomon is an alternative interface that provides a more flexible layout, not confined to a grid. Previous work suggests that, even when options appear in a grid, Nomon may be faster and easier to use than scanning systems. However, previous work primarily tested Nomon with non–motor-impaired individuals, and evaluation with potential end-users was limited to a single motor-impaired participant. We provide a usability study following seven participants with motor impairments and compare their performance with Nomon against a row-column scanning system. Most participants were faster with Nomon in a picture selection task, while entry rates varied more in a text-entry task. However, we found participants had to click more times per selection using Nomon, motivating future research into mitigating this increased click load. All but one participant preferred using Nomon; most reported it felt faster and had better predictive text.', 'doi': '10.1145/3597638.3608415', 'url': 'https://doi.org/10.1145/3597638.3608415', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Usability Study of Nomon: A Flexible Interface for Single-Switch Users', 'author': 'Bonaker, Nicholas and Nel, Emli-Mari and Vertanen, Keith and Broderick, Tamara', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608415'}"
“I just thought it was me”: How Smartphones Fail Users with Mild-to-Moderate Dexterity Differences,10.1145/3597638.3608396,"Accessibility solutions often focus on the experiences of people with more severe disabilities, such as those who are unable to perform certain tasks unassisted. However, disability exists on a spectrum, and people with moderate disabilities may be overlooked when studying or sampling for differences. As a result, these individuals and their needs are excluded from relevant research. In this study, we interviewed 12 adults with mild-to-moderate dexterity impairments about their experiences using smartphones and other mobile devices. While our participants frequently experienced accessibility challenges, they struggled to know where to find help, in part because of discomfort with traditional labels of disability and accessibility. We found four key themes: (1) There were large gaps in available and usable accessibility tools for this population, (2) Users were unlikely to seek out accessibility features due to complex disability identity, (3) Contextual concerns impacted mobile device use, and (4) Users relied on self-created adaptations and modifications to improve usability. We suggest that individuals with mild-to-moderate dexterity challenges are a unique cohort that would benefit from further consideration from the accessibility community and accessibility features that support their needs.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '12', 'articleno': '4', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Accessibility solutions often focus on the experiences of people with more severe disabilities, such as those who are unable to perform certain tasks unassisted. However, disability exists on a spectrum, and people with moderate disabilities may be overlooked when studying or sampling for differences. As a result, these individuals and their needs are excluded from relevant research. In this study, we interviewed 12 adults with mild-to-moderate dexterity impairments about their experiences using smartphones and other mobile devices. While our participants frequently experienced accessibility challenges, they struggled to know where to find help, in part because of discomfort with traditional labels of disability and accessibility. We found four key themes: (1) There were large gaps in available and usable accessibility tools for this population, (2) Users were unlikely to seek out accessibility features due to complex disability identity, (3) Contextual concerns impacted mobile device use, and (4) Users relied on self-created adaptations and modifications to improve usability. We suggest that individuals with mild-to-moderate dexterity challenges are a unique cohort that would benefit from further consideration from the accessibility community and accessibility features that support their needs.', 'doi': '10.1145/3597638.3608396', 'url': 'https://doi.org/10.1145/3597638.3608396', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“I just thought it was me”: How Smartphones Fail Users with Mild-to-Moderate Dexterity Differences', 'author': 'Bowman, Molly and Robinson, Jerry and Buehler, Erin and Kane, Shaun', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608396'}"
TouchPilot: Designing a Guidance System that Assists Blind People in Learning Complex 3D Structures,10.1145/3597638.3608426,"Making complex structures accessible to blind people is challenging due to the need for skilled explainers. Interactive 3D printed models (I3Ms) have been developed to enable independent learning of 3D models through activating audio labels. However, they present single-layered information and require users to identify interactive elements through a pinpointing action, which might be insufficient for learning complex and unfamiliar subjects. In this paper, we investigate I3Ms for complex structures. We propose TouchPilot, a guidance system designed based on a study that observed learner-explainer interaction styles. TouchPilot guides users step by step through navigation, exploration of hierarchical elements, and confirmation of their entire areas. A follow-up study found that the guidance system led to better learning outcomes and higher independence compared to a pinpointing system. Feedback suggests that being primed by the guidance system systematically, followed by pinpointing freely for review, is preferred for learning complex structures.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'computer vision, guidance, interactive 3D printed models, visual impairments', 'numpages': '18', 'articleno': '5', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Making complex structures accessible to blind people is challenging due to the need for skilled explainers. Interactive 3D printed models (I3Ms) have been developed to enable independent learning of 3D models through activating audio labels. However, they present single-layered information and require users to identify interactive elements through a pinpointing action, which might be insufficient for learning complex and unfamiliar subjects. In this paper, we investigate I3Ms for complex structures. We propose TouchPilot, a guidance system designed based on a study that observed learner-explainer interaction styles. TouchPilot guides users step by step through navigation, exploration of hierarchical elements, and confirmation of their entire areas. A follow-up study found that the guidance system led to better learning outcomes and higher independence compared to a pinpointing system. Feedback suggests that being primed by the guidance system systematically, followed by pinpointing freely for review, is preferred for learning complex structures.', 'doi': '10.1145/3597638.3608426', 'url': 'https://doi.org/10.1145/3597638.3608426', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'TouchPilot: Designing a Guidance System that Assists Blind People in Learning Complex 3D Structures', 'author': 'Wang, Xiyue and Kayukawa, Seita and Takagi, Hironobu and Asakawa, Chieko', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608426'}"
VisPhoto: Photography for People with Visual Impairments via Post-Production of Omnidirectional Camera Imaging,10.1145/3597638.3608422,"Many people with visual impairments would like to take photographs. However, they often have difficulty pointing the camera at the target. In this paper, we address this problem by proposing a novel photo-taking system called VisPhoto. Unlike conventional methods, VisPhoto generates a photograph in post-production. When the shutter button is pressed, VisPhoto captures an omnidirectional camera image that contains the surrounding scene of the camera. In post-production, the system outputs a cropped region as a “photograph” that satisfies the user’s preference. We conducted an experiment consisting of two parts. First, 24 people with visual impairments took photographs with a genuine iPhone camera app, a conventional method, and VisPhoto. Second, 20 sighted people evaluated the quality of the photographs. The experimental results showed that the participants with visual impairments preferred to use VisPhoto to take photographs of difficult targets, whereas they preferred the conventional method for easy targets. Moreover, we revealed that their preferences for photo-taking methods were influenced by the participants’ needs and values about photography and their confidence in their photographic abilities.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Photography, object detection, omnidirectional camera, sound recording', 'numpages': '17', 'articleno': '6', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Many people with visual impairments would like to take photographs. However, they often have difficulty pointing the camera at the target. In this paper, we address this problem by proposing a novel photo-taking system called VisPhoto. Unlike conventional methods, VisPhoto generates a photograph in post-production. When the shutter button is pressed, VisPhoto captures an omnidirectional camera image that contains the surrounding scene of the camera. In post-production, the system outputs a cropped region as a “photograph” that satisfies the user’s preference. We conducted an experiment consisting of two parts. First, 24 people with visual impairments took photographs with a genuine iPhone camera app, a conventional method, and VisPhoto. Second, 20 sighted people evaluated the quality of the photographs. The experimental results showed that the participants with visual impairments preferred to use VisPhoto to take photographs of difficult targets, whereas they preferred the conventional method for easy targets. Moreover, we revealed that their preferences for photo-taking methods were influenced by the participants’ needs and values about photography and their confidence in their photographic abilities.', 'doi': '10.1145/3597638.3608422', 'url': 'https://doi.org/10.1145/3597638.3608422', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'VisPhoto: Photography for People with Visual Impairments via Post-Production of Omnidirectional Camera Imaging', 'author': 'Hirabayashi, Naoki and Iwamura, Masakazu and Cheng, Zheng and Minatani, Kazunori and Kise, Koichi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608422'}"
Barriers and Benefits: The Path to Accessible Makerspaces,10.1145/3597638.3608414,"Motivated by the philosophical overlap between makerspace culture and the needs of assistive technology users, we investigated the ways that makerspaces can support the development of new technologies by and for disabled makers. Using eleven semi-structured interviews with makerspace operators and disabled makerspace users, we identified five categories of barriers to makerspace participation: recruitment/outreach, physical access, financial, access to information, and belonging. Based on these interviews, we highlight ways makerspaces can better welcome makers with disabilities: enabling members to create adaptive technologies for the space (“makerspacing the makerspace”), making the physical space and information within the space accessible, and fostering belonging by building relationships with the disability community. Overall, our work contributes to our understanding of the possibilities and challenges of connecting the disabled community with the maker community and suggests new directions for collaboration, especially towards building hybrid makerspaces that provide multiple modalities for connection and creativity.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, co-design, hackathons, makerspaces', 'numpages': '14', 'articleno': '7', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Motivated by the philosophical overlap between makerspace culture and the needs of assistive technology users, we investigated the ways that makerspaces can support the development of new technologies by and for disabled makers. Using eleven semi-structured interviews with makerspace operators and disabled makerspace users, we identified five categories of barriers to makerspace participation: recruitment/outreach, physical access, financial, access to information, and belonging. Based on these interviews, we highlight ways makerspaces can better welcome makers with disabilities: enabling members to create adaptive technologies for the space (“makerspacing the makerspace”), making the physical space and information within the space accessible, and fostering belonging by building relationships with the disability community. Overall, our work contributes to our understanding of the possibilities and challenges of connecting the disabled community with the maker community and suggests new directions for collaboration, especially towards building hybrid makerspaces that provide multiple modalities for connection and creativity.', 'doi': '10.1145/3597638.3608414', 'url': 'https://doi.org/10.1145/3597638.3608414', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Barriers and Benefits: The Path to Accessible Makerspaces', 'author': 'Allen, Katherine H. and Balaska, Audrey K. and Aronson, Reuben M. and Rogers, Chris and Short, Elaine Schaertl', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608414'}"
Understanding Digital Content Creation Needs of Blind and Low Vision People,10.1145/3597638.3608387,"Creative activities play an essential role in everyday life. Recently, there has been increasing interest in the accessibility community to support blind and low vision (BLV) people’s digital creative experiences. We conducted a mixed-method study to gain a comprehensive understanding of their creative needs to inform and focus this line of research. Through a large-scale survey (N = 165) and follow-up interviews (N = 15), we learned that BLV people are interested in a more diverse range of creative tasks than what is currently accessible. In particular, many forms of visual content creation and advanced expressions still remain challenging. Participants pointed out both accessibility improvements and social changes needed to fulfill personal creative pursuits. In turn, we discuss potential design ideas to move toward more inclusive creative practices, such as developing alternative, non-visual information-sharing methods and establishing visual information presentation guidelines specific to creative contexts.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, blind and low vision, creativity support', 'numpages': '15', 'articleno': '8', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Creative activities play an essential role in everyday life. Recently, there has been increasing interest in the accessibility community to support blind and low vision (BLV) people’s digital creative experiences. We conducted a mixed-method study to gain a comprehensive understanding of their creative needs to inform and focus this line of research. Through a large-scale survey (N = 165) and follow-up interviews (N = 15), we learned that BLV people are interested in a more diverse range of creative tasks than what is currently accessible. In particular, many forms of visual content creation and advanced expressions still remain challenging. Participants pointed out both accessibility improvements and social changes needed to fulfill personal creative pursuits. In turn, we discuss potential design ideas to move toward more inclusive creative practices, such as developing alternative, non-visual information-sharing methods and establishing visual information presentation guidelines specific to creative contexts.', 'doi': '10.1145/3597638.3608387', 'url': 'https://doi.org/10.1145/3597638.3608387', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Digital Content Creation Needs of Blind and Low Vision People', 'author': 'Zhang, Lotus and Sun, Simon and Findlater, Leah', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608387'}"
A Gallery In My Hand: A Multi-Exhibition Investigation of Accessible and Inclusive Gallery Experiences for Blind and Low Vision Visitors,10.1145/3597638.3608391,"With their overwhelming reliance on visual arts and artefacts, art galleries and museums are often considered unapproachable spaces for people who are blind or have low vision (BLV). We present the results of an ongoing collaboration with an Australian regional gallery in their efforts to improve access by BLV people to their permanent collection and temporary exhibitions. We describe the approach taken to develop interpretive technology-mediated experiences for BLV visitors for their 2021 and 2022 flagship exhibitions, along with evaluation from the perspectives of both BLV visitors and the gallery staff. Outcomes include: broader awareness within the gallery for inclusive design practice and public engagement; better understandings of the needs of BLV visitors, in particular relating to confidence building and technology integration; understanding of gallery constraints and challenges; and a framework for the development of future inclusive gallery experiences.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, art, blind, inclusive design, low vision', 'numpages': '15', 'articleno': '9', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'With their overwhelming reliance on visual arts and artefacts, art galleries and museums are often considered unapproachable spaces for people who are blind or have low vision (BLV). We present the results of an ongoing collaboration with an Australian regional gallery in their efforts to improve access by BLV people to their permanent collection and temporary exhibitions. We describe the approach taken to develop interpretive technology-mediated experiences for BLV visitors for their 2021 and 2022 flagship exhibitions, along with evaluation from the perspectives of both BLV visitors and the gallery staff. Outcomes include: broader awareness within the gallery for inclusive design practice and public engagement; better understandings of the needs of BLV visitors, in particular relating to confidence building and technology integration; understanding of gallery constraints and challenges; and a framework for the development of future inclusive gallery experiences.', 'doi': '10.1145/3597638.3608391', 'url': 'https://doi.org/10.1145/3597638.3608391', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Gallery In My Hand: A Multi-Exhibition Investigation of Accessible and Inclusive Gallery Experiences for Blind and Low Vision Visitors', 'author': 'Butler, Matthew and Tandori, Erica J and Dziekan, Vince and Ellis, Kirsten and Hall, Jenna and Holloway, Leona M and Nagassa, Ruth G and Marriott, Kim', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608391'}"
Understanding Curators' Practices and Challenge of Making Exhibitions More Accessible for People with Visual Impairments,10.1145/3597638.3608384,"Assistive technologies are increasingly developed and applied in exhibition environments to help blind and low vision (BLV) people deal with the challenges they face when visiting exhibitions. While studies have examined the experiences of BLV people using such technologies, little is known about the experiences and challenges of curators incorporating assistive technologies into exhibitions to make them more accessible to BLV people. This research focuses on assistive technologies for BLV people in exhibitions from a curatorial perspective. We conducted semi-structured interviews with twenty-two experienced curators to understand their practices and challenges. We also curated a list of assistive technologies from published papers and used them as probes to seek curators’ attitudes and perceptions of such technologies. We uncovered four critical themes related to curators’ challenges of making exhibitions more accessible to BLV people. We further identified a vicious circle, which prevents curators from making exhibitions more accessible and discussed possible ways to support curators in making exhibitions more accessible to BLV people.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Exhibition, accessibility, assistive technology, blind, curator, interview, low vision, museum, probe study, visual impairment, visually impaired', 'numpages': '18', 'articleno': '10', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Assistive technologies are increasingly developed and applied in exhibition environments to help blind and low vision (BLV) people deal with the challenges they face when visiting exhibitions. While studies have examined the experiences of BLV people using such technologies, little is known about the experiences and challenges of curators incorporating assistive technologies into exhibitions to make them more accessible to BLV people. This research focuses on assistive technologies for BLV people in exhibitions from a curatorial perspective. We conducted semi-structured interviews with twenty-two experienced curators to understand their practices and challenges. We also curated a list of assistive technologies from published papers and used them as probes to seek curators’ attitudes and perceptions of such technologies. We uncovered four critical themes related to curators’ challenges of making exhibitions more accessible to BLV people. We further identified a vicious circle, which prevents curators from making exhibitions more accessible and discussed possible ways to support curators in making exhibitions more accessible to BLV people.', 'doi': '10.1145/3597638.3608384', 'url': 'https://doi.org/10.1145/3597638.3608384', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""Understanding Curators' Practices and Challenge of Making Exhibitions More Accessible for People with Visual Impairments"", 'author': 'Huang, Yuru and Zhang, Jingling and Jin, Xiaofu and Fan, Mingming', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608384'}"
Understanding Challenges and Opportunities in Body Movement Education of People who are Blind or have Low Vision,10.1145/3597638.3608409,"Actively participating in body movement such as dance, sports, and fitness activities is challenging for people who are blind or have low vision (BLV). Teachers primarily rely on verbal instructions and physical demonstrations with limited accessibility. Recent work shows that technology can support body movement education for BLV people. However, there is limited involvement with the BLV community and their teachers to understand their needs. By conducting a series of two surveys, 23 interviews and four focus groups, we gather the voices and perspectives of BLV people and their teachers. This provides a rich understanding of the challenges of body movement education. We identify ten major themes, four key design challenges, and propose potential solutions. We encourage the assistive technologies community to co-design potential solutions to these identified design challenges promoting the quality of life of BLV people and supporting the teachers in the provision of inclusive education.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'blind, body movement, design challenges, education, low vision', 'numpages': '19', 'articleno': '11', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Actively participating in body movement such as dance, sports, and fitness activities is challenging for people who are blind or have low vision (BLV). Teachers primarily rely on verbal instructions and physical demonstrations with limited accessibility. Recent work shows that technology can support body movement education for BLV people. However, there is limited involvement with the BLV community and their teachers to understand their needs. By conducting a series of two surveys, 23 interviews and four focus groups, we gather the voices and perspectives of BLV people and their teachers. This provides a rich understanding of the challenges of body movement education. We identify ten major themes, four key design challenges, and propose potential solutions. We encourage the assistive technologies community to co-design potential solutions to these identified design challenges promoting the quality of life of BLV people and supporting the teachers in the provision of inclusive education.', 'doi': '10.1145/3597638.3608409', 'url': 'https://doi.org/10.1145/3597638.3608409', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Challenges and Opportunities in Body Movement Education of People who are Blind or have Low Vision', 'author': 'De Silva, Madhuka and Goodwin, Sarah and Holloway, Leona and Butler, Matthew', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608409'}"
Comparing Natural Language and Vibro-Audio Modalities for Inclusive STEM Learning with Blind and Low Vision Users,10.1145/3597638.3608429,"Data representations continue to be produced in predominantly visual forms within STEM disciplines. The disparity in access to these graphical representations between students who are blind or have low vision (BLV) and their sighted peers is exacerbated as the adoption of digital screens become more prevalent in educational settings. Standard accessibility solutions rely heavily on natural language processing, e.g., screen readers, for non-visual information access. But can other non-visual modalities, like touch, be effective for learning graphical content rendered on touchscreens? To investigate this question, we conducted a user study with a multimodal touchscreen learning system to assess the effectiveness of two non-visual graphical presentation modalities: 1) a vibro-audio condition, which used the device's embedded vibration motor plus an auditory content overview (a spatial and multimodal technique), and 2) a natural language condition that provided a complete description of the content (a cognitively mediated technique). BLV participants (N = 19) were presented with the learning system and asked to answer multiple-choice questions about three different graph types using both presentation modalities. Findings showed that the two presentation modalities were functionally equivalent for learning the graphical information presented, suggesting that for these stimuli, the presentation modality did not have a significant effect on participant graph learning accuracy. However, the type of graph being learned did have a reliable effect. Moreover, a majority of the participants stated a preference towards the natural language condition and, on average, learned graphs faster than with the vibro-audio condition. The similarity found between the two learning modalities is interpreted as supporting user learning preferences while providing redundancy in the information being communicated. This approach layers the various types of information found in graphical representations (text, numerical, spatial) for individuals with accessible learning needs, providing more control, independence, and responsive tools to optimize their own educational materials.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Blindness and low vision, Information access, Natural language, STEM graphs, Vibro-audio', 'numpages': '17', 'articleno': '12', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Data representations continue to be produced in predominantly visual forms within STEM disciplines. The disparity in access to these graphical representations between students who are blind or have low vision (BLV) and their sighted peers is exacerbated as the adoption of digital screens become more prevalent in educational settings. Standard accessibility solutions rely heavily on natural language processing, e.g., screen readers, for non-visual information access. But can other non-visual modalities, like touch, be effective for learning graphical content rendered on touchscreens? To investigate this question, we conducted a user study with a multimodal touchscreen learning system to assess the effectiveness of two non-visual graphical presentation modalities: 1) a vibro-audio condition, which used the device's embedded vibration motor plus an auditory content overview (a spatial and multimodal technique), and 2) a natural language condition that provided a complete description of the content (a cognitively mediated technique). BLV participants (N = 19) were presented with the learning system and asked to answer multiple-choice questions about three different graph types using both presentation modalities. Findings showed that the two presentation modalities were functionally equivalent for learning the graphical information presented, suggesting that for these stimuli, the presentation modality did not have a significant effect on participant graph learning accuracy. However, the type of graph being learned did have a reliable effect. Moreover, a majority of the participants stated a preference towards the natural language condition and, on average, learned graphs faster than with the vibro-audio condition. The similarity found between the two learning modalities is interpreted as supporting user learning preferences while providing redundancy in the information being communicated. This approach layers the various types of information found in graphical representations (text, numerical, spatial) for individuals with accessible learning needs, providing more control, independence, and responsive tools to optimize their own educational materials."", 'doi': '10.1145/3597638.3608429', 'url': 'https://doi.org/10.1145/3597638.3608429', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Comparing Natural Language and Vibro-Audio Modalities for Inclusive STEM Learning with Blind and Low Vision Users', 'author': 'Brown, Justin R. and Doore, Stacy A. and Dimmel, Justin K. and Giudice, Norbert and Giudice, Nicholas A.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608429'}"
Notably Inaccessible — Data Driven Understanding of Data Science Notebook (In)Accessibility,10.1145/3597638.3608417,"Computational notebooks, tools that facilitate storytelling through exploration, data analysis, and information visualization, have become the widely accepted standard in the data science community. These notebooks have been widely adopted through notebook software such as Jupyter, Datalore and Google Colab, both in academia and industry. While there is extensive research to learn how data scientists use computational notebooks, identify their pain points, and enable collaborative data science practices, very little is known about the various accessibility barriers experienced by blind and visually impaired (BVI) users using these notebooks. BVI users are unable to use computational notebook interfaces due to (1) inaccessibility of the interface, (2) common ways in which data is represented in these interfaces, and (3) inability for popular libraries to provide accessible outputs. We perform a large scale systematic analysis of 100000 Jupyter notebooks to identify various accessibility challenges in published notebooks affecting the creation and consumption of these notebooks. Through our findings, we make recommendations to improve accessibility of the artifacts of a notebook, suggest authoring practices, and propose changes to infrastructure to make notebooks accessible.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Data science, computational notebooks, measurement', 'numpages': '19', 'articleno': '13', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Computational notebooks, tools that facilitate storytelling through exploration, data analysis, and information visualization, have become the widely accepted standard in the data science community. These notebooks have been widely adopted through notebook software such as Jupyter, Datalore and Google Colab, both in academia and industry. While there is extensive research to learn how data scientists use computational notebooks, identify their pain points, and enable collaborative data science practices, very little is known about the various accessibility barriers experienced by blind and visually impaired (BVI) users using these notebooks. BVI users are unable to use computational notebook interfaces due to (1) inaccessibility of the interface, (2) common ways in which data is represented in these interfaces, and (3) inability for popular libraries to provide accessible outputs. We perform a large scale systematic analysis of 100000 Jupyter notebooks to identify various accessibility challenges in published notebooks affecting the creation and consumption of these notebooks. Through our findings, we make recommendations to improve accessibility of the artifacts of a notebook, suggest authoring practices, and propose changes to infrastructure to make notebooks accessible.', 'doi': '10.1145/3597638.3608417', 'url': 'https://doi.org/10.1145/3597638.3608417', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Notably Inaccessible — Data Driven Understanding of Data Science Notebook (In)Accessibility', 'author': 'Potluri, Venkatesh and Singanamalla, Sudheesh and Tieanklin, Nussara and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608417'}"
Playing with Feeling: Exploring Vibrotactile Feedback and Aesthetic Experiences for Developing Haptic Wearables for Blind and Low Vision Music Learning,10.1145/3597638.3608397,"Musical haptic wearables (MHWs) that convey information through vibrotactile feedback holds the potential to support the music learning of a blind or low vision (BLV) music learner. Yet, it is unclear how these technologies can give functional support to a BLV person. We also investigated material preferences in such technologies to understand the role of non-functional aesthetic experiences in shaping their music learning. We conducted 5 co-design workshops with 10 BLV participants. Participants explored eleven materials common in a music learning environment and engaged in bodystorming with a prototype that communicated six vibrotactile patterns. Through thematic analysis, we found that MHWs with vibrotactile alerts and variations in vibration are suited to communicate instructional information, aid music reading and support technical guidance and practice. We categorized the participants’ material experiences into sensorial, interpretive, and affective levels. Based on our findings, we discuss considerations when designing vibrotactile interactions to support music learning for BLV people and highlight material experiences that should be emphasized to make the music learning experience wholesome for BLV music learners.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Assistive Technologies, Blind and Low Vision Music Learning, Material Aesthetics, Material Experiences, Musical haptic wearables, User Experience Design, Vibrotactile Feedback', 'numpages': '16', 'articleno': '14', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Musical haptic wearables (MHWs) that convey information through vibrotactile feedback holds the potential to support the music learning of a blind or low vision (BLV) music learner. Yet, it is unclear how these technologies can give functional support to a BLV person. We also investigated material preferences in such technologies to understand the role of non-functional aesthetic experiences in shaping their music learning. We conducted 5 co-design workshops with 10 BLV participants. Participants explored eleven materials common in a music learning environment and engaged in bodystorming with a prototype that communicated six vibrotactile patterns. Through thematic analysis, we found that MHWs with vibrotactile alerts and variations in vibration are suited to communicate instructional information, aid music reading and support technical guidance and practice. We categorized the participants’ material experiences into sensorial, interpretive, and affective levels. Based on our findings, we discuss considerations when designing vibrotactile interactions to support music learning for BLV people and highlight material experiences that should be emphasized to make the music learning experience wholesome for BLV music learners.', 'doi': '10.1145/3597638.3608397', 'url': 'https://doi.org/10.1145/3597638.3608397', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Playing with Feeling: Exploring Vibrotactile Feedback and Aesthetic Experiences for Developing Haptic Wearables for Blind and Low Vision Music Learning', 'author': 'Lu, Leon and Kang, Jin and Crispin, Chase and Girouard, Audrey', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608397'}"
“Not There Yet”: Feasibility and Challenges of Mobile Sound Recognition to Support Deaf and Hard-of-Hearing People,10.1145/3597638.3608431,"While recent advances have enabled mobile sound recognition tools for deaf and hard of hearing (DHH) people, these tools have only been studied in the lab or through short, controlled experiments. To assess the real-world feasibility and guide the future designs of mobile sound awareness systems, we conducted a three-week field study of SoundWatch, a smartwatch-based sound recognition app, with 10 DHH participants. Our findings suggest the app's utility in increasing environmental awareness and facilitating everyday tasks for DHH users. However, several challenges, such as background noises, variability of real-world sounds, and confusion among similar sounding sounds, indicated that mobile sound recognition solutions are “not there yet” for adoption and use in daily life. We close by presenting HCI design opportunities to improve model reliability by increasing contextual awareness, supporting end-user customization, and fostering the collective improvement of sound recognition models.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '14', 'articleno': '15', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""While recent advances have enabled mobile sound recognition tools for deaf and hard of hearing (DHH) people, these tools have only been studied in the lab or through short, controlled experiments. To assess the real-world feasibility and guide the future designs of mobile sound awareness systems, we conducted a three-week field study of SoundWatch, a smartwatch-based sound recognition app, with 10 DHH participants. Our findings suggest the app's utility in increasing environmental awareness and facilitating everyday tasks for DHH users. However, several challenges, such as background noises, variability of real-world sounds, and confusion among similar sounding sounds, indicated that mobile sound recognition solutions are “not there yet” for adoption and use in daily life. We close by presenting HCI design opportunities to improve model reliability by increasing contextual awareness, supporting end-user customization, and fostering the collective improvement of sound recognition models."", 'doi': '10.1145/3597638.3608431', 'url': 'https://doi.org/10.1145/3597638.3608431', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“Not There Yet”: Feasibility and Challenges of Mobile Sound Recognition to Support Deaf and Hard-of-Hearing People', 'author': 'Huang, Jeremy Zhengqi and Chhabria, Hriday and Jain, Dhruv', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608431'}"
Enhancing Non-Speech Information Communicated in Closed Captioning Through Critical Design,10.1145/3597638.3608398,"The communication of non-speech information (NSI) in closed captioning is essential in providing full access and increased enjoyment of video content, particularly for d/Deaf or Hard-of-Hearing (DHH) viewers. We identified the limitations and frustrations of current NSI captioning through needfinding interviews and then employed a critical design framework to develop a medium-fidelity audio-reactive animated overlay prototype to explore the opinions and values of DHH users regarding NSI communication through surveys and interviews. The results show that current NSI captioning strategies lack consistency across platforms, adequate temporal information, clarity in emotional conveyance, and lack customization options. The study suggests that novel sound communication technologies show promise in enhancing certain aspects NSI communication and that there is a strong desire to move beyond the current, inflexible format of single-track closed captions for NSI communication.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Captioning, Non-Speech Sounds', 'numpages': '14', 'articleno': '16', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The communication of non-speech information (NSI) in closed captioning is essential in providing full access and increased enjoyment of video content, particularly for d/Deaf or Hard-of-Hearing (DHH) viewers. We identified the limitations and frustrations of current NSI captioning through needfinding interviews and then employed a critical design framework to develop a medium-fidelity audio-reactive animated overlay prototype to explore the opinions and values of DHH users regarding NSI communication through surveys and interviews. The results show that current NSI captioning strategies lack consistency across platforms, adequate temporal information, clarity in emotional conveyance, and lack customization options. The study suggests that novel sound communication technologies show promise in enhancing certain aspects NSI communication and that there is a strong desire to move beyond the current, inflexible format of single-track closed captions for NSI communication.', 'doi': '10.1145/3597638.3608398', 'url': 'https://doi.org/10.1145/3597638.3608398', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Enhancing Non-Speech Information Communicated in Closed Captioning Through Critical Design', 'author': 'May, Lloyd and Park, So Yeon and Berger, Jonathan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608398'}"
Investigation into Stress Triggers in Autistic Adults for the Development of Technological Self-Interventions,10.1145/3597638.3608392,"Adults with autism spectrum disorder (ASD) face daily challenges dealing with stress, for example, at work, or in public spaces, which can be overwhelming in terms of coping with sensory overload. Technological interventions have been created in order to attempt to counter symptoms of stress, through improving cognition and functioning of target users. Mobile devices, virtual reality (VR) and augmented reality (AR) applications have demonstrated the ability to support and improve clinical practices through sensory based technologies. In this paper, we present the initial phase of research concerning an online survey conducted with over 200 autistic participants and caregivers. We report on the identification of currently used technological preferences, triggers associated with stress, issues regarding sensitivity and perception, the needs of caregivers, as well as the perceived potential for new technologies. The study has identified that sensitivity to sound and light remain key stress triggers for those on the autism spectrum. &nbsp;Headphones and mobile devices were considered to be suitable methods of&nbsp;accessing multi-sensory stimuli&nbsp;to help manage stress.&nbsp; Augmented Reality and VR were identified by autistic participants&nbsp;as technologies that might aid in the reduction of unpredictable stress-related events. The insights gained from the research will inform future research regarding the design of technological self-interventions for stress management in autistic adults.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Autism, Stress Management, Stress Triggers', 'numpages': '17', 'articleno': '17', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Adults with autism spectrum disorder (ASD) face daily challenges dealing with stress, for example, at work, or in public spaces, which can be overwhelming in terms of coping with sensory overload. Technological interventions have been created in order to attempt to counter symptoms of stress, through improving cognition and functioning of target users. Mobile devices, virtual reality (VR) and augmented reality (AR) applications have demonstrated the ability to support and improve clinical practices through sensory based technologies. In this paper, we present the initial phase of research concerning an online survey conducted with over 200 autistic participants and caregivers. We report on the identification of currently used technological preferences, triggers associated with stress, issues regarding sensitivity and perception, the needs of caregivers, as well as the perceived potential for new technologies. The study has identified that sensitivity to sound and light remain key stress triggers for those on the autism spectrum. &nbsp;Headphones and mobile devices were considered to be suitable methods of&nbsp;accessing multi-sensory stimuli&nbsp;to help manage stress.&nbsp; Augmented Reality and VR were identified by autistic participants&nbsp;as technologies that might aid in the reduction of unpredictable stress-related events. The insights gained from the research will inform future research regarding the design of technological self-interventions for stress management in autistic adults.', 'doi': '10.1145/3597638.3608392', 'url': 'https://doi.org/10.1145/3597638.3608392', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Investigation into Stress Triggers in Autistic Adults for the Development of Technological Self-Interventions', 'author': 'McGowan, John Joseph and Mcgregor, Iain Peter', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608392'}"
AdaptiveSound: An Interactive Feedback-Loop System to Improve Sound Recognition for Deaf and Hard of Hearing Users,10.1145/3597638.3608390,"Sound recognition tools have wide-ranging impacts for deaf and hard of hearing (DHH) people from being informed of safety-critical information (e.g., fire alarms, sirens) to more mundane but still useful information (e.g., door knock, microwave beeps). However, prior sound recognition systems use models that are pre-trained on generic sound datasets and do not adapt well to diverse variations of real-world sounds. We introduce AdaptiveSound, a real-time system for portable devices (e.g., smartphones) that allows DHH users to provide corrective feedback to the sound recognition model to adapt the model to diverse acoustic environments. AdaptiveSound is informed by prior surveys of sound recognition systems, where DHH users strongly desired the ability to provide feedback to a pre-trained sound recognition model to fine-tune it to their environments. Through quantitative experiments and field evaluations with 12 DHH users, we show that AdaptiveSound can achieve a significantly higher accuracy (+14.6\%) than prior state-of-the art systems in diverse real-world locations (e.g., homes, parks, streets, and malls) with little end-user effort (about 10 minutes of feedback).","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Deaf, Human-AI, applied machine learning, audio event detection, deaf, hard of hearing, human-in-the-loop, incremental learning, reinforcement learning, sound awareness', 'numpages': '12', 'articleno': '18', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sound recognition tools have wide-ranging impacts for deaf and hard of hearing (DHH) people from being informed of safety-critical information (e.g., fire alarms, sirens) to more mundane but still useful information (e.g., door knock, microwave beeps). However, prior sound recognition systems use models that are pre-trained on generic sound datasets and do not adapt well to diverse variations of real-world sounds. We introduce AdaptiveSound, a real-time system for portable devices (e.g., smartphones) that allows DHH users to provide corrective feedback to the sound recognition model to adapt the model to diverse acoustic environments. AdaptiveSound is informed by prior surveys of sound recognition systems, where DHH users strongly desired the ability to provide feedback to a pre-trained sound recognition model to fine-tune it to their environments. Through quantitative experiments and field evaluations with 12 DHH users, we show that AdaptiveSound can achieve a significantly higher accuracy (+14.6\\%) than prior state-of-the art systems in diverse real-world locations (e.g., homes, parks, streets, and malls) with little end-user effort (about 10 minutes of feedback).', 'doi': '10.1145/3597638.3608390', 'url': 'https://doi.org/10.1145/3597638.3608390', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'AdaptiveSound: An Interactive Feedback-Loop System to Improve Sound Recognition for Deaf and Hard of Hearing Users', 'author': 'Do, Hang and Dang, Quan and Huang, Jeremy Zhengqi and Jain, Dhruv', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608390'}"
Understanding Noise Sensitivity through Interactions in Two Online Autism Forums,10.1145/3597638.3608413,"Noise sensitivity is challenging for many people and particularly prevalent for autistic people. In an effort to understand these experiences from the perspective of autistic people and the opportunities for assistive technologies to support noise-sensitive people, we analyzed the posts and responses of noise-sensitive autistic people and their allies in two online forums. These forums were intentionally chosen for their inclusion and centering of autistic people’s experiences as we strove to understand this space from their perspective. Our analysis of both original discussion forum posts about noise sensitivity and comments in response to them reveal three themes: sharing experiences of noise sensitivity, managing noise sensitivity, and disclosing autism. Our work contributes a foundational understanding of the lived experiences of autistic people who are noise-sensitive and the implications for the design of assistive technology to support noise sensitivity.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Assistive Technology, Autism, Noise Sensitivity, Online Communities', 'numpages': '12', 'articleno': '19', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Noise sensitivity is challenging for many people and particularly prevalent for autistic people. In an effort to understand these experiences from the perspective of autistic people and the opportunities for assistive technologies to support noise-sensitive people, we analyzed the posts and responses of noise-sensitive autistic people and their allies in two online forums. These forums were intentionally chosen for their inclusion and centering of autistic people’s experiences as we strove to understand this space from their perspective. Our analysis of both original discussion forum posts about noise sensitivity and comments in response to them reveal three themes: sharing experiences of noise sensitivity, managing noise sensitivity, and disclosing autism. Our work contributes a foundational understanding of the lived experiences of autistic people who are noise-sensitive and the implications for the design of assistive technology to support noise sensitivity.', 'doi': '10.1145/3597638.3608413', 'url': 'https://doi.org/10.1145/3597638.3608413', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Noise Sensitivity through Interactions in Two Online Autism Forums', 'author': 'Dotch, Emani and Johnson, Jazette and Black, Rebecca W. and Hayes, Gillian R', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608413'}"
A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE,10.1145/3597638.3608412,"Technologies for blind and low-vision (BLV) people have long been a focus of Human-Computer Interaction (HCI) and accessibility (ASSETS) research. To map and assess this cross-disciplinary field, prior literature reviews have focused on specific BLV research areas (e.g., navigation assistance) or study methodologies (e.g., qualitative methods). In this paper, we provide a more holistic examination, combining both quantitative bibliometric analyses with qualitative assessments. Using keyword queries of terms focused on the human (e.g., people) and their visual status (e.g., blind, low-vision), we first derived a dataset of 880 papers published between 2010-2022 from ACM and IEEE conferences and journals. We then apply a programmatic analysis of this dataset followed by a qualitative analysis of the 100 most-cited papers. Our findings highlight four major research areas: Accessibility at Home \&amp; on the Go, Non-Visual Interaction, Orientation \&amp; Mobility, and Education. We also capture the diversity of denominations used to refer to the BLV community and their co-occurrences, as well as computer systems targeting both blind and low-vision users with a focus on visual substitution. We close by suggesting areas for future work and hope to stimulate discussions in our field.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'bibliographic coupling, blind, low-vision, systematic review, visual impairment', 'numpages': '20', 'articleno': '20', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Technologies for blind and low-vision (BLV) people have long been a focus of Human-Computer Interaction (HCI) and accessibility (ASSETS) research. To map and assess this cross-disciplinary field, prior literature reviews have focused on specific BLV research areas (e.g., navigation assistance) or study methodologies (e.g., qualitative methods). In this paper, we provide a more holistic examination, combining both quantitative bibliometric analyses with qualitative assessments. Using keyword queries of terms focused on the human (e.g., people) and their visual status (e.g., blind, low-vision), we first derived a dataset of 880 papers published between 2010-2022 from ACM and IEEE conferences and journals. We then apply a programmatic analysis of this dataset followed by a qualitative analysis of the 100 most-cited papers. Our findings highlight four major research areas: Accessibility at Home \\&amp; on the Go, Non-Visual Interaction, Orientation \\&amp; Mobility, and Education. We also capture the diversity of denominations used to refer to the BLV community and their co-occurrences, as well as computer systems targeting both blind and low-vision users with a focus on visual substitution. We close by suggesting areas for future work and hope to stimulate discussions in our field.', 'doi': '10.1145/3597638.3608412', 'url': 'https://doi.org/10.1145/3597638.3608412', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Large-Scale Mixed-Methods Analysis of Blind and Low-vision Research in ACM and IEEE', 'author': 'Thoo, Yong-Joon and Jeanneret Medina, Maximiliano and Froehlich, Jon E. and Ruffieux, Nicolas and Lalanne, Denis', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608412'}"
User Perceptions and Preferences for Online Surveys in American Sign Language: An Exploratory Study,10.1145/3597638.3608444,"In order to gather data from the signing deaf community, efforts have been made to create online surveys in American Sign Language (ASL), despite a lack of user studies and UX/UI design guidelines informing the development of online survey tools featuring ASL. In this paper, we present SL-Surveys, an ASL-centric survey tool prototype showcasing a set of potential designs for multiple-choice, scalar, and multi-select questions. SL-Surveys was developed in an iterative process expressly for an exploratory think-aloud study investigating user experiences and perceptions of the designs. This preliminary study was conducted with seven deaf ASL-signing participants using a computer. The new design process, prototypes, user study and results make important strides towards a future where designs are not constrained by existing standards and practices based on written languages.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '17', 'articleno': '21', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In order to gather data from the signing deaf community, efforts have been made to create online surveys in American Sign Language (ASL), despite a lack of user studies and UX/UI design guidelines informing the development of online survey tools featuring ASL. In this paper, we present SL-Surveys, an ASL-centric survey tool prototype showcasing a set of potential designs for multiple-choice, scalar, and multi-select questions. SL-Surveys was developed in an iterative process expressly for an exploratory think-aloud study investigating user experiences and perceptions of the designs. This preliminary study was conducted with seven deaf ASL-signing participants using a computer. The new design process, prototypes, user study and results make important strides towards a future where designs are not constrained by existing standards and practices based on written languages.', 'doi': '10.1145/3597638.3608444', 'url': 'https://doi.org/10.1145/3597638.3608444', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'User Perceptions and Preferences for Online Surveys in American Sign Language: An Exploratory Study', 'author': 'Boll, Rachel and Mahajan, Shruti and Burke, Tish and Alkhudaidi, Khulood and Henriques, Brittany and Cordova, Isabelle and Walker, Zoey and Solovey, Erin and Reis, Jeanne', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608444'}"
Blending Accessibility in UI Framework Documentation to Build Awareness,10.1145/3597638.3608380,"The lack of accessibility awareness among industry professionals is one of the reasons for rampant inaccessible websites and applications. This problem is exacerbated by the industry norm of having a single place dedicated to accessibility in the documentation of UI frameworks, which makes accessibility difficult for developers to discover and implement as part of their workflows. This paper presents the Blended Approach (BA), a novel approach and framework for improving accessibility awareness through documentation. Unlike the conventional practice, it recommends sprinkling and repeating short snippets on accessibility throughout the documentation while linking developers to detailed explanations on the dedicated accessibility page. Thus, BA places the topic of accessibility on an equal footing as other common programming concerns such as performance, security, and UX. As a case study, we applied BA to the onboarding tutorial of Flutter, a popular UI toolkit. The positive feedback we received in our evaluation with 11 professional developers suggests BA can be a viable and effective approach.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'UI development, accessibility, documentation, programming, software developers', 'numpages': '12', 'articleno': '22', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The lack of accessibility awareness among industry professionals is one of the reasons for rampant inaccessible websites and applications. This problem is exacerbated by the industry norm of having a single place dedicated to accessibility in the documentation of UI frameworks, which makes accessibility difficult for developers to discover and implement as part of their workflows. This paper presents the Blended Approach (BA), a novel approach and framework for improving accessibility awareness through documentation. Unlike the conventional practice, it recommends sprinkling and repeating short snippets on accessibility throughout the documentation while linking developers to detailed explanations on the dedicated accessibility page. Thus, BA places the topic of accessibility on an equal footing as other common programming concerns such as performance, security, and UX. As a case study, we applied BA to the onboarding tutorial of Flutter, a popular UI toolkit. The positive feedback we received in our evaluation with 11 professional developers suggests BA can be a viable and effective approach.', 'doi': '10.1145/3597638.3608380', 'url': 'https://doi.org/10.1145/3597638.3608380', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Blending Accessibility in UI Framework Documentation to Build Awareness', 'author': 'Pandey, Maulishree and Dong, Tao', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608380'}"
Screen Magnification for Readers with Low Vision: A Study on Usability and Performance,10.1145/3597638.3608383,"We present a study with 20 participants with low vision who operated two types of screen magnification (lens and full) on a laptop computer to read two types of document (text and web page). Our purposes were to comparatively assess the two magnification modalities, and to obtain some insight into how people with low vision use the mouse to control the center of magnification. These observations may inform the design of systems for the automatic control of the center of magnification. Our results show that there were no significant differences in reading performances or in subjective preferences between the two magnification modes. However, when using the lens mode, our participants adopted more consistent and uniform mouse motion patterns, while longer and more frequent pauses and shorter overall path lengths were measured using the full mode. Analysis of the distribution of gaze points (as measured by a gaze tracker) using the full mode shows that, when reading a text document, most participants preferred to move the area of interest to a specific region of the screen.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'reading, screen magnification, visual impairment', 'numpages': '15', 'articleno': '23', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present a study with 20 participants with low vision who operated two types of screen magnification (lens and full) on a laptop computer to read two types of document (text and web page). Our purposes were to comparatively assess the two magnification modalities, and to obtain some insight into how people with low vision use the mouse to control the center of magnification. These observations may inform the design of systems for the automatic control of the center of magnification. Our results show that there were no significant differences in reading performances or in subjective preferences between the two magnification modes. However, when using the lens mode, our participants adopted more consistent and uniform mouse motion patterns, while longer and more frequent pauses and shorter overall path lengths were measured using the full mode. Analysis of the distribution of gaze points (as measured by a gaze tracker) using the full mode shows that, when reading a text document, most participants preferred to move the area of interest to a specific region of the screen.', 'doi': '10.1145/3597638.3608383', 'url': 'https://doi.org/10.1145/3597638.3608383', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Screen Magnification for Readers with Low Vision: A Study on Usability and Performance', 'author': 'Tang, Meini and Manduchi, Roberto and Chung, Susana and Prado, Raquel', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608383'}"
30 Years of Solving the Wrong Problem: How Recolouring Tool Design Fails those with Colour Vision Deficiency,10.1145/3597638.3608407,"By far the most available assistive tool to assist people with Colour Vision Deficiency (CVD) is recolouring. However, despite over three decades of research designing and developing recolouring tools, no work yet has looked to understand how these tools are actually used or perceived by people with CVD. To address this, we first analyzed posts and comments from the r/colorblind subreddit in order to get an unconstrained understanding of the perspectives people with CVD have on recolouring tools. We then conducted an observation study with follow-on interview to further understand how people with CVD use and perceive these tools in a more controlled setting. Our findings suggest that recolouring tools are rarely used how their designers intended and that a better future solution is to design with true inclusivity by solving the actual problems people with CVD have, rather than attempting to ‘fix’ CVD.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Colour Filter, Colour Vision Deficiency, Daltonization, Recolouring', 'numpages': '13', 'articleno': '24', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'By far the most available assistive tool to assist people with Colour Vision Deficiency (CVD) is recolouring. However, despite over three decades of research designing and developing recolouring tools, no work yet has looked to understand how these tools are actually used or perceived by people with CVD. To address this, we first analyzed posts and comments from the r/colorblind subreddit in order to get an unconstrained understanding of the perspectives people with CVD have on recolouring tools. We then conducted an observation study with follow-on interview to further understand how people with CVD use and perceive these tools in a more controlled setting. Our findings suggest that recolouring tools are rarely used how their designers intended and that a better future solution is to design with true inclusivity by solving the actual problems people with CVD have, rather than attempting to ‘fix’ CVD.', 'doi': '10.1145/3597638.3608407', 'url': 'https://doi.org/10.1145/3597638.3608407', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '30 Years of Solving the Wrong Problem: How Recolouring Tool Design Fails those with Colour Vision Deficiency', 'author': 'Geddes, Connor and Flatla, David R. and Connelly, Ciabhan L.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608407'}"
Cripping Data Visualizations: Crip Technoscience as a Critical Lens for Designing Digital Access,10.1145/3597638.3608427,"Data visualizations have become the primary mechanism for engaging with quantitative information. However, many of these visualizations are inaccessible to blind and low vision people. This paper investigates the challenge of designing accessible data visualizations through the lens of crip technoscience. We present four speculative design case studies that conceptually explore four qualities of access built on crip wisdom: access as an ongoing process, a frictional practice, an aesthetic experience, and transformation. Each speculative study embodies inquiry and futuring, making visible common assumptions about access and exploring how an alternative crip-informed framework can shape designs that foreground the creativity of disabled people. We end by presenting tactics for designing digital access that de-centers the innovation discourse.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, blind and low vision, crip theory, data visualizations', 'numpages': '16', 'articleno': '25', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Data visualizations have become the primary mechanism for engaging with quantitative information. However, many of these visualizations are inaccessible to blind and low vision people. This paper investigates the challenge of designing accessible data visualizations through the lens of crip technoscience. We present four speculative design case studies that conceptually explore four qualities of access built on crip wisdom: access as an ongoing process, a frictional practice, an aesthetic experience, and transformation. Each speculative study embodies inquiry and futuring, making visible common assumptions about access and exploring how an alternative crip-informed framework can shape designs that foreground the creativity of disabled people. We end by presenting tactics for designing digital access that de-centers the innovation discourse.', 'doi': '10.1145/3597638.3608427', 'url': 'https://doi.org/10.1145/3597638.3608427', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Cripping Data Visualizations: Crip Technoscience as a Critical Lens for Designing Digital Access', 'author': 'Hsueh, Stacy and Vincenzi, Beatrice and Murdeshwar, Akshata and Ciolfi Felice, Marianela', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608427'}"
"Working at the Intersection of Race, Disability and Accessibility",10.1145/3597638.3608389,"Examinations of intersectionality and identity dimensions in accessibility research have primarily considered disability separately from a person’s race and ethnicity. Accessibility work often does not include considerations of race as a construct, or treats race as a shallow demographic variable, if race is mentioned at all. The lack of attention to race as a construct in accessibility research presents an oversight in our field, often systematically eliminating whole areas of need and vital perspectives from the work we do. Further, there has been little focus on the intersection of race and disability within accessibility research, and the relevance of their interplay. When research in race or disability does not mention the other, this work overlooks the potential to better understand the full nuance of marginalized and “otherized” groups. To address this gap, we present a series of case studies exploring the potential for research that lies at the intersection of race and disability. We provide examples of how to integrate racial equity perspectives into accessibility research, through positive examples found in these case studies and reflect on teaching at the intersection of race, disability, and technology. This paper highlights the value of considering how constructs of race and disability work alongside each other within accessibility research studies, designs of socio-technical systems, and education. Our analysis provides recommendations towards establishing this research direction.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Disability, Inclusion, Intersectionality, Race', 'numpages': '18', 'articleno': '26', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Examinations of intersectionality and identity dimensions in accessibility research have primarily considered disability separately from a person’s race and ethnicity. Accessibility work often does not include considerations of race as a construct, or treats race as a shallow demographic variable, if race is mentioned at all. The lack of attention to race as a construct in accessibility research presents an oversight in our field, often systematically eliminating whole areas of need and vital perspectives from the work we do. Further, there has been little focus on the intersection of race and disability within accessibility research, and the relevance of their interplay. When research in race or disability does not mention the other, this work overlooks the potential to better understand the full nuance of marginalized and “otherized” groups. To address this gap, we present a series of case studies exploring the potential for research that lies at the intersection of race and disability. We provide examples of how to integrate racial equity perspectives into accessibility research, through positive examples found in these case studies and reflect on teaching at the intersection of race, disability, and technology. This paper highlights the value of considering how constructs of race and disability work alongside each other within accessibility research studies, designs of socio-technical systems, and education. Our analysis provides recommendations towards establishing this research direction.', 'doi': '10.1145/3597638.3608389', 'url': 'https://doi.org/10.1145/3597638.3608389', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Working at the Intersection of Race, Disability and Accessibility', 'author': 'Harrington, Christina N. and Desai, Aashaka and Lewis, Aaleyah and Moharana, Sanika and Ross, Anne Spencer and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608389'}"
"Complex Dynamics: Disability, Assistive Technology, and the LGBTQIA+ Community Center Experience in the United States",10.1145/3597638.3608401,"In this study, we explore the experiences of LGBTQIA+ individuals with disabilities in community centers in the United States, with a focus on the role of assistive technology (AT). Our research addresses three key questions: (1) How do LGBTQIA+ individuals with disabilities navigate intersecting identities in community centers? (2) How do social connections impact AT use in these environments? (3) How do community center norms and structures influence AT use? Through 11 semi-structured interviews, we examine the challenges and barriers faced by people with disabilities, their motivations for visiting centers, the impact of social dynamics, structures, and norms on AT use, and the importance of social connections. To address these challenges and foster lasting change, we offer recommendations for designing more inclusive and affordable AT, nurturing interdependence, and promoting collaboration between LGBTQIA+ community centers and disability organizations.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Assistive Technology, Community Centers and Spaces, Disability, Intersectionality, LGBTQIA+, Marginalized Communities, Qualitative Research, Stigma, Thematic Analysis', 'numpages': '15', 'articleno': '27', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this study, we explore the experiences of LGBTQIA+ individuals with disabilities in community centers in the United States, with a focus on the role of assistive technology (AT). Our research addresses three key questions: (1) How do LGBTQIA+ individuals with disabilities navigate intersecting identities in community centers? (2) How do social connections impact AT use in these environments? (3) How do community center norms and structures influence AT use? Through 11 semi-structured interviews, we examine the challenges and barriers faced by people with disabilities, their motivations for visiting centers, the impact of social dynamics, structures, and norms on AT use, and the importance of social connections. To address these challenges and foster lasting change, we offer recommendations for designing more inclusive and affordable AT, nurturing interdependence, and promoting collaboration between LGBTQIA+ community centers and disability organizations.', 'doi': '10.1145/3597638.3608401', 'url': 'https://doi.org/10.1145/3597638.3608401', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Complex Dynamics: Disability, Assistive Technology, and the LGBTQIA+ Community Center Experience in the United States', 'author': 'Crawford, Kirk Andrew and Spiel, Katta and Hamidi, Foad', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608401'}"
Towards a Social Justice Aligned Makerspace: Co-designing Custom Assistive Technology within a University Ecosystem,10.1145/3597638.3608393,"Digital fabrication methods offer exciting opportunities for producing customized assistive technology (AT). However, utilizing these tools currently requires a high level of technical expertise as well as time and money investments. Furthermore, facilitating collaboration between end users and makers needs effective and inclusive approaches with shared language and support for asynchronous, dispersed communication of design requirements. While these Do-It-Yourself (DIY) approaches are shown to support end-user agency and furthering technology democratization, research has to yet explore how they can further align with social justice values and practices. We explored these possibilities by facilitating DIY-AT design with students with disabilities within a university makerspace. By explicitly encouraging participants to consider social justice issues important to them as they engaged in DIY-AT design, we studied the considerations and supports needed for facilitating flexible co-design activities and broader conversations about accessibility barriers at the university. Adopting a transdisciplinary approach, we offer lessons learned about the potential of co-designing DIY-ATs as a way to investigate questions of social justice, inclusion, and access in academic contexts.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': '3D printing, assistive technology, digital fabrication, higher education, makerspaces', 'numpages': '13', 'articleno': '28', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Digital fabrication methods offer exciting opportunities for producing customized assistive technology (AT). However, utilizing these tools currently requires a high level of technical expertise as well as time and money investments. Furthermore, facilitating collaboration between end users and makers needs effective and inclusive approaches with shared language and support for asynchronous, dispersed communication of design requirements. While these Do-It-Yourself (DIY) approaches are shown to support end-user agency and furthering technology democratization, research has to yet explore how they can further align with social justice values and practices. We explored these possibilities by facilitating DIY-AT design with students with disabilities within a university makerspace. By explicitly encouraging participants to consider social justice issues important to them as they engaged in DIY-AT design, we studied the considerations and supports needed for facilitating flexible co-design activities and broader conversations about accessibility barriers at the university. Adopting a transdisciplinary approach, we offer lessons learned about the potential of co-designing DIY-ATs as a way to investigate questions of social justice, inclusion, and access in academic contexts.', 'doi': '10.1145/3597638.3608393', 'url': 'https://doi.org/10.1145/3597638.3608393', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Towards a Social Justice Aligned Makerspace: Co-designing Custom Assistive Technology within a University Ecosystem', 'author': 'Higgins, Erin and Oliver, Zaria and Hamidi, Foad', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608393'}"
“You Can't Possibly Have ADHD”: Exploring Validation and Tensions around Diagnosis within Unbounded ADHD Social Media Communities,10.1145/3597638.3608400,"Despite a common misconception that Attention-Deficit Hyperactivity Disorder (ADHD) is a childhood disorder, symptoms often persist into adulthood and many adults remain undiagnosed for a variety of reasons. Increased adoption of social media such as Instagram, TikTok, and Twitter has led to increased representation of neurodivergent community members with ADHD. As a result of the COVID-19 pandemic and increased social media use, many have come to recognize themselves in content made by social media members with ADHD and thus sought diagnoses of their own. Social media provides a valuable source of information, first-hand experiences, support, and validation through shared experiences. These networks, which we call “unbounded Online Health Communities”, have encouraged patients to advocate for themselves using information and support they have received from online ADHD communities. To understand these communities better and privilege the lived experiences of people with ADHD, we conduct a digital ethnography of three social media platforms to explore community content, specifically around acceptance, diagnoses, and tensions with the medical community. We discuss these informal online health communities as a source of knowledge, different, but no less important than that of traditional Online Health Communities and further the view of these communities as a valuable resource of shared expertise.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'ADHD, digital mental health, neurodivergence, online health communities, social media', 'numpages': '17', 'articleno': '29', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Despite a common misconception that Attention-Deficit Hyperactivity Disorder (ADHD) is a childhood disorder, symptoms often persist into adulthood and many adults remain undiagnosed for a variety of reasons. Increased adoption of social media such as Instagram, TikTok, and Twitter has led to increased representation of neurodivergent community members with ADHD. As a result of the COVID-19 pandemic and increased social media use, many have come to recognize themselves in content made by social media members with ADHD and thus sought diagnoses of their own. Social media provides a valuable source of information, first-hand experiences, support, and validation through shared experiences. These networks, which we call “unbounded Online Health Communities”, have encouraged patients to advocate for themselves using information and support they have received from online ADHD communities. To understand these communities better and privilege the lived experiences of people with ADHD, we conduct a digital ethnography of three social media platforms to explore community content, specifically around acceptance, diagnoses, and tensions with the medical community. We discuss these informal online health communities as a source of knowledge, different, but no less important than that of traditional Online Health Communities and further the view of these communities as a valuable resource of shared expertise.', 'doi': '10.1145/3597638.3608400', 'url': 'https://doi.org/10.1145/3597638.3608400', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""“You Can't Possibly Have ADHD”: Exploring Validation and Tensions around Diagnosis within Unbounded ADHD Social Media Communities"", 'author': 'Eagle, Tessa and Ringland, Kathryn E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608400'}"
What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study,10.1145/3597638.3608955,"Over the last decade there has been considerable research into how artificial intelligence (AI), specifically computer vision, can assist people who are blind or have low-vision (BLV) to understand their environment. However, there has been almost no research into whether the tasks (object detection, image captioning, text recognition etc.) and devices (smartphones, smart-glasses etc.) investigated by researchers align with the needs and preferences of BLV people. We identified 646 studies published in the last two and a half years that have investigated such assistive AI techniques. We analysed these papers to determine the task, device and participation by BLV individuals. We then interviewed 24 BLV people and asked for their top five AI-based applications and to rank the applications found in the literature. We found only a weak positive correlation between BLV participants’ perceived importance of tasks and researchers’ focus and that participants prefer conversational agent interface and head-mounted devices.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'augmented reality, mobile application, recognition, smart devices, virtual reality, wearable', 'numpages': '21', 'articleno': '30', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Over the last decade there has been considerable research into how artificial intelligence (AI), specifically computer vision, can assist people who are blind or have low-vision (BLV) to understand their environment. However, there has been almost no research into whether the tasks (object detection, image captioning, text recognition etc.) and devices (smartphones, smart-glasses etc.) investigated by researchers align with the needs and preferences of BLV people. We identified 646 studies published in the last two and a half years that have investigated such assistive AI techniques. We analysed these papers to determine the task, device and participation by BLV individuals. We then interviewed 24 BLV people and asked for their top five AI-based applications and to rank the applications found in the literature. We found only a weak positive correlation between BLV participants’ perceived importance of tasks and researchers’ focus and that participants prefer conversational agent interface and head-mounted devices.', 'doi': '10.1145/3597638.3608955', 'url': 'https://doi.org/10.1145/3597638.3608955', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study', 'author': 'Gamage, Bhanuka and Do, Thanh-Toan and Price, Nicholas Seow Chiang and Lowery, Arthur and Marriott, Kim', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608955'}"
Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision,10.1145/3597638.3608395,"The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs, as long as the effort of providing examples is balanced with the benefit of the personalization received. As an example, this paper presents the design and evaluation of Find My Things, an end-to-end application that can be taught by people who are blind or low vision to find their personal things. Through synthesis of the design process, this paper offers design considerations for the teaching loop that is so critical to realizing the power of teachable AI for accessibility.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Artificial Intelligence, Teachable AI', 'numpages': '12', 'articleno': '31', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The opportunity for artificial intelligence, or AI, to enable accessibility is rapidly growing, but widely impactful applications can be challenging to build given the diversity of user need within and across disability communities. Teachable AI systems give users with disabilities a way to leverage the power of AI to personalize applications for their own specific needs, as long as the effort of providing examples is balanced with the benefit of the personalization received. As an example, this paper presents the design and evaluation of Find My Things, an end-to-end application that can be taught by people who are blind or low vision to find their personal things. Through synthesis of the design process, this paper offers design considerations for the teaching loop that is so critical to realizing the power of teachable AI for accessibility.', 'doi': '10.1145/3597638.3608395', 'url': 'https://doi.org/10.1145/3597638.3608395', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision', 'author': 'Morrison, Cecily and Grayson, Martin and Marques, Rita Faia and Massiceti, Daniela and Longden, Camilla and Wen, Linda and Cutrell, Edward', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608395'}"
The Potential of a Visual Dialogue Agent In a Tandem Automated Audio Description System for Videos,10.1145/3597638.3608402,"The relentless pace of video production exacerbates the digital accessibility gap that individuals who are blind or low vision (BLV) face on a daily basis, resulting in disproportionate exclusion from community opportunities and risk management. Whereas previous automated audio description (AD) systems provide single-tool approaches for delivering minimum viable description (MVD) or delivering on-demand visual question answering (VQA), we present a tandem AI-based AD tool that combines MVD and on-demand VQA. A user study with 26 BLV individuals explored how the tandem system may be used under the conditions of delivering MVD and/or on-demand VQA with AI-only or human-in-the-loop support. When each tool was used in isolation, AI-only conditions scored significantly lower in both user enjoyment and comprehension. When used in tandem, AI-only conditions matched outcomes delivered with human-in-the-loop, which suggests that AI-only AD tools may be most effective when both types of tools are used in tandem. A multimodal analysis of interactions with the tandem system revealed areas for system improvement in terms of the timing of AD delivery and accurate content delivery. We discuss how the use of both types of tools in a tandem system can mitigate some of the digital frictions that have plagued efforts in machine learning and automated tools for accessibility.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'AI, Audio Description, Blind and Low Vision, Minimum Viable Description, Virtual Agents, Virtual Volunteer, Visual Assistance, Visual Dialogue, Visual Question Answering', 'numpages': '17', 'articleno': '32', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The relentless pace of video production exacerbates the digital accessibility gap that individuals who are blind or low vision (BLV) face on a daily basis, resulting in disproportionate exclusion from community opportunities and risk management. Whereas previous automated audio description (AD) systems provide single-tool approaches for delivering minimum viable description (MVD) or delivering on-demand visual question answering (VQA), we present a tandem AI-based AD tool that combines MVD and on-demand VQA. A user study with 26 BLV individuals explored how the tandem system may be used under the conditions of delivering MVD and/or on-demand VQA with AI-only or human-in-the-loop support. When each tool was used in isolation, AI-only conditions scored significantly lower in both user enjoyment and comprehension. When used in tandem, AI-only conditions matched outcomes delivered with human-in-the-loop, which suggests that AI-only AD tools may be most effective when both types of tools are used in tandem. A multimodal analysis of interactions with the tandem system revealed areas for system improvement in terms of the timing of AD delivery and accurate content delivery. We discuss how the use of both types of tools in a tandem system can mitigate some of the digital frictions that have plagued efforts in machine learning and automated tools for accessibility.', 'doi': '10.1145/3597638.3608402', 'url': 'https://doi.org/10.1145/3597638.3608402', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'The Potential of a Visual Dialogue Agent In a Tandem Automated Audio Description System for Videos', 'author': 'Stangl, Abigale and Ihorn, Shasta and Siu, Yue-Ting and Bodi, Aditya and Castanon, Mar and Narins, Lothar D and Yoon, Ilmi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608402'}"
Reimagining Machine Learning's Role in Assistive Technology by Co-Designing Exergames with Children Using a Participatory Machine Learning Design Probe,10.1145/3597638.3608421,"The paramount measure of success for a machine learning model has historically been predictive power and accuracy, but even a gold-standard accuracy benchmark fails when it inappropriately misrepresents a disabled or minority body. In this work, we reframe the role of machine learning as a provocation through a case study of participatory work co-creating exergames by employing machine learning and its training as a source of play and motivation rather than an accurate diagnostic tool for children with and without Sensory Based Motor Disorder. We created a design probe, Cirkus, that supports nearly any aminal locomotion exergame while collecting movement data for training a bespoke machine learning model. During 5 participatory workshops with a total of 30 children using Cirkus, we co-created a catalog of 17 exergames and a resulting machine-learning model. We discuss the potential implications of reframing machine learning’s role in Assistive Technology for values other than accuracy, share the challenges of using “messy” movement data from children with disabilities in an ever-changing co-creation context for training machine learning, and present broader implications of using machine learning in therapy games.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Designing with Children, Games, Participatory Machine Learning, Physical Therapy, Play, Sensory Based Motor Disorder', 'numpages': '16', 'articleno': '33', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The paramount measure of success for a machine learning model has historically been predictive power and accuracy, but even a gold-standard accuracy benchmark fails when it inappropriately misrepresents a disabled or minority body. In this work, we reframe the role of machine learning as a provocation through a case study of participatory work co-creating exergames by employing machine learning and its training as a source of play and motivation rather than an accurate diagnostic tool for children with and without Sensory Based Motor Disorder. We created a design probe, Cirkus, that supports nearly any aminal locomotion exergame while collecting movement data for training a bespoke machine learning model. During 5 participatory workshops with a total of 30 children using Cirkus, we co-created a catalog of 17 exergames and a resulting machine-learning model. We discuss the potential implications of reframing machine learning’s role in Assistive Technology for values other than accuracy, share the challenges of using “messy” movement data from children with disabilities in an ever-changing co-creation context for training machine learning, and present broader implications of using machine learning in therapy games.', 'doi': '10.1145/3597638.3608421', 'url': 'https://doi.org/10.1145/3597638.3608421', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""Reimagining Machine Learning's Role in Assistive Technology by Co-Designing Exergames with Children Using a Participatory Machine Learning Design Probe"", 'author': ""Duval, Jared and Turmo Vidal, Laia and M\\'{a}rquez Segura, Elena and Li, Yinchu and Waern, Annika"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608421'}"
The Sem-Lex Benchmark: Modeling ASL Signs and their Phonemes,10.1145/3597638.3608408,"Sign language recognition and translation technologies have the potential to increase access and inclusion of deaf signing communities, but research progress is bottlenecked by a lack of representative data. We introduce a new resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The Benchmark is the current largest of its kind, consisting of over 84k videos of isolated sign productions from deaf ASL signers who gave informed consent and received compensation. Human experts aligned these videos with other sign language resources including ASL-LEX, SignBank, and ASL Citizen, enabling useful expansions for sign and phonological feature recognition. We present a suite of experiments which make use of the linguistic information in ASL-LEX, evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated sign recognition (ISR). We use an SL-GCN model to show that the phonological features are recognizable with 85\% accuracy, and that they are effective as an auxiliary target to ISR. Learning to recognize phonological features alongside gloss results in a 6\% improvement for few-shot ISR accuracy and a 2\% improvement for ISR accuracy overall. Instructions for downloading the data can be found at https://github.com/leekezar/SemLex.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'american sign language, islr, phonology, sign language, sign recognition', 'numpages': '10', 'articleno': '34', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sign language recognition and translation technologies have the potential to increase access and inclusion of deaf signing communities, but research progress is bottlenecked by a lack of representative data. We introduce a new resource for American Sign Language (ASL) modeling, the Sem-Lex Benchmark. The Benchmark is the current largest of its kind, consisting of over 84k videos of isolated sign productions from deaf ASL signers who gave informed consent and received compensation. Human experts aligned these videos with other sign language resources including ASL-LEX, SignBank, and ASL Citizen, enabling useful expansions for sign and phonological feature recognition. We present a suite of experiments which make use of the linguistic information in ASL-LEX, evaluating the practicality and fairness of the Sem-Lex Benchmark for isolated sign recognition (ISR). We use an SL-GCN model to show that the phonological features are recognizable with 85\\% accuracy, and that they are effective as an auxiliary target to ISR. Learning to recognize phonological features alongside gloss results in a 6\\% improvement for few-shot ISR accuracy and a 2\\% improvement for ISR accuracy overall. Instructions for downloading the data can be found at https://github.com/leekezar/SemLex.', 'doi': '10.1145/3597638.3608408', 'url': 'https://doi.org/10.1145/3597638.3608408', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'The Sem-Lex Benchmark: Modeling ASL Signs and their Phonemes', 'author': 'Kezar, Lee and Thomason, Jesse and Caselli, Naomi and Sehyr, Zed and Pontecorvo, Elana', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608408'}"
Towards real-time and large-scale web accessbility,10.1145/3597638.3608403,"We rely on large-scale web accessibility evaluations to obtain snapshots of Internet Health and understand trends and behaviours impacting overall web accessibility. Such evaluations are financially and time exhaustive, making the possibility of more real-time measurements of Internet Health infeasible. In this paper, we investigate the impacts of optimising the page selection processes of large-scale web accessibility evaluations. We set out to conduct an automated accessibility evaluation of 1500 websites using the ‘Home+’ sampling method (for each website, we evaluated the home page and all pages linked belonging to the same domain) as our baseline; then compared the agreement rates of web accessibility evaluations on further sub-sampled datasets. Accessibility data was successfully captured on 987 websites. Our findings demonstrate that a strong accessibility evaluation agreement between the baseline and the sub-sample datasets could be reached with a sub-sample of just 20\% of the pages, significantly reducing the effort and resources required to conduct large-scale web accessibility evaluations.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Internet Health, Large-Scale Analysis, Web Accessibility', 'numpages': '9', 'articleno': '35', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We rely on large-scale web accessibility evaluations to obtain snapshots of Internet Health and understand trends and behaviours impacting overall web accessibility. Such evaluations are financially and time exhaustive, making the possibility of more real-time measurements of Internet Health infeasible. In this paper, we investigate the impacts of optimising the page selection processes of large-scale web accessibility evaluations. We set out to conduct an automated accessibility evaluation of 1500 websites using the ‘Home+’ sampling method (for each website, we evaluated the home page and all pages linked belonging to the same domain) as our baseline; then compared the agreement rates of web accessibility evaluations on further sub-sampled datasets. Accessibility data was successfully captured on 987 websites. Our findings demonstrate that a strong accessibility evaluation agreement between the baseline and the sub-sample datasets could be reached with a sub-sample of just 20\\% of the pages, significantly reducing the effort and resources required to conduct large-scale web accessibility evaluations.', 'doi': '10.1145/3597638.3608403', 'url': 'https://doi.org/10.1145/3597638.3608403', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Towards real-time and large-scale web accessbility', 'author': ""P. Carvalho, Lu\\'{\\i}s and Guerreiro, Tiago and Lawson, Shaun and Montague, Kyle"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608403'}"
The Eyes Have It: Visual Feedback Methods to Make Walking in Immersive Virtual Reality More Accessible for People With Mobility Impairments While Utilizing Head-Mounted Displays,10.1145/3597638.3608406,"The use of Head-Mounted Displays (HMDs) in Virtual Reality (VR) can cause gait disturbance problems for users because they are unable to see the real world while in VR. This is particularly challenging for individuals with mobility impairments who rely heavily on visual cues to maintain balance. The limited research that has been conducted on this issue has not focused on ways to solve it. IN this study, we investigated how different visual feedback methods affect walking patterns (i.e., gait) in VR. The study involved 50 participants, including 25 individuals with mobility impairments due to multiple sclerosis and 25 without mobility impairments. The participants completed timed walking tasks in both the real world and in VR environments that included various types of visual feedback, such as spatial, static, and rhythmic. The results showed that static and rhythmic visual feedback significantly improved gait performance in VR for people with mobility impairments compared to no visual feedback in VR. The results will help to make more accessible virtual environments for people with mobility impairments.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Head-Mounted Displays, Virtual reality, accessibility, gait disturbances, gait improvement, usability, visual feedback', 'numpages': '10', 'articleno': '36', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The use of Head-Mounted Displays (HMDs) in Virtual Reality (VR) can cause gait disturbance problems for users because they are unable to see the real world while in VR. This is particularly challenging for individuals with mobility impairments who rely heavily on visual cues to maintain balance. The limited research that has been conducted on this issue has not focused on ways to solve it. IN this study, we investigated how different visual feedback methods affect walking patterns (i.e., gait) in VR. The study involved 50 participants, including 25 individuals with mobility impairments due to multiple sclerosis and 25 without mobility impairments. The participants completed timed walking tasks in both the real world and in VR environments that included various types of visual feedback, such as spatial, static, and rhythmic. The results showed that static and rhythmic visual feedback significantly improved gait performance in VR for people with mobility impairments compared to no visual feedback in VR. The results will help to make more accessible virtual environments for people with mobility impairments.', 'doi': '10.1145/3597638.3608406', 'url': 'https://doi.org/10.1145/3597638.3608406', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'The Eyes Have It: Visual Feedback Methods to Make Walking in Immersive Virtual Reality More Accessible for People With Mobility Impairments While Utilizing Head-Mounted Displays', 'author': 'Mahmud, M. Rasel and Cordova, Alberto and Quarles, John', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608406'}"
Embodied Exploration: Facilitating Remote Accessibility Assessment for Wheelchair Users with Virtual Reality,10.1145/3597638.3608410,"Acquiring accessibility information about unfamiliar places in advance is essential for wheelchair users to make better decisions about physical visits. Today’s assessment approaches such as phone calls, photos/videos, or 360° virtual tours often fall short of providing the specific accessibility details needed for individual differences. For example, they may not reveal crucial information like whether the legroom underneath a table is spacious enough or if the spatial configuration of an appliance is convenient for wheelchair users. In response, we present Embodied Exploration, a Virtual Reality (VR) technique to deliver the experience of a physical visit while keeping the convenience of remote assessment. Embodied Exploration&nbsp;allows wheelchair users to explore high-fidelity digital replicas of physical environments with themselves embodied by avatars, leveraging the increasingly affordable VR headsets. With a preliminary exploratory study, we investigated the needs and iteratively refined our techniques. Through a real-world user study with six wheelchair users, we found Embodied Exploration&nbsp;is able to facilitate remote and accurate accessibility assessment. We also discuss design implications for embodiment, safety, and practicality.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Embodied Interaction, Mobility and Physical Disability, User-centered Design, Virtual Reality, Wheelchair Users', 'numpages': '17', 'articleno': '37', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Acquiring accessibility information about unfamiliar places in advance is essential for wheelchair users to make better decisions about physical visits. Today’s assessment approaches such as phone calls, photos/videos, or 360° virtual tours often fall short of providing the specific accessibility details needed for individual differences. For example, they may not reveal crucial information like whether the legroom underneath a table is spacious enough or if the spatial configuration of an appliance is convenient for wheelchair users. In response, we present Embodied Exploration, a Virtual Reality (VR) technique to deliver the experience of a physical visit while keeping the convenience of remote assessment. Embodied Exploration&nbsp;allows wheelchair users to explore high-fidelity digital replicas of physical environments with themselves embodied by avatars, leveraging the increasingly affordable VR headsets. With a preliminary exploratory study, we investigated the needs and iteratively refined our techniques. Through a real-world user study with six wheelchair users, we found Embodied Exploration&nbsp;is able to facilitate remote and accurate accessibility assessment. We also discuss design implications for embodiment, safety, and practicality.', 'doi': '10.1145/3597638.3608410', 'url': 'https://doi.org/10.1145/3597638.3608410', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Embodied Exploration: Facilitating Remote Accessibility Assessment for Wheelchair Users with Virtual Reality', 'author': 'Pei, Siyou and Chen, Alexander and Chen, Chen and Li, Franklin Mingzhe and Fozzard, Megan and Chi, Hao-Yun and Weibel, Nadir and Carrington, Patrick and Zhang, Yang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608410'}"
“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People,10.1145/3597638.3608386,"As social VR applications grow in popularity, blind and low vision users encounter continued accessibility barriers. Yet social VR, which enables multiple people to engage in the same virtual space, presents a unique opportunity to allow other people to support a user’s access needs. To explore this opportunity, we designed a framework based on physical sighted guidance that enables a guide to support a blind or low vision user with navigation and visual interpretation. A user can virtually hold on to their guide and move with them, while the guide can describe the environment. We studied the use of our framework with 16 blind and low vision participants and found that they had a wide range of preferences. For example, we found that participants wanted to use their guide to support social interactions and establish a human connection with a human-appearing guide. We also highlight opportunities for novel guidance abilities in VR, such as dynamically altering an inaccessible environment. Through this work, we open a novel design space for a versatile approach for making VR fully accessible.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'blind and low vision, sighted guide, social virtual reality', 'numpages': '14', 'articleno': '38', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'As social VR applications grow in popularity, blind and low vision users encounter continued accessibility barriers. Yet social VR, which enables multiple people to engage in the same virtual space, presents a unique opportunity to allow other people to support a user’s access needs. To explore this opportunity, we designed a framework based on physical sighted guidance that enables a guide to support a blind or low vision user with navigation and visual interpretation. A user can virtually hold on to their guide and move with them, while the guide can describe the environment. We studied the use of our framework with 16 blind and low vision participants and found that they had a wide range of preferences. For example, we found that participants wanted to use their guide to support social interactions and establish a human connection with a human-appearing guide. We also highlight opportunities for novel guidance abilities in VR, such as dynamically altering an inaccessible environment. Through this work, we open a novel design space for a versatile approach for making VR fully accessible.', 'doi': '10.1145/3597638.3608386', 'url': 'https://doi.org/10.1145/3597638.3608386', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“The Guide Has Your Back”: Exploring How Sighted Guides Can Enhance Accessibility in Social Virtual Reality for Blind and Low Vision People', 'author': 'Collins, Jazmin and Jung, Crescentia and Jang, Yeonju and Montour, Danielle and Won, Andrea Stevenson and Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608386'}"
Comparing Locomotion Techniques in Virtual Reality for People with Upper-Body Motor Impairments,10.1145/3597638.3608394,"Although virtual reality (VR) is becoming increasingly popular and many interaction techniques for navigating virtual environments, known as “locomotion techniques,” exist, data regarding the accessibility of locomotion techniques for people with upper-body motor impairments do not exist, making it difficult to understand which locomotion techniques work well for users and why. To address this gap, we conducted a study with 19 participants with upper-body motor impairments who completed a navigation task in VR using six seated locomotion techniques. We collected task performance data and elicited participant feedback using questionnaires and interviews. We found that participants performed similarly well with three techniques that required one controller input, little to no upper-body movement, and had a low perceived workload: Teleport, Astral Body, and Sliding Looking. However, Teleport was consistently favored in interview responses and could be considered the best technique for this group of participants. On the other hand, participants performed similarly poorly with three techniques that required arm, head, and torso movement and also rated these techniques as having a high workload: Chicken Acceleration, Grab and Pull, and Throw Teleport. However, participants did not necessarily prefer or want to use the techniques with which they performed best or had the lowest perceived workloads. Factors such as enjoyment, exercise, and presence sometimes outweighed accessibility. This finding suggests that accessibility alone should not override all other considerations when designing or recommending locomotion techniques to people with upper-body motor impairments, and that users with disabilities should have a range of accessible locomotion techniques to choose from based on their preferences.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '15', 'articleno': '39', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Although virtual reality (VR) is becoming increasingly popular and many interaction techniques for navigating virtual environments, known as “locomotion techniques,” exist, data regarding the accessibility of locomotion techniques for people with upper-body motor impairments do not exist, making it difficult to understand which locomotion techniques work well for users and why. To address this gap, we conducted a study with 19 participants with upper-body motor impairments who completed a navigation task in VR using six seated locomotion techniques. We collected task performance data and elicited participant feedback using questionnaires and interviews. We found that participants performed similarly well with three techniques that required one controller input, little to no upper-body movement, and had a low perceived workload: Teleport, Astral Body, and Sliding Looking. However, Teleport was consistently favored in interview responses and could be considered the best technique for this group of participants. On the other hand, participants performed similarly poorly with three techniques that required arm, head, and torso movement and also rated these techniques as having a high workload: Chicken Acceleration, Grab and Pull, and Throw Teleport. However, participants did not necessarily prefer or want to use the techniques with which they performed best or had the lowest perceived workloads. Factors such as enjoyment, exercise, and presence sometimes outweighed accessibility. This finding suggests that accessibility alone should not override all other considerations when designing or recommending locomotion techniques to people with upper-body motor impairments, and that users with disabilities should have a range of accessible locomotion techniques to choose from based on their preferences.', 'doi': '10.1145/3597638.3608394', 'url': 'https://doi.org/10.1145/3597638.3608394', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Comparing Locomotion Techniques in Virtual Reality for People with Upper-Body Motor Impairments', 'author': 'Franz, Rachel L. and Yu, Jinghan and Wobbrock, Jacob O.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608394'}"
A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities,10.1145/3597638.3608388,"People with disabilities (PWD) have shown a growing presence in the emerging social virtual reality (VR). To support disability representation, some social VR platforms start to involve disability features in avatar design. However, it is unclear how disability disclosure via avatars (and the way to present it) would affect PWD’s social experiences and interaction dynamics with others. To fill this gap, we conducted a diary study with 10 PWD who freely explored VRChat—a popular commercial social VR platform—for two weeks, comparing their experiences between using regular avatars and avatars with disability signifiers (i.e., avatar features that indicate the user’s disability in real life). We found that PWD preferred using avatars with disability signifiers and wanted to further enhance their aesthetics and interactivity. However, such avatars also caused embodied, explicit harassment targeting PWD. We revealed the unique factors that led to such harassment and derived design implications and protection mechanisms to inspire more safe and inclusive social VR.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Social virtual reality, avatar, diary study, disability disclosure, harassment, protection mechanisms', 'numpages': '17', 'articleno': '40', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People with disabilities (PWD) have shown a growing presence in the emerging social virtual reality (VR). To support disability representation, some social VR platforms start to involve disability features in avatar design. However, it is unclear how disability disclosure via avatars (and the way to present it) would affect PWD’s social experiences and interaction dynamics with others. To fill this gap, we conducted a diary study with 10 PWD who freely explored VRChat—a popular commercial social VR platform—for two weeks, comparing their experiences between using regular avatars and avatars with disability signifiers (i.e., avatar features that indicate the user’s disability in real life). We found that PWD preferred using avatars with disability signifiers and wanted to further enhance their aesthetics and interactivity. However, such avatars also caused embodied, explicit harassment targeting PWD. We revealed the unique factors that led to such harassment and derived design implications and protection mechanisms to inspire more safe and inclusive social VR.', 'doi': '10.1145/3597638.3608388', 'url': 'https://doi.org/10.1145/3597638.3608388', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities', 'author': 'Zhang, Kexin and Deldari, Elmira and Yao, Yaxing and Zhao, Yuhang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608388'}"
Experiences of Autistic Twitch Livestreamers: “I have made easily the most meaningful and impactful relationships”,10.1145/3597638.3608416,"We present perspectives from 10 autistic Twitch streamers regarding their experiences as livestreamers and how autism uniquely colors their experiences. Livestreaming offers a social online experience distinct from in-person, face-to-face communication, where autistic people tend to encounter challenges. Our reflexive thematic analysis of interviews with 10 participants showcases autistic livestreamers’ perspectives in their own words. Our findings center on the importance of having streamers establishing connections with other, sharing autistic identities, controlling a space for social interaction, personal growth, and accessibility challenges. In our discussion, we highlight the crucial value of having a medium for autistic representation, as well as design opportunities for streaming platforms to onboard autistic livestreamers and to facilitate livestreamers communication with their audience.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Twitch, autism, autistic, live streaming', 'numpages': '15', 'articleno': '41', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present perspectives from 10 autistic Twitch streamers regarding their experiences as livestreamers and how autism uniquely colors their experiences. Livestreaming offers a social online experience distinct from in-person, face-to-face communication, where autistic people tend to encounter challenges. Our reflexive thematic analysis of interviews with 10 participants showcases autistic livestreamers’ perspectives in their own words. Our findings center on the importance of having streamers establishing connections with other, sharing autistic identities, controlling a space for social interaction, personal growth, and accessibility challenges. In our discussion, we highlight the crucial value of having a medium for autistic representation, as well as design opportunities for streaming platforms to onboard autistic livestreamers and to facilitate livestreamers communication with their audience.', 'doi': '10.1145/3597638.3608416', 'url': 'https://doi.org/10.1145/3597638.3608416', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Experiences of Autistic Twitch Livestreamers: “I have made easily the most meaningful and impactful relationships”', 'author': 'Mok, Terrance and Tang, Anthony and McCrimmon, Adam and Oehlberg, Lora', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608416'}"
Exploring Community-Driven Descriptions for Making Livestreams Accessible,10.1145/3597638.3608425,"People watch livestreams to connect with others and learn about their hobbies. Livestreams feature multiple visual streams including the main video, webcams, on-screen overlays, and chat, all of which are inaccessible to livestream viewers with visual impairments. While prior work explores creating audio descriptions for recorded videos, live videos present new challenges: authoring descriptions in real-time, describing domain-specific content, and prioritizing which complex visual information to describe. We explore inviting livestream community members who are domain experts to provide live descriptions. We first conducted a study with 18 sighted livestream community members authoring descriptions for livestreams using three different description methods: live descriptions using text, live descriptions using speech, and asynchronous descriptions using text. We then conducted a study with 9 livestream community members with visual impairments, who shared their current strategies and challenges for watching livestreams and provided feedback on the community-written descriptions. We conclude with implications for improving the accessibility of livestreams.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Audio Descriptions, Blind and Low Vision, Live Video Streaming, Livestreaming, Visual Impairments', 'numpages': '13', 'articleno': '42', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People watch livestreams to connect with others and learn about their hobbies. Livestreams feature multiple visual streams including the main video, webcams, on-screen overlays, and chat, all of which are inaccessible to livestream viewers with visual impairments. While prior work explores creating audio descriptions for recorded videos, live videos present new challenges: authoring descriptions in real-time, describing domain-specific content, and prioritizing which complex visual information to describe. We explore inviting livestream community members who are domain experts to provide live descriptions. We first conducted a study with 18 sighted livestream community members authoring descriptions for livestreams using three different description methods: live descriptions using text, live descriptions using speech, and asynchronous descriptions using text. We then conducted a study with 9 livestream community members with visual impairments, who shared their current strategies and challenges for watching livestreams and provided feedback on the community-written descriptions. We conclude with implications for improving the accessibility of livestreams.', 'doi': '10.1145/3597638.3608425', 'url': 'https://doi.org/10.1145/3597638.3608425', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Exploring Community-Driven Descriptions for Making Livestreams Accessible', 'author': 'Killough, Daniel and Pavel, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608425'}"
Jod: Examining Design and Implementation of a Videoconferencing Platform for Mixed Hearing Groups,10.1145/3597638.3608382,"Videoconferencing usage has surged in recent years, but current platforms present significant accessibility barriers for the 430 million d/Deaf or hard of hearing people worldwide. Informed by prior work examining accessibility barriers in current videoconferencing platforms, we designed and developed Jod, a videoconferencing platform to facilitate communication in mixed hearing groups. Key features include support for customizing visual layouts and a notification system to request attention and influence behavior. Using Jod, we conducted six mixed hearing group sessions with 34 participants, including 18 d/Deaf or hard of hearing participants, 10 hearing participants, and 6 sign language interpreters. We found participants engaged in visual layout rearrangements based on their hearing ability and dynamically adapted to the changing group communication context, and that notifications were useful but raised a need for designs to cause fewer interruptions. We provide insights for future videoconferencing designs and conclude with recommendations for conducting mixed hearing studies.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Accessible Research Methods, DHH, Deaf, Hard of Hearing, Mixed Hearing Groups, Videoconferencing', 'numpages': '18', 'articleno': '43', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Videoconferencing usage has surged in recent years, but current platforms present significant accessibility barriers for the 430 million d/Deaf or hard of hearing people worldwide. Informed by prior work examining accessibility barriers in current videoconferencing platforms, we designed and developed Jod, a videoconferencing platform to facilitate communication in mixed hearing groups. Key features include support for customizing visual layouts and a notification system to request attention and influence behavior. Using Jod, we conducted six mixed hearing group sessions with 34 participants, including 18 d/Deaf or hard of hearing participants, 10 hearing participants, and 6 sign language interpreters. We found participants engaged in visual layout rearrangements based on their hearing ability and dynamically adapted to the changing group communication context, and that notifications were useful but raised a need for designs to cause fewer interruptions. We provide insights for future videoconferencing designs and conclude with recommendations for conducting mixed hearing studies.', 'doi': '10.1145/3597638.3608382', 'url': 'https://doi.org/10.1145/3597638.3608382', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Jod: Examining Design and Implementation of a Videoconferencing Platform for Mixed Hearing Groups', 'author': 'Mittal, Anant and Gupta, Meghna and Poddar, Roshni and Naik, Tarini and Kuppuraj, Seethalakshmi and Fogarty, James and Kumar, Pratyush and Jain, Mohit', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608382'}"
Improving the Accessibility of Screen-Shared Presentations by Enabling Concurrent Exploration,10.1145/3597638.3608411,"Presenters often screen-share slides over remote meetings as visual aids. Screen reader users however often do not have adequate ways to access slide content during presentations. To inform the design of more accessible interfaces, we ran a formative design workshop to elicit what screen reader users value during live screen-shared presentations, which are Prioritize Key Information, Reduce Cognitive Load, Provide Exploration Independence, Minimize Control Effort, and Encode Spatial Information. Based on these values, we created two prototypes that users experienced in a follow-up design probe. These interactions provide users control over what, when, and how slide elements are read out and introduce spatial audio separation between the presenter’s voice and screen reader audio. Participants found that a laptop-based prototype designed for the first four values significantly improved the accessibility of screen-shared content over existing tools, even with a presenter who was perceived to present materials accessibly. The importance of concurrent exploration is highlighted by participants’ frequent and diverse use of access to consume information complementary to the presenter, align focus, and understand information using alternative means. A phone-based prototype that encoded spatial information and greater exploration independence at the cost of increased cognitive load received mixed feedback, illustrating the balance between values that interaction design needs to strike. We conclude with insights and considerations for how the identified values and corresponding interactions can be used to improve the accessibility of screen-shared presentations.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Computer-Mediated Communication, Screen Sharing, Slide Presentations, Video Calling', 'numpages': '16', 'articleno': '44', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Presenters often screen-share slides over remote meetings as visual aids. Screen reader users however often do not have adequate ways to access slide content during presentations. To inform the design of more accessible interfaces, we ran a formative design workshop to elicit what screen reader users value during live screen-shared presentations, which are Prioritize Key Information, Reduce Cognitive Load, Provide Exploration Independence, Minimize Control Effort, and Encode Spatial Information. Based on these values, we created two prototypes that users experienced in a follow-up design probe. These interactions provide users control over what, when, and how slide elements are read out and introduce spatial audio separation between the presenter’s voice and screen reader audio. Participants found that a laptop-based prototype designed for the first four values significantly improved the accessibility of screen-shared content over existing tools, even with a presenter who was perceived to present materials accessibly. The importance of concurrent exploration is highlighted by participants’ frequent and diverse use of access to consume information complementary to the presenter, align focus, and understand information using alternative means. A phone-based prototype that encoded spatial information and greater exploration independence at the cost of increased cognitive load received mixed feedback, illustrating the balance between values that interaction design needs to strike. We conclude with insights and considerations for how the identified values and corresponding interactions can be used to improve the accessibility of screen-shared presentations.', 'doi': '10.1145/3597638.3608411', 'url': 'https://doi.org/10.1145/3597638.3608411', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Improving the Accessibility of Screen-Shared Presentations by Enabling Concurrent Exploration', 'author': 'Fan, Danyang and Junuzovic, Sasa and Tang, John and Jaeger, Thomas', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608411'}"
"“If I’m supposed to be the facilitator, I should be the host”: Understanding the Accessibility of Videoconferencing for Blind and Low Vision Meeting Facilitators",10.1145/3597638.3608420,"With remote work becoming a prevalent practice, the use of videoconferencing tools has significantly increased. While the accessibility of these tools in remote work has been studied previously, there is a lack of understanding of how professionals with disabilities conduct meetings using them and what accessibility means in this context. To fill this gap, we investigated the experience and accessibility practices of 18 blind and low vision (BLV) meeting facilitators who regularly use videoconferencing tools. Our findings reveal that BLV professionals undertake several steps to facilitate meetings effectively, including preparing materials in advance, ensuring the security of the meetings, maintaining awareness of attendees’ activity, coordinating with co-hosts to overcome accessibility obstacles, maintaining professionalism, and advocating for accessible meeting practices and technology. We discuss how our findings reveal barriers to career advancement for BLV professionals, help understand the interdependent activity of meeting facilitators and co-hosts, and provide recommendations for making videoconferencing more accessible.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, blind and low vision, career advancement, facilitators, hosts, remote work, videoconferencing', 'numpages': '14', 'articleno': '45', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'With remote work becoming a prevalent practice, the use of videoconferencing tools has significantly increased. While the accessibility of these tools in remote work has been studied previously, there is a lack of understanding of how professionals with disabilities conduct meetings using them and what accessibility means in this context. To fill this gap, we investigated the experience and accessibility practices of 18 blind and low vision (BLV) meeting facilitators who regularly use videoconferencing tools. Our findings reveal that BLV professionals undertake several steps to facilitate meetings effectively, including preparing materials in advance, ensuring the security of the meetings, maintaining awareness of attendees’ activity, coordinating with co-hosts to overcome accessibility obstacles, maintaining professionalism, and advocating for accessible meeting practices and technology. We discuss how our findings reveal barriers to career advancement for BLV professionals, help understand the interdependent activity of meeting facilitators and co-hosts, and provide recommendations for making videoconferencing more accessible.', 'doi': '10.1145/3597638.3608420', 'url': 'https://doi.org/10.1145/3597638.3608420', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“If I’m supposed to be the facilitator, I should be the host”: Understanding the Accessibility of Videoconferencing for Blind and Low Vision Meeting Facilitators', 'author': 'Akter, Taslima and Cha, Yoonha and Figueira, Isabela and Branham, Stacy M. and Piper, Anne Marie', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608420'}"
Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low-vision People,10.1145/3597638.3608423,"Being able to analyze and derive insights from data, which we call Daily Data Analysis (DDA), is an increasingly important skill in everyday life. While the accessibility community has explored ways to make data more accessible to blind and low-vision (BLV) people, little is known about how BLV people perform DDA. Knowing BLV people’s strategies and challenges in DDA would allow the community to make DDA more accessible to them. Toward this goal, we conducted a mixed-methods study of interviews and think-aloud sessions with BLV people (N=16). Our study revealed five key approaches for DDA (i.e., overview obtaining, column comparison, key statistics identification, note-taking, and data validation) and the associated challenges. We discussed the implications of our findings and highlighted potential directions to make DDA more accessible for BLV people.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'BLV, DDA, blind and low vision, daily data analysis, data accessibility, data exploration, interview, qualitative study, think-aloud', 'numpages': '15', 'articleno': '46', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Being able to analyze and derive insights from data, which we call Daily Data Analysis (DDA), is an increasingly important skill in everyday life. While the accessibility community has explored ways to make data more accessible to blind and low-vision (BLV) people, little is known about how BLV people perform DDA. Knowing BLV people’s strategies and challenges in DDA would allow the community to make DDA more accessible to them. Toward this goal, we conducted a mixed-methods study of interviews and think-aloud sessions with BLV people (N=16). Our study revealed five key approaches for DDA (i.e., overview obtaining, column comparison, key statistics identification, note-taking, and data validation) and the associated challenges. We discussed the implications of our findings and highlighted potential directions to make DDA more accessible for BLV people.', 'doi': '10.1145/3597638.3608423', 'url': 'https://doi.org/10.1145/3597638.3608423', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Strategies and Challenges of Conducting Daily Data Analysis (DDA) Among Blind and Low-vision People', 'author': 'Jiang, Chutian and Lei, Wentao and Kuang, Emily and Han, Teng and Fan, Mingming', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608423'}"
Developing and Deploying a Real-World Solution for Accessible Slide Reading and Authoring for Blind Users,10.1145/3597638.3608418,"Presentation software like Microsoft PowerPoint and Google Slides remains largely inaccessible for blind users because screen readers are not well suited to 2-D “artboards” that contain different objects in arbitrary arrangements lacking any inherent reading order. To investigate this problem, prior work by Zhang \&amp; Wobbrock (2023) developed multimodal interaction techniques in a prototype system called A11yBoard, but their system was limited to a single artboard in a self-contained prototype and was unable to support real-world use. In this work, we present a major extension of A11yBoard that expands upon its initial interaction techniques, addresses numerous real-world issues, and makes it deployable with Google Slides. We describe the new features developed for A11yBoard for Google Slides along with our participatory design process with a blind co-author. We also present two case studies based on real-world deployments showing that participants were able to independently complete slide reading and authoring tasks that were not possible without sighted assistance previously. We conclude with several design guidelines for making accessible digital content creation tools.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '15', 'articleno': '47', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Presentation software like Microsoft PowerPoint and Google Slides remains largely inaccessible for blind users because screen readers are not well suited to 2-D “artboards” that contain different objects in arbitrary arrangements lacking any inherent reading order. To investigate this problem, prior work by Zhang \\&amp; Wobbrock (2023) developed multimodal interaction techniques in a prototype system called A11yBoard, but their system was limited to a single artboard in a self-contained prototype and was unable to support real-world use. In this work, we present a major extension of A11yBoard that expands upon its initial interaction techniques, addresses numerous real-world issues, and makes it deployable with Google Slides. We describe the new features developed for A11yBoard for Google Slides along with our participatory design process with a blind co-author. We also present two case studies based on real-world deployments showing that participants were able to independently complete slide reading and authoring tasks that were not possible without sighted assistance previously. We conclude with several design guidelines for making accessible digital content creation tools.', 'doi': '10.1145/3597638.3608418', 'url': 'https://doi.org/10.1145/3597638.3608418', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Developing and Deploying a Real-World Solution for Accessible Slide Reading and Authoring for Blind Users', 'author': 'Zhang, Zhuohao and Kim, Gene S-H and Wobbrock, Jacob O.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608418'}"
Understanding Peer-to-Peer Instructional Support in an Online Community for Blind Audio Producers,10.1145/3597638.3608399,"Exchange of peer-to-peer support in online Q&amp;A communities plays an instrumental role in helping people learn and use complex software tools. While prior work has documented how disabled people support each other in finding accessible practices and workarounds in different contexts, research on understanding their participation in dedicated online Q&amp;A communities has been limited. Through the analysis of 180 conversation threads consisting of 1140 posts in an online text-based Q&amp;A community of blind and low-vision audio producers, we reveal various strategies members in this community use to formulate their queries and provide effective solutions regarding screen reader based navigation of complex graphical user interfaces. We reflect upon our findings to discuss the complexities blind and low-vision software users face in developing a shared understanding during collaborative troubleshooting through textual conversations and reimagine how online Q&amp;A platforms could enhance peer-to-peer instructional support among screen reader users.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Q&amp;A, audio production, blind, low vision, online forum, screen reader', 'numpages': '15', 'articleno': '48', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Exchange of peer-to-peer support in online Q&amp;A communities plays an instrumental role in helping people learn and use complex software tools. While prior work has documented how disabled people support each other in finding accessible practices and workarounds in different contexts, research on understanding their participation in dedicated online Q&amp;A communities has been limited. Through the analysis of 180 conversation threads consisting of 1140 posts in an online text-based Q&amp;A community of blind and low-vision audio producers, we reveal various strategies members in this community use to formulate their queries and provide effective solutions regarding screen reader based navigation of complex graphical user interfaces. We reflect upon our findings to discuss the complexities blind and low-vision software users face in developing a shared understanding during collaborative troubleshooting through textual conversations and reimagine how online Q&amp;A platforms could enhance peer-to-peer instructional support among screen reader users.', 'doi': '10.1145/3597638.3608399', 'url': 'https://doi.org/10.1145/3597638.3608399', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Peer-to-Peer Instructional Support in an Online Community for Blind Audio Producers', 'author': 'Saha, Abir and Gergle, Darren and Piper, Anne Marie', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608399'}"
Azimuth: Designing Accessible Dashboards for Screen Reader Users,10.1145/3597638.3608405,"Dashboards are frequently used to monitor and share data across a breadth of domains including business, finance, sports, public policy, and healthcare, just to name a few. The combination of different components (e.g., key performance indicators, charts, filtering widgets) and the interactivity between components makes dashboards powerful interfaces for data monitoring and analysis. However, these very characteristics also often make dashboards inaccessible to blind and low vision (BLV) users. Through a co-design study with two screen reader users, we investigate challenges faced by BLV users and identify design goals to support effective screen reader-based interactions with dashboards. Operationalizing the findings from the co-design process, we present a prototype system, Azimuth, that generates dashboards optimized for screen reader-based navigation along with complementary descriptions to support dashboard comprehension and interaction. Based on a follow-up study with five BLV participants, we showcase how our generated dashboards support BLV users and enable them to perform both targeted and open-ended analysis. Reflecting on our design process and study feedback, we discuss opportunities for future work on supporting interactive data analysis, understanding dashboard accessibility at scale, and investigating alternative devices and modalities for designing accessible visualization dashboards.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Dashboards, screen readers, text generation', 'numpages': '16', 'articleno': '49', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Dashboards are frequently used to monitor and share data across a breadth of domains including business, finance, sports, public policy, and healthcare, just to name a few. The combination of different components (e.g., key performance indicators, charts, filtering widgets) and the interactivity between components makes dashboards powerful interfaces for data monitoring and analysis. However, these very characteristics also often make dashboards inaccessible to blind and low vision (BLV) users. Through a co-design study with two screen reader users, we investigate challenges faced by BLV users and identify design goals to support effective screen reader-based interactions with dashboards. Operationalizing the findings from the co-design process, we present a prototype system, Azimuth, that generates dashboards optimized for screen reader-based navigation along with complementary descriptions to support dashboard comprehension and interaction. Based on a follow-up study with five BLV participants, we showcase how our generated dashboards support BLV users and enable them to perform both targeted and open-ended analysis. Reflecting on our design process and study feedback, we discuss opportunities for future work on supporting interactive data analysis, understanding dashboard accessibility at scale, and investigating alternative devices and modalities for designing accessible visualization dashboards.', 'doi': '10.1145/3597638.3608405', 'url': 'https://doi.org/10.1145/3597638.3608405', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Azimuth: Designing Accessible Dashboards for Screen Reader Users', 'author': 'Srinivasan, Arjun and Harshbarger, Tim and Hilliker, Darrell and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608405'}"
Beyond Audio Description: Exploring 360° Video Accessibility with Blind and Low Vision Users Through Collaborative Creation,10.1145/3597638.3608381,"While audio description (AD) is a standard method for making traditional videos more accessible to blind and low vision (BLV) users, we lack an understanding of how to make 360° videos accessible while preserving their immersive nature. Through individual interviews and collaborative design workshops, we explored ways to improve 360° video accessibility with immersion and engagement in mind. Our design workshops presented a unique opportunity for participants with diverse backgrounds to build on each others’ personal and professional experiences and collaboratively develop accessible 360° video prototypes. Participants included both AD creators and users, with a focus on BLV AD creators as their perspectives are underrepresented in prior work. We found that immersive video accessibility went beyond an extension of traditional video accessibility techniques. Participants valued accurate vocabulary and different points of view for descriptions, preferred a variety of presentation locations for spatialized AD, appreciated sound effects for setting the mood and subtly guiding, and wished to engage multiple senses to boost engagement. We conclude with implications for immersive media accessibility and future research directions to support disabled people as creators of access technology.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': '360° videos, audio description, blind and low vision, co-design, design workshop, video accessibility', 'numpages': '17', 'articleno': '50', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'While audio description (AD) is a standard method for making traditional videos more accessible to blind and low vision (BLV) users, we lack an understanding of how to make 360° videos accessible while preserving their immersive nature. Through individual interviews and collaborative design workshops, we explored ways to improve 360° video accessibility with immersion and engagement in mind. Our design workshops presented a unique opportunity for participants with diverse backgrounds to build on each others’ personal and professional experiences and collaboratively develop accessible 360° video prototypes. Participants included both AD creators and users, with a focus on BLV AD creators as their perspectives are underrepresented in prior work. We found that immersive video accessibility went beyond an extension of traditional video accessibility techniques. Participants valued accurate vocabulary and different points of view for descriptions, preferred a variety of presentation locations for spatialized AD, appreciated sound effects for setting the mood and subtly guiding, and wished to engage multiple senses to boost engagement. We conclude with implications for immersive media accessibility and future research directions to support disabled people as creators of access technology.', 'doi': '10.1145/3597638.3608381', 'url': 'https://doi.org/10.1145/3597638.3608381', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Beyond Audio Description: Exploring 360° Video Accessibility with Blind and Low Vision Users Through Collaborative Creation', 'author': 'Jiang, Lucy and Phutane, Mahika and Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608381'}"
Watch Your Language: Using Smartwatches to Support Communication,10.1145/3597638.3608379,"With an ageing population and increased prevalence of people living with complex communication needs there is a growing need to design scalable high-tech augmentative and alternative communication (AAC) apps to support agency and social participation. For end-users it is currently difficult to regulate the prominence of most mainstream high-tech AAC devices and tablet-based apps – they are socially conspicuous, offer poor portability, are aesthetically unconsidered, and obstruct vital non-verbal communication pathways. In response to this, we leverage participatory design techniques to design and evaluate two discreet and inconspicuous AAC smartwatch apps. We engage with a community of people living with the language impairment aphasia, to collaboratively build and iterate both a smartwatch app for ‘public’ communication: Watch Out and ‘private’ cognitive support: Watch In. Following this, we evaluate both apps during an experience prototyping workshop with an actor and subsequent focus group. We report results from communication interactions with both apps, interviews and feedback responses. Participants were not only successful in using both AAC smartwatch apps but, critically, the wearable and discreet intervention did not restrict users’ agency and non-verbal communication.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'AAC, Accessibility, Alternative and Augmentative Communication, Discreet and Wearable Devices, Smartwatches', 'numpages': '21', 'articleno': '51', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'With an ageing population and increased prevalence of people living with complex communication needs there is a growing need to design scalable high-tech augmentative and alternative communication (AAC) apps to support agency and social participation. For end-users it is currently difficult to regulate the prominence of most mainstream high-tech AAC devices and tablet-based apps – they are socially conspicuous, offer poor portability, are aesthetically unconsidered, and obstruct vital non-verbal communication pathways. In response to this, we leverage participatory design techniques to design and evaluate two discreet and inconspicuous AAC smartwatch apps. We engage with a community of people living with the language impairment aphasia, to collaboratively build and iterate both a smartwatch app for ‘public’ communication: Watch Out and ‘private’ cognitive support: Watch In. Following this, we evaluate both apps during an experience prototyping workshop with an actor and subsequent focus group. We report results from communication interactions with both apps, interviews and feedback responses. Participants were not only successful in using both AAC smartwatch apps but, critically, the wearable and discreet intervention did not restrict users’ agency and non-verbal communication.', 'doi': '10.1145/3597638.3608379', 'url': 'https://doi.org/10.1145/3597638.3608379', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Watch Your Language: Using Smartwatches to Support Communication', 'author': 'Curtis, Humphrey and Neate, Timothy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608379'}"
Screen or No Screen? Lessons Learnt from a Real-World Deployment Study of Using Voice Assistants With and Without Touchscreen for Older Adults,10.1145/3597638.3608378,"While voice user interfaces offer increased accessibility due to hands-free and eyes-free interactions, older adults often have challenges such as constructing structured requests and perceiving how such devices operate. Voice-first user interfaces have the potential to address these challenges by enabling multimodal interactions. Standalone voice + touchscreen Voice Assistants&nbsp;(VAs), such as Echo Show, are specific types of devices that adopt such interfaces and are gaining popularity. However, the affordances of the additional touchscreen for older adults are unknown. Through a 40-day real-world deployment with older adults living independently, we present a within-subjects study (N = 16; age M = 82.5, SD = 7.77, min. = 70, max. = 97) to understand how a built-in touchscreen might benefit older adults during device setup, conducting self-report diary survey, and general uses. We found that while participants appreciated the visual outputs, they still preferred to respond via speech instead of touch. We identified six design implications that can inform future innovations of senior-friendly VAs for managing healthcare and improving quality of life.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Older Adults, Real-World Deployment Study, Voice Assistants (VAs)', 'numpages': '21', 'articleno': '52', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'While voice user interfaces offer increased accessibility due to hands-free and eyes-free interactions, older adults often have challenges such as constructing structured requests and perceiving how such devices operate. Voice-first user interfaces have the potential to address these challenges by enabling multimodal interactions. Standalone voice + touchscreen Voice Assistants&nbsp;(VAs), such as Echo Show, are specific types of devices that adopt such interfaces and are gaining popularity. However, the affordances of the additional touchscreen for older adults are unknown. Through a 40-day real-world deployment with older adults living independently, we present a within-subjects study (N = 16; age M = 82.5, SD = 7.77, min. = 70, max. = 97) to understand how a built-in touchscreen might benefit older adults during device setup, conducting self-report diary survey, and general uses. We found that while participants appreciated the visual outputs, they still preferred to respond via speech instead of touch. We identified six design implications that can inform future innovations of senior-friendly VAs for managing healthcare and improving quality of life.', 'doi': '10.1145/3597638.3608378', 'url': 'https://doi.org/10.1145/3597638.3608378', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Screen or No Screen? Lessons Learnt from a Real-World Deployment Study of Using Voice Assistants With and Without Touchscreen for Older Adults', 'author': 'Chen, Chen and Lifset, Ella T and Han, Yichen and Roy, Arkajyoti and Hogarth, Michael and Moore, Alison A and Farcas, Emilia and Weibel, Nadir', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608378'}"
Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces,10.1145/3597638.3608404,"Conversational assistants, an inherently accessible mode of interaction for blind and low vision (BLV) individuals, offer opportunities to support nonvisual access in new ways. In this paper, we explore whether and how human-like conversations can support access to interfaces, advancing the impersonal linear access provided by screen readers today. We first interviewed 10 BLV participants about this approach, but found it difficult to situate conversations around a future technology. So we turned to a speculative design approach and created four audio fictions: pre-recorded dialogues between users and their hypothetical screen reader assistants wherein assistants assumed distinct roles: a friend, butler, expert, and caregiver. We presented the audio fictions to 14 BLV participants and found that personable conversations can meaningfully extend the screen reader experience. We observed a tension between AI adaptation and screen reader customization. Participants further expressed a need to maintain control at three distinct levels: granular cursor movement, screen representation, and task assistance. Through the lens of assistant roles, we address fundamental questions about anthropomorphizing CAs and assistive technology broadly.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'blind and low vision, conversational agents, design fiction, interviews, screen readers, voice assistants', 'numpages': '18', 'articleno': '53', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Conversational assistants, an inherently accessible mode of interaction for blind and low vision (BLV) individuals, offer opportunities to support nonvisual access in new ways. In this paper, we explore whether and how human-like conversations can support access to interfaces, advancing the impersonal linear access provided by screen readers today. We first interviewed 10 BLV participants about this approach, but found it difficult to situate conversations around a future technology. So we turned to a speculative design approach and created four audio fictions: pre-recorded dialogues between users and their hypothetical screen reader assistants wherein assistants assumed distinct roles: a friend, butler, expert, and caregiver. We presented the audio fictions to 14 BLV participants and found that personable conversations can meaningfully extend the screen reader experience. We observed a tension between AI adaptation and screen reader customization. Participants further expressed a need to maintain control at three distinct levels: granular cursor movement, screen representation, and task assistance. Through the lens of assistant roles, we address fundamental questions about anthropomorphizing CAs and assistive technology broadly.', 'doi': '10.1145/3597638.3608404', 'url': 'https://doi.org/10.1145/3597638.3608404', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Speaking with My Screen Reader: Using Audio Fictions to Explore Conversational Access to Interfaces', 'author': 'Phutane, Mahika and Jung, Crescentia and Chen, Niu and Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608404'}"
Investigating Day-to-day Experiences with Conversational Agents by Users with Traumatic Brain Injury,10.1145/3597638.3608385,"Traumatic brain injury (TBI) can cause cognitive, communication, and psychological challenges that profoundly limit independence in everyday life. Conversational Agents (CAs) can provide individuals with TBI with cognitive and communication support, although little is known about how they make use of CAs to address injury-related needs. In this study, we gave nine adults with TBI an at-home CA for four weeks to investigate use patterns, challenges, and design requirements, focusing particularly on injury-related use. The findings revealed significant gaps between the current capabilities of CAs and accessibility challenges faced by TBI users. We also identified 14 TBI-related activities that participants engaged in with CAs. We categorized those activities into four groups: mental health, cognitive activities, healthcare and rehabilitation, and routine activities. Design implications focus on accessibility improvements and functional designs of CAs that can better support the day-to-day needs of people with TBI.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Conversational agents, accessibility, traumatic brain injury, usability', 'numpages': '15', 'articleno': '54', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Traumatic brain injury (TBI) can cause cognitive, communication, and psychological challenges that profoundly limit independence in everyday life. Conversational Agents (CAs) can provide individuals with TBI with cognitive and communication support, although little is known about how they make use of CAs to address injury-related needs. In this study, we gave nine adults with TBI an at-home CA for four weeks to investigate use patterns, challenges, and design requirements, focusing particularly on injury-related use. The findings revealed significant gaps between the current capabilities of CAs and accessibility challenges faced by TBI users. We also identified 14 TBI-related activities that participants engaged in with CAs. We categorized those activities into four groups: mental health, cognitive activities, healthcare and rehabilitation, and routine activities. Design implications focus on accessibility improvements and functional designs of CAs that can better support the day-to-day needs of people with TBI.', 'doi': '10.1145/3597638.3608385', 'url': 'https://doi.org/10.1145/3597638.3608385', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Investigating Day-to-day Experiences with Conversational Agents by Users with Traumatic Brain Injury', 'author': ""Hu, Yaxin and Lim, Hajin and Johnson, Hailey L and O'Shaughnessy, Josephine M. and Kakonge, Lisa and Turkstra, Lyn and Duff, Melissa and Toma, Catalina and Mutlu, Bilge"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608385'}"
A feasibility study on the use of audio-based ecological momentary assessment with persons with aphasia,10.1145/3597638.3608419,"We describe a smartphone/smartwatch system to evaluate anomia in individuals with aphasia by using audio-recording-based ecological momentary assessments. The system delivers object-naming assessments to a participant's smartwatch, whereby a prompt signals the availability of images of these objects on the watch screen. Participants attempt to speak the names of the images that appear on the watch display out loud and into the watch as they go about their lives. We conducted a three-week feasibility study with six participants with mild to moderate aphasia. Participants were assigned to either a nine-item (four prompts per day with nine images) or single-item (36 prompts per day with one image each) ecological momentary assessment protocol. Compliance in recording an audio response to a prompt was approximately 80\% for both protocols. Qualitative analysis of the participants' interviews suggests that the participants felt capable of completing the protocol, but opinions about using a smartwatch were mixed. We review participant feedback and highlight the importance of considering a population's specific cognitive or motor impairments when designing technology and training protocols.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'anomia, aphasia, ecological momentary assessment, experience sampling, health research, microinteraction, smartwatch, stroke', 'numpages': '7', 'articleno': '55', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""We describe a smartphone/smartwatch system to evaluate anomia in individuals with aphasia by using audio-recording-based ecological momentary assessments. The system delivers object-naming assessments to a participant's smartwatch, whereby a prompt signals the availability of images of these objects on the watch screen. Participants attempt to speak the names of the images that appear on the watch display out loud and into the watch as they go about their lives. We conducted a three-week feasibility study with six participants with mild to moderate aphasia. Participants were assigned to either a nine-item (four prompts per day with nine images) or single-item (36 prompts per day with one image each) ecological momentary assessment protocol. Compliance in recording an audio response to a prompt was approximately 80\\% for both protocols. Qualitative analysis of the participants' interviews suggests that the participants felt capable of completing the protocol, but opinions about using a smartwatch were mixed. We review participant feedback and highlight the importance of considering a population's specific cognitive or motor impairments when designing technology and training protocols."", 'doi': '10.1145/3597638.3608419', 'url': 'https://doi.org/10.1145/3597638.3608419', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A feasibility study on the use of audio-based ecological momentary assessment with persons with aphasia', 'author': 'Hester, Jack and Le, Ha and Intille, Stephen and Meier, Erin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3608419'}"
SignIt! An Android Game for Sign Bilingual Play,10.1145/3597638.3614484,"The Deaf or Hard-of-Hearing (DHH) community constitutes over 430 million people globally, with about 70 million of them using sign language as their primary means of communication. Learning sign language poses significant challenges, and researchers have explored the potential of digital games as educational tools for teaching sign language. However, existing educational games are limited to a narrow range of words and feature only single-player modes. Consequently, our objective was to design a sign language-based game that not only facilitates learning but also enables social gameplay and encourages user-generated content. In this work, we present SignIt!, an accessible quiz platform co-designed with the DHH community. SignIt! empowers DHH players to play sign language-based quizzes either individually or in competitive settings, as well as to create their own quiz content. Through a user study involving five DHH participants, we found SignIt! was deemed easy-to-learn, intuitive, and accessible by our participants. They expressed various motivations for using SignIt!, including passing time, learning quiz content, and connecting with friends.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'creation, data, deaf, game, hard of hearing, learning, multiple-choice question', 'numpages': '4', 'articleno': '56', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The Deaf or Hard-of-Hearing (DHH) community constitutes over 430 million people globally, with about 70 million of them using sign language as their primary means of communication. Learning sign language poses significant challenges, and researchers have explored the potential of digital games as educational tools for teaching sign language. However, existing educational games are limited to a narrow range of words and feature only single-player modes. Consequently, our objective was to design a sign language-based game that not only facilitates learning but also enables social gameplay and encourages user-generated content. In this work, we present SignIt!, an accessible quiz platform co-designed with the DHH community. SignIt! empowers DHH players to play sign language-based quizzes either individually or in competitive settings, as well as to create their own quiz content. Through a user study involving five DHH participants, we found SignIt! was deemed easy-to-learn, intuitive, and accessible by our participants. They expressed various motivations for using SignIt!, including passing time, learning quiz content, and connecting with friends.', 'doi': '10.1145/3597638.3614484', 'url': 'https://doi.org/10.1145/3597638.3614484', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'SignIt! An Android Game for Sign Bilingual Play', 'author': 'Poddar, Roshni and YM, Pradyumna and Jayakumar, Divya Prabha and Naik, Tarini and Tripathi, Punyat and TP, Nabeel and Yeddula, Hemanth Reddy and Kumar, Pratyush and Jain, Mohit and Swaminathan, Manohar', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614484'}"
Practices and Barriers of Cooking Training for Blind and Low Vision People,10.1145/3597638.3614494,"Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'blind and low vision, cooking, kitchen, rehabilitation training', 'numpages': '5', 'articleno': '57', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Cooking is a vital yet challenging activity for blind and low vision (BLV) people, which involves many visual tasks that can be difficult and dangerous. BLV training services, such as vision rehabilitation, can effectively improve BLV people’s independence and quality of life in daily tasks, such as cooking. However, there is a lack of understanding on the practices employed by the training professionals and the barriers faced by BLV people in such training. To fill the gap, we interviewed six professionals to explore their training strategies and technology recommendations for BLV clients in cooking activities. Our findings revealed the fundamental principles, practices, and barriers in current BLV training services, identifying the gaps between training and reality.', 'doi': '10.1145/3597638.3614494', 'url': 'https://doi.org/10.1145/3597638.3614494', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Practices and Barriers of Cooking Training for Blind and Low Vision People', 'author': 'Wang, Ru and Zhou, Nihan and Nguyen, Tam and Mondal, Sanbrita and Mutlu, Bilge and Zhao, Yuhang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614494'}"
The Robot in Our Path: Investigating the Perceptions of People with Motor Disabilities on Navigating Public Space Alongside Sidewalk Robots,10.1145/3597638.3614508,"Sidewalk robots are becoming increasingly common worldwide, yet their operation on public walkways presents challenges for pedestrians. This is especially true for people with motor disabilities (PWMD) who already manage obstacles such as inadequate ramps and public incivility. The addition of sidewalk robots could further intensify these difficulties, which poses an urgent need to examine how the design of sidewalk robots may influence the daily navigation experiences of PWMD. This poster illustrates findings from semi-structured interviews with ten PWMD, providing insights into their perspectives on the presence of sidewalk robots. The study uncovers potential conflicts in shared sidewalk use and the adaptive actions PWMD described needing to undertake in response. Interviewees raised concerns about whether the robots could accommodate the needs of PWMD, as compared to people walking on foot, and the repercussions of any shortcomings in this regard. Our research also examines tensions stemming from different robotic design choices, indicating the necessity for more accessible public robot designs. We further delve into PWMD’s interaction needs and modalities for routine operation and in the event of robot malfunction. As cities increasingly allow for the deployment of robots in public spaces, this work seeks to inform equitable design and deployment guidelines for sidewalk robots and calls for further research into the implications of the rise of public robots for the diverse populations that make up any given municipality.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'delivery robots, human-robot interaction, public space, sidewalk robots', 'numpages': '6', 'articleno': '58', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sidewalk robots are becoming increasingly common worldwide, yet their operation on public walkways presents challenges for pedestrians. This is especially true for people with motor disabilities (PWMD) who already manage obstacles such as inadequate ramps and public incivility. The addition of sidewalk robots could further intensify these difficulties, which poses an urgent need to examine how the design of sidewalk robots may influence the daily navigation experiences of PWMD. This poster illustrates findings from semi-structured interviews with ten PWMD, providing insights into their perspectives on the presence of sidewalk robots. The study uncovers potential conflicts in shared sidewalk use and the adaptive actions PWMD described needing to undertake in response. Interviewees raised concerns about whether the robots could accommodate the needs of PWMD, as compared to people walking on foot, and the repercussions of any shortcomings in this regard. Our research also examines tensions stemming from different robotic design choices, indicating the necessity for more accessible public robot designs. We further delve into PWMD’s interaction needs and modalities for routine operation and in the event of robot malfunction. As cities increasingly allow for the deployment of robots in public spaces, this work seeks to inform equitable design and deployment guidelines for sidewalk robots and calls for further research into the implications of the rise of public robots for the diverse populations that make up any given municipality.', 'doi': '10.1145/3597638.3614508', 'url': 'https://doi.org/10.1145/3597638.3614508', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'The Robot in Our Path: Investigating the Perceptions of People with Motor Disabilities on Navigating Public Space Alongside Sidewalk Robots', 'author': 'Han, Howard and Li, Franklin Mingzhe and Martelaro, Nikolas and Byrne, Daragh and Fox, Sarah E', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614508'}"
Towards Gaze-Led Audio Description in Accessibility System for Architectural Heritage: Evidence from an Eye-Tracking Study,10.1145/3597638.3614509,"The paper presents empirical evidence in how differently art history experts and non-experts scan images of architectural objects. The study is a work-in-progress on a ubiquitous system enhancing accessibility to architectural heritage for a broad audience with a special focus on the Blind and Visually Impaired (BVI). The system provides city navigation for BVI along with the location-based real-time information and Gaze-Led Audio Descriptions (GLAD) about architectural artefacts. Gaze visualizations of non-experts are used to guide professional audio describers in shaping more natural, gaze-led narratives.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility systems, audio description, cultural heritage, eye tracking, visually impaired', 'numpages': '5', 'articleno': '59', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The paper presents empirical evidence in how differently art history experts and non-experts scan images of architectural objects. The study is a work-in-progress on a ubiquitous system enhancing accessibility to architectural heritage for a broad audience with a special focus on the Blind and Visually Impaired (BVI). The system provides city navigation for BVI along with the location-based real-time information and Gaze-Led Audio Descriptions (GLAD) about architectural artefacts. Gaze visualizations of non-experts are used to guide professional audio describers in shaping more natural, gaze-led narratives.', 'doi': '10.1145/3597638.3614509', 'url': 'https://doi.org/10.1145/3597638.3614509', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Towards Gaze-Led Audio Description in Accessibility System for Architectural Heritage: Evidence from an Eye-Tracking Study', 'author': 'Krejtz, Izabela and Paw\\l{}owska, Aneta and Milczarski, Piotr and Rutkowska-Siuda, Daria and H\\l{}oba\\.{z}, Artur and Wendorff, Anna and Wisiecka, Katarzyna and undefinedniegula, Anna and Duchowski, Andrew T. and Krejtz, Krzysztof', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614509'}"
A toolkit of approaches for digital mapping and correction of visual distortion,10.1145/3597638.3614510,"Visual distortion, known as metamorphopsia, is a serious visual deficit with no effective clinical treatment and which cannot be corrected by traditional optical glasses. In this paper, we introduce a toolkit of approaches for digitally mapping and correcting visual distortion, that might eventually be incorporated in a low vision aid VR headset. We describe three different approaches spanning data-driven and generative designs and leveraging either uniocular or binocular cues. We present our proposed demonstrator, our evaluation roadmap, and challenges for the field. Initial tests with simulated data demonstrate the effectiveness of the approach. Once clinically validated, we hope these approaches will enable accurate mapping of visual distortion and eventually lead to the development of ‘digital glasses’ capable of correcting the effects of metamorphopsia and restoring healthy vision.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Visual distortion, computer graphics, corrective displays., image warping, metamorphopsia', 'numpages': '5', 'articleno': '60', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Visual distortion, known as metamorphopsia, is a serious visual deficit with no effective clinical treatment and which cannot be corrected by traditional optical glasses. In this paper, we introduce a toolkit of approaches for digitally mapping and correcting visual distortion, that might eventually be incorporated in a low vision aid VR headset. We describe three different approaches spanning data-driven and generative designs and leveraging either uniocular or binocular cues. We present our proposed demonstrator, our evaluation roadmap, and challenges for the field. Initial tests with simulated data demonstrate the effectiveness of the approach. Once clinically validated, we hope these approaches will enable accurate mapping of visual distortion and eventually lead to the development of ‘digital glasses’ capable of correcting the effects of metamorphopsia and restoring healthy vision.', 'doi': '10.1145/3597638.3614510', 'url': 'https://doi.org/10.1145/3597638.3614510', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A toolkit of approaches for digital mapping and correction of visual distortion', 'author': 'Ling, Ye and Frohlich, David M and Williamson, Tom H and Guillemaut, Jean-Yves', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614510'}"
Supporting ASL Communication Between Hearing Parents and Deaf Children,10.1145/3597638.3614511,"The vast majority of deaf or hard-of-hearing (DHH) children are born to hearing parents. Due to a lack of immersive exposure to their natural language - sign language - they are at severe risk of language deprivation. In response to this challenge, this paper presents a novel computer-mediated communication platform named Tabletop Interactive Play System (TIPS). It serves as a test-bed to investigate technical and ethical solutions that enable hearing parents to use American Sign Language (ASL) during face-to-face play with their with their DHH children. The TIPS platform offers a variety of user options in three key aspects: (1) ASL recommendation in alignment with hearing parents’ real-time speech; (2) ASL display through different form-factors (Augmented Reality (AR) projection, tablet, and smart glasses); and (3) autonomy support to enhance users’ sense of agency and trust in the system. In this paper, we will describe the system’s design, implementation, and preliminary evaluation results.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'American Sign Language, Augmented Reality, Computer-Mediated Communication, Deaf or Hard-of-Hearing, Parent-Child Interaction', 'numpages': '5', 'articleno': '61', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The vast majority of deaf or hard-of-hearing (DHH) children are born to hearing parents. Due to a lack of immersive exposure to their natural language - sign language - they are at severe risk of language deprivation. In response to this challenge, this paper presents a novel computer-mediated communication platform named Tabletop Interactive Play System (TIPS). It serves as a test-bed to investigate technical and ethical solutions that enable hearing parents to use American Sign Language (ASL) during face-to-face play with their with their DHH children. The TIPS platform offers a variety of user options in three key aspects: (1) ASL recommendation in alignment with hearing parents’ real-time speech; (2) ASL display through different form-factors (Augmented Reality (AR) projection, tablet, and smart glasses); and (3) autonomy support to enhance users’ sense of agency and trust in the system. In this paper, we will describe the system’s design, implementation, and preliminary evaluation results.', 'doi': '10.1145/3597638.3614511', 'url': 'https://doi.org/10.1145/3597638.3614511', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Supporting ASL Communication Between Hearing Parents and Deaf Children', 'author': 'Hossain, Ekram and Bao, Ashley and Newman, Kaleb Slater and Mann, Madeleine and Wang, Hecong and Li, Yifan and Kurumada, Chigusa and Hall, Wyatte and Bai, Zhen', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614511'}"
"Understanding Experiences, Attitudes and Perspectives towards Designing Interactive Creative Tools for Teachers of Visually Impaired Students",10.1145/3597638.3614512,"Many academic subjects are inaccessible for students who are blind or have low vision (BLV) due to the prevalence of visual aids to represent concepts. Interactive devices offer promise as creative tools for teachers of the visually impaired (TVIs) as they can support real-time iteration of adapted learning materials, display changing information, and provide embodied learning experiences for BLV students. We conducted semi-structured interviews with 5 educators (all have TVI experience) and identified their considerations when creating adaptations, attitudes towards technology, and perspectives on existing barriers to access. Our findings reveal and reaffirm unresolved challenges in the adaptation process as well as offer insights into key factors that must be considered when selecting the type of adaptation. From these findings, we formulate design recommendations for interactive tools that support TVIs in creating effective adaptations for BLV students.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, interactive creative tools for education, visual impairments', 'numpages': '5', 'articleno': '62', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Many academic subjects are inaccessible for students who are blind or have low vision (BLV) due to the prevalence of visual aids to represent concepts. Interactive devices offer promise as creative tools for teachers of the visually impaired (TVIs) as they can support real-time iteration of adapted learning materials, display changing information, and provide embodied learning experiences for BLV students. We conducted semi-structured interviews with 5 educators (all have TVI experience) and identified their considerations when creating adaptations, attitudes towards technology, and perspectives on existing barriers to access. Our findings reveal and reaffirm unresolved challenges in the adaptation process as well as offer insights into key factors that must be considered when selecting the type of adaptation. From these findings, we formulate design recommendations for interactive tools that support TVIs in creating effective adaptations for BLV students.', 'doi': '10.1145/3597638.3614512', 'url': 'https://doi.org/10.1145/3597638.3614512', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding Experiences, Attitudes and Perspectives towards Designing Interactive Creative Tools for Teachers of Visually Impaired Students', 'author': 'Boadi-Agyemang, Abena and Carter, Elizabeth Jeanne and Siu, Alexa F and Steinfeld, Aaron and Orta Martinez, Melisa', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614512'}"
Enhancing Textual Accessibility for Readers with Dyslexia through Transfer Learning,10.1145/3597638.3614473,"This paper explores automated modification of text to make it more accessible for people with dyslexia, a reading disorder affecting a significant percentage of the global population. The modifications are both in terms of changing the appearance of text and simplification of the words, grammar, and length of textual material. For simplification of text, we built a dataset with original and dyslexia-friendly text verified by human readers that it improve their reading experience by 27\% on average. Then we developed a pipeline to generate dyslexia-friendly text automatically using transfer learning. The model learns styles appropriate for dyslexic users and generates dyslexia-friendly text from arbitrary textual data, which is easier for people with dyslexia to read and interpret.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Dyslexic friendly text, neural text generation, style transfer', 'numpages': '5', 'articleno': '63', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper explores automated modification of text to make it more accessible for people with dyslexia, a reading disorder affecting a significant percentage of the global population. The modifications are both in terms of changing the appearance of text and simplification of the words, grammar, and length of textual material. For simplification of text, we built a dataset with original and dyslexia-friendly text verified by human readers that it improve their reading experience by 27\\% on average. Then we developed a pipeline to generate dyslexia-friendly text automatically using transfer learning. The model learns styles appropriate for dyslexic users and generates dyslexia-friendly text from arbitrary textual data, which is easier for people with dyslexia to read and interpret.', 'doi': '10.1145/3597638.3614473', 'url': 'https://doi.org/10.1145/3597638.3614473', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Enhancing Textual Accessibility for Readers with Dyslexia through Transfer Learning', 'author': 'Madjidi, Elham and Crick, Christopher', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614473'}"
Communication and Collaboration Among DHH People in a Co-located Collaborative Multiplayer AR Environment,10.1145/3597638.3614479,"One of the most recent developments in augmented reality (AR) technology is co-located collaborative multiplayer AR environments, yet little research has been done to determine how communication and collaboration function in these settings. There is a gap in our understanding of whether these environments are currently accessible for Deaf and Hard of Hearing (DHH) people. Although technical aspects, such as design, implementation, accuracy, etc., of the technologies and users’ behavior with assistive AR technologies, have been covered in plenty of prior literature, very few studies have paid attention to how DHH people communicate and collaborate in co-located collaborative AR environments. As an initial step toward addressing the gap in current literature, our ongoing research focuses on how DHH people communicate and collaborate in a co-located collaborative multiplayer AR environment. As part of our study, we conducted gameplay experiments and one-on-one semi-structured interviews with 17 DHH participants. In this piece of work, among the prominent themes that stood out in our findings, we further discuss communication and collaboration, specifically multi-modal communication (verbal and non-verbal) among users and how multi-modal communication affected their collaboration in the environment.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Augmented Reality, Human-computer Interaction', 'numpages': '5', 'articleno': '64', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'One of the most recent developments in augmented reality (AR) technology is co-located collaborative multiplayer AR environments, yet little research has been done to determine how communication and collaboration function in these settings. There is a gap in our understanding of whether these environments are currently accessible for Deaf and Hard of Hearing (DHH) people. Although technical aspects, such as design, implementation, accuracy, etc., of the technologies and users’ behavior with assistive AR technologies, have been covered in plenty of prior literature, very few studies have paid attention to how DHH people communicate and collaborate in co-located collaborative AR environments. As an initial step toward addressing the gap in current literature, our ongoing research focuses on how DHH people communicate and collaborate in a co-located collaborative multiplayer AR environment. As part of our study, we conducted gameplay experiments and one-on-one semi-structured interviews with 17 DHH participants. In this piece of work, among the prominent themes that stood out in our findings, we further discuss communication and collaboration, specifically multi-modal communication (verbal and non-verbal) among users and how multi-modal communication affected their collaboration in the environment.', 'doi': '10.1145/3597638.3614479', 'url': 'https://doi.org/10.1145/3597638.3614479', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Communication and Collaboration Among DHH People in a Co-located Collaborative Multiplayer AR Environment', 'author': 'Luna, Sanzida Mojib and Tigwell, Garreth W. and Papangelis, Konstantinos and Xu, Jiangnan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614479'}"
Building the Habit of Authoring Alt Text: Design for Making a Change,10.1145/3597638.3614495,"Alternative (alt) text is essential for the accessibility of images to screen reader users. Alt text can be authored by end users or generated automatically using machine learning. Automated methods, while fast and cheap, do not result in high quality alt text. Meanwhile, the large number of images with no alt text on social media indicates that many users do not author alt text for the images they upload. Little is known about whether design elements that motivate and support users are integrated into alt text user interfaces (UIs). We reviewed the current state of alt text UIs in literature, and characterized their design according to behavior change and accountability design factors. We found that the majority of reviewed UIs contained general prompts with no specific motivator or guidance. Many of the UIs did not incorporate design elements to encourage accountability or behavior change. We recommend that future alt text authoring tools incorporate design elements which decrease the user’s cognitive effort, employ motivational and well-timed triggers, and leverage user accountability through alt text visibility and its association with the user’s social presence.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accountability, alt text, behavior change, image accessibility', 'numpages': '5', 'articleno': '65', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Alternative (alt) text is essential for the accessibility of images to screen reader users. Alt text can be authored by end users or generated automatically using machine learning. Automated methods, while fast and cheap, do not result in high quality alt text. Meanwhile, the large number of images with no alt text on social media indicates that many users do not author alt text for the images they upload. Little is known about whether design elements that motivate and support users are integrated into alt text user interfaces (UIs). We reviewed the current state of alt text UIs in literature, and characterized their design according to behavior change and accountability design factors. We found that the majority of reviewed UIs contained general prompts with no specific motivator or guidance. Many of the UIs did not incorporate design elements to encourage accountability or behavior change. We recommend that future alt text authoring tools incorporate design elements which decrease the user’s cognitive effort, employ motivational and well-timed triggers, and leverage user accountability through alt text visibility and its association with the user’s social presence.', 'doi': '10.1145/3597638.3614495', 'url': 'https://doi.org/10.1145/3597638.3614495', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Building the Habit of Authoring Alt Text: Design for Making a Change', 'author': 'Bellscheidt, Selah and Metcalf, Hazel and Pham, Di and Elglaly, Yasmine N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614495'}"
Design Recommendations for an Inclusive Online Sexual Health Clinic for Blind and Partially Sighted People,10.1145/3597638.3614487,"Sexually transmitted infections are highly prevalent in the United Kingdom, and chlamydia, which is largely asymptomatic, is the most common. Digital health is being integrated into health service provision to support the increasing demand for testing and treatment. However, with this integration, there is a need to ensure that the design, development, and deployment of digital health platforms are inclusive and accessible for all. This paper explores the needs of blind and partially sighted people when using an online sexual health clinic through interviews with seven blind and partially sighted people. The findings identified key barriers and facilitators that impact the accessibility of online sexual health clinics, including the accessibility of the visual content, particularly for self-testing and the need to consider the privacy of disclosing sexual health information. We align with the principles of Universal Design and, as such, present design recommendations to inform the design of inclusive online sexual health interventions.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '4', 'articleno': '66', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sexually transmitted infections are highly prevalent in the United Kingdom, and chlamydia, which is largely asymptomatic, is the most common. Digital health is being integrated into health service provision to support the increasing demand for testing and treatment. However, with this integration, there is a need to ensure that the design, development, and deployment of digital health platforms are inclusive and accessible for all. This paper explores the needs of blind and partially sighted people when using an online sexual health clinic through interviews with seven blind and partially sighted people. The findings identified key barriers and facilitators that impact the accessibility of online sexual health clinics, including the accessibility of the visual content, particularly for self-testing and the need to consider the privacy of disclosing sexual health information. We align with the principles of Universal Design and, as such, present design recommendations to inform the design of inclusive online sexual health interventions.', 'doi': '10.1145/3597638.3614487', 'url': 'https://doi.org/10.1145/3597638.3614487', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Design Recommendations for an Inclusive Online Sexual Health Clinic for Blind and Partially Sighted People', 'author': 'Mooney, Danita and Bandukda, Maryam and Patel, Dilisha', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614487'}"
Conveying Uncertainty in Data Visualizations to Screen-Reader Users Through Non-Visual Means,10.1145/3597638.3614502,"Incorporating uncertainty in data visualizations is critical for users to interpret and reliably draw informed conclusions from the underlying data. However, visualization creators conventionally convey the information regarding uncertainty in data visualizations using visual techniques (e.g., error bars), which disenfranchises screen-reader users, who may be blind or have low vision. In this preliminary exploration, we investigated ways to convey uncertainty in data visualizations to screen-reader users. Specifically, we conducted semi-structured interviews, finding that these users prefer to obtain statistical information on uncertainty expressed in plain language, conveyed holistically with avenues to explore the data further in a drilled-down manner. To support screen-reader users in extracting information about uncertainty in online data visualizations, we utilized our findings to enhance VoxLens—an open-source JavaScript plug-in that makes online data visualizations accessible to screen-reader users.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'blind, screen reader, uncertainty, visualizations, voxlens', 'numpages': '6', 'articleno': '67', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Incorporating uncertainty in data visualizations is critical for users to interpret and reliably draw informed conclusions from the underlying data. However, visualization creators conventionally convey the information regarding uncertainty in data visualizations using visual techniques (e.g., error bars), which disenfranchises screen-reader users, who may be blind or have low vision. In this preliminary exploration, we investigated ways to convey uncertainty in data visualizations to screen-reader users. Specifically, we conducted semi-structured interviews, finding that these users prefer to obtain statistical information on uncertainty expressed in plain language, conveyed holistically with avenues to explore the data further in a drilled-down manner. To support screen-reader users in extracting information about uncertainty in online data visualizations, we utilized our findings to enhance VoxLens—an open-source JavaScript plug-in that makes online data visualizations accessible to screen-reader users.', 'doi': '10.1145/3597638.3614502', 'url': 'https://doi.org/10.1145/3597638.3614502', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Conveying Uncertainty in Data Visualizations to Screen-Reader Users Through Non-Visual Means', 'author': 'Sharif, Ather and Zhong, Ruican and Wang, Yadi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614502'}"
Conceptualizing Celebratory Technologies for Neurodiversity to Reduce Social Stigma,10.1145/3597638.3614478,"Social stigma is a complex phenomenon that plagues humanity. Disability is a key target of stigma, with physical, cognitive, emotional conditions being highly scrutinized by society. Many technologies aimed to support disabled people attempt to do so by directly changing their individual interactions with the world. When attention is drawn to affirm disability identities, it risks being perceived as inspiration porn. However, there is another way to level the playing field, to be inclusive rather than objectifying; that aim to empower stigmatized individuals rather than provide a single example of inspiration for the nondisabled observer that does little to reduce the negative stereotype. This paper introduces the concept of Celebratory Technologies for Neurodiversity. Drawing on a framework of social change specific to the stigma of autism; the tenets of the Neurodiversity movement; and the results of a current design probe with an adult with autism, initial suggestions for creating Celebratory Technologies for Neurodiversity support are provided.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Celebratory Technology, Neurodiversity, Social Justice, Stigma', 'numpages': '4', 'articleno': '68', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Social stigma is a complex phenomenon that plagues humanity. Disability is a key target of stigma, with physical, cognitive, emotional conditions being highly scrutinized by society. Many technologies aimed to support disabled people attempt to do so by directly changing their individual interactions with the world. When attention is drawn to affirm disability identities, it risks being perceived as inspiration porn. However, there is another way to level the playing field, to be inclusive rather than objectifying; that aim to empower stigmatized individuals rather than provide a single example of inspiration for the nondisabled observer that does little to reduce the negative stereotype. This paper introduces the concept of Celebratory Technologies for Neurodiversity. Drawing on a framework of social change specific to the stigma of autism; the tenets of the Neurodiversity movement; and the results of a current design probe with an adult with autism, initial suggestions for creating Celebratory Technologies for Neurodiversity support are provided.', 'doi': '10.1145/3597638.3614478', 'url': 'https://doi.org/10.1145/3597638.3614478', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Conceptualizing Celebratory Technologies for Neurodiversity to Reduce Social Stigma', 'author': 'Boyd, LouAnne', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614478'}"
Manipulation by Ocular Operations for Real-time Effect (MOORE): A Concise and User-Friendly Method for Eye-Based Interaction,10.1145/3597638.3614483,"The rapid development of information science and technology has significantly enhanced people's quality of life. Electronic products are now indispensable in both professional and daily contexts. However, there exists a segment of the population who suffer from movement disorders and diseases, limiting their ability to benefit from the convenience offered by technological advancements, especially in utilizing computer-based electronic products. Eye-tracking technology has emerged as a promising interactive method to address this challenge. However, the current mainstream eye-tracking applications (e.g., Windows’ built-in Eye Control software) exhibit limitations or insufficiency in terms of operation accuracy and user experience. To resolve these issues, this paper proposes a concise and user-friendly method based on eye-tracking technology to assist individuals with movement disorders in utilizing computers. The user experiments show that the present solution overperforms Windows’ built-in function. This approach offers a cost-effective and easy-to-learn solution, enabling this specific population to regain interactive control over computers and reintegrate into the modern information society.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Eye tracking, Eye-based interactions, Gaze detection, Hands-free', 'numpages': '6', 'articleno': '69', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""The rapid development of information science and technology has significantly enhanced people's quality of life. Electronic products are now indispensable in both professional and daily contexts. However, there exists a segment of the population who suffer from movement disorders and diseases, limiting their ability to benefit from the convenience offered by technological advancements, especially in utilizing computer-based electronic products. Eye-tracking technology has emerged as a promising interactive method to address this challenge. However, the current mainstream eye-tracking applications (e.g., Windows’ built-in Eye Control software) exhibit limitations or insufficiency in terms of operation accuracy and user experience. To resolve these issues, this paper proposes a concise and user-friendly method based on eye-tracking technology to assist individuals with movement disorders in utilizing computers. The user experiments show that the present solution overperforms Windows’ built-in function. This approach offers a cost-effective and easy-to-learn solution, enabling this specific population to regain interactive control over computers and reintegrate into the modern information society."", 'doi': '10.1145/3597638.3614483', 'url': 'https://doi.org/10.1145/3597638.3614483', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Manipulation by Ocular Operations for Real-time Effect (MOORE): A Concise and User-Friendly Method for Eye-Based Interaction', 'author': 'Jiang, Vincent and Han, Hailiang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614483'}"
Large-Scale Anonymized Text-based Disability Discourse Dataset,10.1145/3597638.3614476,"The involvement of individuals with disabilities in online discussions related to disability and accessibility is a critical area of study. While previous research has qualitatively examined the participation of individuals with disabilities on social media platforms, large-scale analysis of social media content by people with disabilities has been an underexplored area. This paper presents a pioneering large-scale study of disability communities on Reddit. We developed an anonymized text-based dataset that consists of 1.5 million comments posted on three subreddits: r/disability, r/Blind, and r/ADHD. Using topic modeling, we analyzed the dataset and identified eight highly-coherent common categories and their associated keywords across the three subreddits. We contribute an Anonymized Disability Discourse Reddit Corpus (ADDReC) of 1.5 million comments that feature eight disability discourse categories.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'ADHD, Reddit, blind, dataset, disability, discourse', 'numpages': '5', 'articleno': '70', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The involvement of individuals with disabilities in online discussions related to disability and accessibility is a critical area of study. While previous research has qualitatively examined the participation of individuals with disabilities on social media platforms, large-scale analysis of social media content by people with disabilities has been an underexplored area. This paper presents a pioneering large-scale study of disability communities on Reddit. We developed an anonymized text-based dataset that consists of 1.5 million comments posted on three subreddits: r/disability, r/Blind, and r/ADHD. Using topic modeling, we analyzed the dataset and identified eight highly-coherent common categories and their associated keywords across the three subreddits. We contribute an Anonymized Disability Discourse Reddit Corpus (ADDReC) of 1.5 million comments that feature eight disability discourse categories.', 'doi': '10.1145/3597638.3614476', 'url': 'https://doi.org/10.1145/3597638.3614476', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Large-Scale Anonymized Text-based Disability Discourse Dataset', 'author': 'Palonis, Brandon and Dobesh, Samantha Jane and Bellscheidt, Selah and Mkaouer, Mohamed Wiem and Liu, Yudong and Elglaly, Yasmine N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614476'}"
Game-based Powered Wheelchair Skills Training: Examining Tensions Between Player Engagement and Therapeutic Requirements,10.1145/3597638.3614475,"Wheelchair skills training is key for achieving greater personal mobility, and interactive solutions offer the potential for a more engaging training experience. In this paper, we examine the tensions between therapeutic requirements and player engagement, through the prototypical development of a wheelchair skills training game for children with Cerebral Palsy. Using an iterative and interdisciplinary design process, exercises from the Wheelchair Skills Program manual are implemented as interactive missions in an open-world, medieval-themed game. The game contains a custom input system that directly uses the player’s wheelchair input peripherals. In bridging the gap between clinical requirements and player engagement, we discuss the challenge of repetition and feedback in therapy, and implications for future work in game design.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'games, interdisciplinary design, iterative design, virtual training, wheelchair training', 'numpages': '5', 'articleno': '71', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Wheelchair skills training is key for achieving greater personal mobility, and interactive solutions offer the potential for a more engaging training experience. In this paper, we examine the tensions between therapeutic requirements and player engagement, through the prototypical development of a wheelchair skills training game for children with Cerebral Palsy. Using an iterative and interdisciplinary design process, exercises from the Wheelchair Skills Program manual are implemented as interactive missions in an open-world, medieval-themed game. The game contains a custom input system that directly uses the player’s wheelchair input peripherals. In bridging the gap between clinical requirements and player engagement, we discuss the challenge of repetition and feedback in therapy, and implications for future work in game design.', 'doi': '10.1145/3597638.3614475', 'url': 'https://doi.org/10.1145/3597638.3614475', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Game-based Powered Wheelchair Skills Training: Examining Tensions Between Player Engagement and Therapeutic Requirements', 'author': 'Ravers, Douwe and Gerling, Kathrin and Naaris, Mari and Konings, Marco J. and Verslype, Sammy and Monbaliu, Elegast and Hallez, Hans', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614475'}"
PartiPlay: A Participatory Game Design Kit for Neurodiverse Classrooms,10.1145/3597638.3614496,"Play is a central aspect of childhood development, with games as a vital tool to promote it. However, neurodivergent children, especially those in neurodiverse environments, are underserved by HCI games research. Most existing work takes on a top-down approach, disregarding neurodivergent interest for the majority of the design process. Co-design is often proposed as a tool to create truly accessible and inclusive gaming experiences. Nevertheless, co-designing with neurodivergent children within neurodiverse groups brings about unique challenges, such as different communication styles, sensory needs and preferences. Building upon recommendations from prior work in neurodivergent, mixed-ability, and child-led co-design, we propose a concrete participatory game design kit for neurodiverse classrooms: PartiPlay. Moreover, we present preliminary findings from an in-the-wild experiment with the said kit, showcasing its ability to create an inclusive co-design process for neurodiverse groups of children. We aim to provide actionable steps for future participatory design research with neurodiverse children.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Children, Classrooms, Co-design, Games, Inclusion, Neurodivergent, Neurodiverse', 'numpages': '5', 'articleno': '72', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Play is a central aspect of childhood development, with games as a vital tool to promote it. However, neurodivergent children, especially those in neurodiverse environments, are underserved by HCI games research. Most existing work takes on a top-down approach, disregarding neurodivergent interest for the majority of the design process. Co-design is often proposed as a tool to create truly accessible and inclusive gaming experiences. Nevertheless, co-designing with neurodivergent children within neurodiverse groups brings about unique challenges, such as different communication styles, sensory needs and preferences. Building upon recommendations from prior work in neurodivergent, mixed-ability, and child-led co-design, we propose a concrete participatory game design kit for neurodiverse classrooms: PartiPlay. Moreover, we present preliminary findings from an in-the-wild experiment with the said kit, showcasing its ability to create an inclusive co-design process for neurodiverse groups of children. We aim to provide actionable steps for future participatory design research with neurodiverse children.', 'doi': '10.1145/3597638.3614496', 'url': 'https://doi.org/10.1145/3597638.3614496', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'PartiPlay: A Participatory Game Design Kit for Neurodiverse Classrooms', 'author': 'Piedade, Patricia and Neto, Isabel and Pires, Ana Cristina and Prada, Rui and Nicolau, Hugo', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614496'}"
“Invisible Illness Is No Longer Invisible”: Making Social VR Avatars More Inclusive for Invisible Disability Representation,10.1145/3597638.3614480,"As social virtual reality (VR) experiences become more popular, it is critical to design accessible and inclusive embodied avatars. At present, there are few, if any, customization features for invisible disabilities (e.g., chronic health conditions, mental health conditions, neurodivergence) in social VR platforms. To our knowledge, researchers have yet to explore how people with invisible disabilities want to self-represent and disclose disabilities through social VR avatars. We fill this gap in current accessibility research by centering the experiences and preferences of people with invisible disabilities. We conducted semi-structured interviews with nine participants and found that people with invisible disabilities used a unique, indirect approach to inform dynamic disclosure practices. Participants were interested in toggling representation on/off across contexts and shared ideas for representation through avatar design. In addition, they proposed ways to make the customization process more accessible (e.g., making it easier to import custom designs). We see our work as a vital contribution to the growing literature that calls for more inclusive social VR.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, avatars, customization, disability disclosure, invisible disabilities, social virtual reality, virtual reality', 'numpages': '4', 'articleno': '73', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'As social virtual reality (VR) experiences become more popular, it is critical to design accessible and inclusive embodied avatars. At present, there are few, if any, customization features for invisible disabilities (e.g., chronic health conditions, mental health conditions, neurodivergence) in social VR platforms. To our knowledge, researchers have yet to explore how people with invisible disabilities want to self-represent and disclose disabilities through social VR avatars. We fill this gap in current accessibility research by centering the experiences and preferences of people with invisible disabilities. We conducted semi-structured interviews with nine participants and found that people with invisible disabilities used a unique, indirect approach to inform dynamic disclosure practices. Participants were interested in toggling representation on/off across contexts and shared ideas for representation through avatar design. In addition, they proposed ways to make the customization process more accessible (e.g., making it easier to import custom designs). We see our work as a vital contribution to the growing literature that calls for more inclusive social VR.', 'doi': '10.1145/3597638.3614480', 'url': 'https://doi.org/10.1145/3597638.3614480', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“Invisible Illness Is No Longer Invisible”: Making Social VR Avatars More Inclusive for Invisible Disability Representation', 'author': 'Gualano, Ria J. and Jiang, Lucy and Zhang, Kexin and Won, Andrea Stevenson and Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614480'}"
Prioritizing Aesthetic Touch: Interpreting Historical Textiles with Digital Embroidery,10.1145/3597638.3614477,"A lack of accessible exhibitions for Blind and low-vision (BLV) visitors in art museums and historical sites is a barrier to equitable participation. The design and fabrication of accessible materials can be time-consuming and expensive for many institutions. We describe our work using digital embroidery machines to interpret historical textiles to provide accessible interpretations for BLV museum visitors and tactile learners. Our work focuses on designing solutions that that are informative, durable, and enjoyable to touch, while affordable to produce. Through a collaborative design process with tactile graphics experts in museums we create prototypes of historical textiles that meet these criteria. We describe our design process and offer fabrication guidelines for patterns and materials.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Blindness and Low Vision, Digital Fabrication, Tactile Graphics', 'numpages': '4', 'articleno': '74', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'A lack of accessible exhibitions for Blind and low-vision (BLV) visitors in art museums and historical sites is a barrier to equitable participation. The design and fabrication of accessible materials can be time-consuming and expensive for many institutions. We describe our work using digital embroidery machines to interpret historical textiles to provide accessible interpretations for BLV museum visitors and tactile learners. Our work focuses on designing solutions that that are informative, durable, and enjoyable to touch, while affordable to produce. Through a collaborative design process with tactile graphics experts in museums we create prototypes of historical textiles that meet these criteria. We describe our design process and offer fabrication guidelines for patterns and materials.', 'doi': '10.1145/3597638.3614477', 'url': 'https://doi.org/10.1145/3597638.3614477', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Prioritizing Aesthetic Touch: Interpreting Historical Textiles with Digital Embroidery', 'author': 'Koseff, Stefanie Malina and Johnston, Daniel Ryan and Kleege, Georgina and Fleet, Chancey and Fogle-Hatch, Cheryl and Race, Lauren and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614477'}"
Testaro: Efficient Ensemble Testing for Web Accessibility,10.1145/3597638.3614505,"As automated web accessibility testing tools become enriched with new and improved tests, it can be impractical to leverage those advances. Each tool offers unique benefits, but effectively using multiple tools would require integrating them into a uniform testing and reporting scheme. Such integration is complex, because tools vary in what they try to detect, what they actually detect, and how they classify, describe, and report defects. Consequently, testers typically use only one tool. Testaro[7] is a novel open-source NPM package that checks compliance with about 650 rules defined by an ensemble of 8 tools: alfa, Axe, Equal Access, HTML CodeSniffer, Nu Html Checker, QualWeb, Testaro, and WAVE. Attendees at the demonstration will, within 5 minutes, create jobs for Testaro, run them, and generate unified reports documenting more accessibility issues than any single tool can discover.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility testing, test automation, test efficiency, web accessibility', 'numpages': '4', 'articleno': '75', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'As automated web accessibility testing tools become enriched with new and improved tests, it can be impractical to leverage those advances. Each tool offers unique benefits, but effectively using multiple tools would require integrating them into a uniform testing and reporting scheme. Such integration is complex, because tools vary in what they try to detect, what they actually detect, and how they classify, describe, and report defects. Consequently, testers typically use only one tool. Testaro[7] is a novel open-source NPM package that checks compliance with about 650 rules defined by an ensemble of 8 tools: alfa, Axe, Equal Access, HTML CodeSniffer, Nu Html Checker, QualWeb, Testaro, and WAVE. Attendees at the demonstration will, within 5 minutes, create jobs for Testaro, run them, and generate unified reports documenting more accessibility issues than any single tool can discover.', 'doi': '10.1145/3597638.3614505', 'url': 'https://doi.org/10.1145/3597638.3614505', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Testaro: Efficient Ensemble Testing for Web Accessibility', 'author': 'Pool, Jonathan Robert', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614505'}"
U.S. Deaf Community Perspectives on Automatic Sign Language Translation,10.1145/3597638.3614507,"Millions of Deaf and hard-of-hearing (DHH) people primarily use a sign language for communication, but there is a lack of adequate sign language interpreting to fill these communication needs. Development of automatic sign language translation (ASLT) systems could help translate between a sign language and spoken language in situations where human interpreters are unavailable, and recent advances in large multi-lingual language models may soon enable ASLT to become a reality. Despite the potential for ASLT, Deaf community perspectives on and requirements for such technologies are poorly understood. In this work, we conduct a survey of Deaf community perspectives in the U.S. on ASLT in order to inform the development of ASLT systems that meet user needs and minimize harms. Our results shed light on scenarios where DHH users in the U.S. might want to use ASLT, their performance expectations for ASLT in these scenarios, design preferences for ASLT interfaces, and the benefits and harms they see in the development of ASLT.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'American Sign Language (ASL), Deaf community, design criteria, machine translation, sign language translation, survey', 'numpages': '7', 'articleno': '76', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Millions of Deaf and hard-of-hearing (DHH) people primarily use a sign language for communication, but there is a lack of adequate sign language interpreting to fill these communication needs. Development of automatic sign language translation (ASLT) systems could help translate between a sign language and spoken language in situations where human interpreters are unavailable, and recent advances in large multi-lingual language models may soon enable ASLT to become a reality. Despite the potential for ASLT, Deaf community perspectives on and requirements for such technologies are poorly understood. In this work, we conduct a survey of Deaf community perspectives in the U.S. on ASLT in order to inform the development of ASLT systems that meet user needs and minimize harms. Our results shed light on scenarios where DHH users in the U.S. might want to use ASLT, their performance expectations for ASLT in these scenarios, design preferences for ASLT interfaces, and the benefits and harms they see in the development of ASLT.', 'doi': '10.1145/3597638.3614507', 'url': 'https://doi.org/10.1145/3597638.3614507', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'U.S. Deaf Community Perspectives on Automatic Sign Language Translation', 'author': 'Tran, Nina and Ladner, Richard E. and Bragg, Danielle', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614507'}"
Towards Street Camera-based Outdoor Navigation for Blind Pedestrians,10.1145/3597638.3614498,"Blind and low-vision (BLV) people use GPS-based systems for outdoor navigation assistance, which provide instructions to get from one place to another. However, such systems do not provide users with real-time, precise information about their location and surroundings which is crucial for safe navigation. In this work, we investigate whether street cameras can be used to address aspects of navigation that BLV people still find challenging with existing GPS-based assistive technologies. We conducted formative interviews with six BLV participants to identify specific challenges they face in outdoor navigation. We discovered three main challenges: anticipating environment layouts, avoiding obstacles while following directions, and crossing noisy street intersections. To address these challenges, we are currently developing a street camera-based navigation system that provides real-time auditory feedback to help BLV users avoid obstacles, know exactly when to cross the street, and understand the overall layout of the environment. We close by discussing our evaluation plan.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Visual impairments, computer vision, outdoor navigation, street camera, testbed evaluation', 'numpages': '6', 'articleno': '77', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blind and low-vision (BLV) people use GPS-based systems for outdoor navigation assistance, which provide instructions to get from one place to another. However, such systems do not provide users with real-time, precise information about their location and surroundings which is crucial for safe navigation. In this work, we investigate whether street cameras can be used to address aspects of navigation that BLV people still find challenging with existing GPS-based assistive technologies. We conducted formative interviews with six BLV participants to identify specific challenges they face in outdoor navigation. We discovered three main challenges: anticipating environment layouts, avoiding obstacles while following directions, and crossing noisy street intersections. To address these challenges, we are currently developing a street camera-based navigation system that provides real-time auditory feedback to help BLV users avoid obstacles, know exactly when to cross the street, and understand the overall layout of the environment. We close by discussing our evaluation plan.', 'doi': '10.1145/3597638.3614498', 'url': 'https://doi.org/10.1145/3597638.3614498', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Towards Street Camera-based Outdoor Navigation for Blind Pedestrians', 'author': 'Jain, Gaurav and Hindi, Basel and Xie, Mingyu and Zhang, Zihao and Srinivasula, Koushik and Ghasemi, Mahshid and Weiner, Daniel and Xu, Xin Yi Therese and Paris, Sophie Ana and Tedjo, Chloe and Bassin, Josh and Malcolm, Michael and Turkcan, Mehmet and Ghaderi, Javad and Kostic, Zoran and Zussman, Gil and Smith, Brian A.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614498'}"
Approaches to Making Live Code Accessible in a Mixed-Vision Music Ensemble,10.1145/3597638.3614489,"The Fil Laptop Orchestra, aka FiLOrk, is an electronic music ensemble that performs through live coding, i.e. collaboratively editing in a shared document to generate sound. In forming FiLOrk, we set out to use music performance and tactile graphics to address a lack of accessible tools and curricula for teaching Blind and Visually Impaired (BVI) people to code. In this poster, researchers and the high school members of FiLOrk, who use screen readers and magnification, report their experiences composing and performing two original works that incorporate assistive technologies. We discuss a shift from instruction to open-ended experimentation, and we highlight technical and pedagogical opportunities for making live coding more accessible.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Blindness, Electronic Music, Laptop Ensemble, Live Coding, Tidal Cycles, Vision Impairment', 'numpages': '5', 'articleno': '78', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The Fil Laptop Orchestra, aka FiLOrk, is an electronic music ensemble that performs through live coding, i.e. collaboratively editing in a shared document to generate sound. In forming FiLOrk, we set out to use music performance and tactile graphics to address a lack of accessible tools and curricula for teaching Blind and Visually Impaired (BVI) people to code. In this poster, researchers and the high school members of FiLOrk, who use screen readers and magnification, report their experiences composing and performing two original works that incorporate assistive technologies. We discuss a shift from instruction to open-ended experimentation, and we highlight technical and pedagogical opportunities for making live coding more accessible.', 'doi': '10.1145/3597638.3614489', 'url': 'https://doi.org/10.1145/3597638.3614489', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Approaches to Making Live Code Accessible in a Mixed-Vision Music Ensemble', 'author': 'Payne, William Christopher and Shen, Xinran and Xu, Eric and Kaney, Matthew and Graves, Maya and Herrera, Matthew and Mau, Madeline and Murray, Diana and Wang, Vinnie and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614489'}"
“Let the Volcano Erupt!”: Designing Sonification to Make Oceanography Accessible for Blind and Low Vision Students in Museum Environment,10.1145/3597638.3614482,"Exploring the potential of data sonification, our study focuses on enhancing accessibility to oceanographic data for blind and low vision (BLV) students in museums. Five auditory display prototypes with accompanying data sonifications were designed, and two prototypes underwent evaluation by testing with blind and low vision students from two schools for the blind. Through interviews and qualitative analysis, the study assessed the students' learning outcomes and their experience on the auditory display prototypes. The results demonstrate the effective conveyance of oceanography concepts and data through the developed audio display prototypes. Additionally, the study derived several design implications for future oceanographic data sonification design in museums. This research contributes to advancing the accessibility of oceanography education for visually impaired students while providing valuable insights for the design of future auditory displays in informal learning environments (ILE).","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Learning Sciences, Oceanography, Sonification', 'numpages': '6', 'articleno': '79', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Exploring the potential of data sonification, our study focuses on enhancing accessibility to oceanographic data for blind and low vision (BLV) students in museums. Five auditory display prototypes with accompanying data sonifications were designed, and two prototypes underwent evaluation by testing with blind and low vision students from two schools for the blind. Through interviews and qualitative analysis, the study assessed the students' learning outcomes and their experience on the auditory display prototypes. The results demonstrate the effective conveyance of oceanography concepts and data through the developed audio display prototypes. Additionally, the study derived several design implications for future oceanographic data sonification design in museums. This research contributes to advancing the accessibility of oceanography education for visually impaired students while providing valuable insights for the design of future auditory displays in informal learning environments (ILE)."", 'doi': '10.1145/3597638.3614482', 'url': 'https://doi.org/10.1145/3597638.3614482', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“Let the Volcano Erupt!”: Designing Sonification to Make Oceanography Accessible for Blind and Low Vision Students in Museum Environment', 'author': 'Li, Huaigu and Bellona, Jon and Smith, Leslie and Bower, Amy and Roberts, Jessica', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614482'}"
Exploring Roundabout Navigation Training with 3D-Printed Tactile Maps,10.1145/3597638.3614485,"Navigating new traffic patterns can be challenging for everyone, but it poses particular difficulties for individuals who are blind or visually impaired (BVI). With the recent introduction of roundabouts in intersection design in the United States, many BVI individuals tend to be unfamiliar with them. Orientation and Mobility (O&amp;M) specialists have been teaching their clients to cross traditional intersections for decades leveraging rectilinear geometry and the predictable rhythm of vehicular and pedestrian traffic to ensure safe street crossing. Conventional training methods for crossing intersections do not directly translate to roundabout navigation since they present unique challenges due to the continuous flow of traffic, complex layouts, and the absence of predictable auditory cues. We propose the development and integration of 3D-printed tactile maps into roundabout navigation training tools to address these challenges and provide tactile educational materials that are portable, durable, lightweight, and informative. Through iterative modifications based on feedback from an O&amp;M specialist, we refined the tactile maps to improve usability and conceptual understanding of roundabout intersections. Our work highlights the potential of 3D-printed tactile maps to empower BVI individuals to confidently and independently navigate roundabout intersections.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '4', 'articleno': '80', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Navigating new traffic patterns can be challenging for everyone, but it poses particular difficulties for individuals who are blind or visually impaired (BVI). With the recent introduction of roundabouts in intersection design in the United States, many BVI individuals tend to be unfamiliar with them. Orientation and Mobility (O&amp;M) specialists have been teaching their clients to cross traditional intersections for decades leveraging rectilinear geometry and the predictable rhythm of vehicular and pedestrian traffic to ensure safe street crossing. Conventional training methods for crossing intersections do not directly translate to roundabout navigation since they present unique challenges due to the continuous flow of traffic, complex layouts, and the absence of predictable auditory cues. We propose the development and integration of 3D-printed tactile maps into roundabout navigation training tools to address these challenges and provide tactile educational materials that are portable, durable, lightweight, and informative. Through iterative modifications based on feedback from an O&amp;M specialist, we refined the tactile maps to improve usability and conceptual understanding of roundabout intersections. Our work highlights the potential of 3D-printed tactile maps to empower BVI individuals to confidently and independently navigate roundabout intersections.', 'doi': '10.1145/3597638.3614485', 'url': 'https://doi.org/10.1145/3597638.3614485', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Exploring Roundabout Navigation Training with 3D-Printed Tactile Maps', 'author': 'Seth, Gaurav and Zhong, Vera and Franck, Lukas and Perr, Anita and Rizzo, John-Ross and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614485'}"
Starting well on design for accessibility: analysis of W3C's 167 accessibility evaluation tools for the design phase,10.1145/3597638.3614474,"Accessibility can be overlooked in the Design-phase in creating digital products. This can lead to increased costs when problems are discovered later in product development or deployment. Our work aimed to discover how well the 167 W3C accessibility evaluation tools support the Design-phase. Using Grounded Theory, we identified key characteristics of the tools and their support. We found that just 30 (18\%) of the tools support the Design-phase; by contrast, 128 (76.5\%) support the Later-phases. Of the 30 tools supporting the Design-phase, 25 (83\%) support color checks but few support the other W3C basic design considerations. Our key contributions are: (1) our comprehensive study of the 167 W3C accessibility evaluation tools; (2) our insights about their support for the Design-phase; (3) recommendations for improved support for accessibility in the Design-phase.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessible design, W3C guideline, W3C list of accessibility evaluation tools, design workflow', 'numpages': '7', 'articleno': '81', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Accessibility can be overlooked in the Design-phase in creating digital products. This can lead to increased costs when problems are discovered later in product development or deployment. Our work aimed to discover how well the 167 W3C accessibility evaluation tools support the Design-phase. Using Grounded Theory, we identified key characteristics of the tools and their support. We found that just 30 (18\\%) of the tools support the Design-phase; by contrast, 128 (76.5\\%) support the Later-phases. Of the 30 tools supporting the Design-phase, 25 (83\\%) support color checks but few support the other W3C basic design considerations. Our key contributions are: (1) our comprehensive study of the 167 W3C accessibility evaluation tools; (2) our insights about their support for the Design-phase; (3) recommendations for improved support for accessibility in the Design-phase.', 'doi': '10.1145/3597638.3614474', 'url': 'https://doi.org/10.1145/3597638.3614474', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""Starting well on design for accessibility: analysis of W3C's 167 accessibility evaluation tools for the design phase"", 'author': 'Hadadi, Samine and Sarsenbayeva, Zhanna and Kay, Judy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614474'}"
Multimodal 3D printed urban maps for blind people. Evaluations and scientific investigations,10.1145/3597638.3614503,"Acquiring spatial information about the surrounding space is of particular importance for people with visual impairments. Thus, we propose a solution consisting of a multimodal map of urban environment which provides verbal audio information that is selectively bound to specific locations on the map. Touch sensors are embedded inside of the map body, in order to provide touch-to-audio interaction. This paper presents the method of construction of the first research device, along with its initial usability study. The results indicate that the system is useful in relaying information embedded inside of the map, it is intuitive and easy to use. Furthermore, the test users engaged themselves deeply into the evaluation phase and into the idea of multimodal 3D printed maps.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '7', 'articleno': '82', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Acquiring spatial information about the surrounding space is of particular importance for people with visual impairments. Thus, we propose a solution consisting of a multimodal map of urban environment which provides verbal audio information that is selectively bound to specific locations on the map. Touch sensors are embedded inside of the map body, in order to provide touch-to-audio interaction. This paper presents the method of construction of the first research device, along with its initial usability study. The results indicate that the system is useful in relaying information embedded inside of the map, it is intuitive and easy to use. Furthermore, the test users engaged themselves deeply into the evaluation phase and into the idea of multimodal 3D printed maps.', 'doi': '10.1145/3597638.3614503', 'url': 'https://doi.org/10.1145/3597638.3614503', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Multimodal 3D printed urban maps for blind people. Evaluations and scientific investigations', 'author': 'Telesinska, Malgorzata', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614503'}"
Understanding Blind and Low Vision Users' Attitudes Towards Spatial Interactions in Desktop Screen Readers,10.1145/3597638.3614490,"Desktop screen readers as a web navigation mechanism for BLV users are tedious and frozen in time, especially in the face of richer ways of presenting spatial information such as tactile and touchscreen devices. In our work, we consider what it means to create and evaluate systems that can present a similarly rich, spatial interaction mechanism plugged into existing screen reader paradigms. We present a formative study conducted with SpaceNav, a custom screen reader that utilizes spatial input and output to navigate two different web applications. We present results from this study, and discuss a new browser extension we are implementing based on our formative study feedback to more robustly test spatial interactions in the context of real world websites. To close, we describe our goals for evaluating the new web extension in a future study.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Blind or low vision users, accessibility, desktop web applications, human-computer interaction, screen readers, spatialized layout', 'numpages': '5', 'articleno': '83', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Desktop screen readers as a web navigation mechanism for BLV users are tedious and frozen in time, especially in the face of richer ways of presenting spatial information such as tactile and touchscreen devices. In our work, we consider what it means to create and evaluate systems that can present a similarly rich, spatial interaction mechanism plugged into existing screen reader paradigms. We present a formative study conducted with SpaceNav, a custom screen reader that utilizes spatial input and output to navigate two different web applications. We present results from this study, and discuss a new browser extension we are implementing based on our formative study feedback to more robustly test spatial interactions in the context of real world websites. To close, we describe our goals for evaluating the new web extension in a future study.', 'doi': '10.1145/3597638.3614490', 'url': 'https://doi.org/10.1145/3597638.3614490', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""Understanding Blind and Low Vision Users' Attitudes Towards Spatial Interactions in Desktop Screen Readers"", 'author': 'Chheda-Kothary, Arnavi and Rios, David A and Smith, Kynnedy Simone and Reyna, Avery and Zhang, Cecilia and Smith, Brian A.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614490'}"
"Understanding the Experiences, Challenges, and Needs of Dementia Caregivers in the Indian Subcontinent",10.1145/3597638.3614500,"In the realm of dementia care-giving within India, the caregivers often resemble hidden patients, bearing the weight of an often-overlooked role. However, there remains a dearth of comprehensive data regarding the emotional and physical challenges caregivers face in the Indian subcontinent. We present a qualitative study that delves into the realities of being a dementia caregiver in India, investigating caregivers’ experiences and examining the impact of several challenges on their daily lives. Through in-depth interviews, we engaged with four primary caregivers of individuals with dementia, delving into how care-giving shapes their social interactions, mental equilibrium, and emotional states. The findings highlight the importance of having an emotional outlet, a support system, and accessible resources for enhancing caregivers’ quality of life. Drawing from these insights, we propose a set of design implications that can guide future endeavors focused on enhancing the overall well-being of dementia caregivers in India. This research effectively bridges a substantial knowledge gap, extending support and understanding to this vital cohort of individuals who stand at the forefront of dementia care.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Dementia Caregivers, Disability Studies, India, Qualitative Research, Social Isolation', 'numpages': '5', 'articleno': '84', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In the realm of dementia care-giving within India, the caregivers often resemble hidden patients, bearing the weight of an often-overlooked role. However, there remains a dearth of comprehensive data regarding the emotional and physical challenges caregivers face in the Indian subcontinent. We present a qualitative study that delves into the realities of being a dementia caregiver in India, investigating caregivers’ experiences and examining the impact of several challenges on their daily lives. Through in-depth interviews, we engaged with four primary caregivers of individuals with dementia, delving into how care-giving shapes their social interactions, mental equilibrium, and emotional states. The findings highlight the importance of having an emotional outlet, a support system, and accessible resources for enhancing caregivers’ quality of life. Drawing from these insights, we propose a set of design implications that can guide future endeavors focused on enhancing the overall well-being of dementia caregivers in India. This research effectively bridges a substantial knowledge gap, extending support and understanding to this vital cohort of individuals who stand at the forefront of dementia care.', 'doi': '10.1145/3597638.3614500', 'url': 'https://doi.org/10.1145/3597638.3614500', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Understanding the Experiences, Challenges, and Needs of Dementia Caregivers in the Indian Subcontinent', 'author': 'Agrawal, Srishti Shekhar and Panchal, Shrey and He, Liang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614500'}"
Proposing Body Doubling as a Continuum of Space/Time and Mutuality: An Investigation with Neurodivergent Participants,10.1145/3597638.3614486,"Body doubling involves using the presence of others to stay focused on or accomplish tasks. The term has emerged as a community-driven phenomenon employed by neurodivergent individuals and technologically mediated services for body doubling have followed. Yet, no academic exploration exists on the topic. We survey 220 people to investigate how, when, and why they engage in body doubling and how they define it. Most participants acknowledged that they had used this technique long before learning its name. We present the variety of ways people engage in body doubling (e.g., at a caf\'{e}, with YouTube), a diverse range of tasks people utilize it for (e.g., studying, dishes, exercising), and their motivations for doing so (e.g., generating momentum, staying on task). We contribute a two-part model of body doubling as a continuum of space/time and mutuality. We close with implications for future development in this space.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'adaptive strategies, body doubling, community-driven knowledge, neurodiversity', 'numpages': '4', 'articleno': '85', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Body doubling involves using the presence of others to stay focused on or accomplish tasks. The term has emerged as a community-driven phenomenon employed by neurodivergent individuals and technologically mediated services for body doubling have followed. Yet, no academic exploration exists on the topic. We survey 220 people to investigate how, when, and why they engage in body doubling and how they define it. Most participants acknowledged that they had used this technique long before learning its name. We present the variety of ways people engage in body doubling (e.g., at a caf\\'{e}, with YouTube), a diverse range of tasks people utilize it for (e.g., studying, dishes, exercising), and their motivations for doing so (e.g., generating momentum, staying on task). We contribute a two-part model of body doubling as a continuum of space/time and mutuality. We close with implications for future development in this space."", 'doi': '10.1145/3597638.3614486', 'url': 'https://doi.org/10.1145/3597638.3614486', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Proposing Body Doubling as a Continuum of Space/Time and Mutuality: An Investigation with Neurodivergent Participants', 'author': 'Eagle, Tessa and Baltaxe-Admony, Leya Breanna and Ringland, Kathryn E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614486'}"
FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition,10.1145/3597638.3614491,"Camera-based text entry using American Sign Language (ASL) fingerspelling has become more feasible due to recent advancements in recognition technology. However, there are numerous situations where camera-based text entry may not be ideal or acceptable. To address this, we present FingerSpeller, a solution that enables camera-free text entry using smart rings. FingerSpeller utilizes accelerometers embedded in five smart rings from TapStrap, a commercially available wearable keyboard, to track finger motion and recognize fingerspelling. A Hidden Markov Model (HMM) based backend with continuous Gaussian modeling facilitates accurate recognition as evaluated in a real-world deployment. In offline isolated word recognition experiments conducted on a 1,164-word dictionary, FingerSpeller achieves an average character accuracy of 91\% and word accuracy of 87\% across three participants. Furthermore, we demonstrate that the system can be downsized to only two rings while maintaining an accuracy level of approximately 90\% compared to the original configuration. This reduction in form factor enhances user comfort and significantly improves the overall usability of the system.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'fingerspelling, sign language, smart rings, text entry, wearables', 'numpages': '5', 'articleno': '86', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Camera-based text entry using American Sign Language (ASL) fingerspelling has become more feasible due to recent advancements in recognition technology. However, there are numerous situations where camera-based text entry may not be ideal or acceptable. To address this, we present FingerSpeller, a solution that enables camera-free text entry using smart rings. FingerSpeller utilizes accelerometers embedded in five smart rings from TapStrap, a commercially available wearable keyboard, to track finger motion and recognize fingerspelling. A Hidden Markov Model (HMM) based backend with continuous Gaussian modeling facilitates accurate recognition as evaluated in a real-world deployment. In offline isolated word recognition experiments conducted on a 1,164-word dictionary, FingerSpeller achieves an average character accuracy of 91\\% and word accuracy of 87\\% across three participants. Furthermore, we demonstrate that the system can be downsized to only two rings while maintaining an accuracy level of approximately 90\\% compared to the original configuration. This reduction in form factor enhances user comfort and significantly improves the overall usability of the system.', 'doi': '10.1145/3597638.3614491', 'url': 'https://doi.org/10.1145/3597638.3614491', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'FingerSpeller: Camera-Free Text Entry Using Smart Rings for American Sign Language Fingerspelling Recognition', 'author': 'Martin, David and Leng, Zikang and Gemicioglu, Tan and Womack, Jon and Heath, Jocelyn and Neubauer, William C and Kwon, Hyeokhyen and Ploetz, Thomas and Starner, Thad', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614491'}"
A Parametric Design Tool to Support Customized Adaptations of Assistive Technologies for People with Hand Impairments,10.1145/3597638.3614492,"This paper aims to showcase the feasibility of a proof-of-concept parametric software that empowers occupational therapists (OTs) in fabricating customized adaptations of assistive technologies (ATs) to effectively address the distinctive requirements of individuals with hand impairments. The platform enables OTs to modify pre-designed models of utensil grips, serving as examples of ATs, based on individuals' hand assessments and anthropometric data. We conducted observations and interviews to gain insights into the AT adaptation processes by OTs for individuals with hand impairments. Subsequently, using an iterative design process, we created an interface prototype that allows for customized adaptations of ATs and invited an OT to participate in the preliminary testing of the prototype. We provide an outline of the design requirements for parametric modeling tools intended to assist OTs and other clinicians in creating personalized ATs for individuals facing constrained hand motor abilities.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '4', 'articleno': '87', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""This paper aims to showcase the feasibility of a proof-of-concept parametric software that empowers occupational therapists (OTs) in fabricating customized adaptations of assistive technologies (ATs) to effectively address the distinctive requirements of individuals with hand impairments. The platform enables OTs to modify pre-designed models of utensil grips, serving as examples of ATs, based on individuals' hand assessments and anthropometric data. We conducted observations and interviews to gain insights into the AT adaptation processes by OTs for individuals with hand impairments. Subsequently, using an iterative design process, we created an interface prototype that allows for customized adaptations of ATs and invited an OT to participate in the preliminary testing of the prototype. We provide an outline of the design requirements for parametric modeling tools intended to assist OTs and other clinicians in creating personalized ATs for individuals facing constrained hand motor abilities."", 'doi': '10.1145/3597638.3614492', 'url': 'https://doi.org/10.1145/3597638.3614492', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Parametric Design Tool to Support Customized Adaptations of Assistive Technologies for People with Hand Impairments', 'author': 'Li, Mixuan and Aflatoony, Leila', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614492'}"
“It’s All About the Pictures:” Understanding How Parents/Guardians With Visual Impairments Co-Read With Their Child(ren),10.1145/3597638.3614488,"Co-reading, an activity where adults collaboratively read books with child(ren), is important for literacy learning and forming human connection. However, parents and guardians with visual impairments do not experience the same level of access to resources when co-reading with their child(ren) as their sighted counterparts, especially as regards images in children’s books. Through conducting an interview study with five visually impaired parents/guardians, we illuminate the importance parents place on images in children’s books, how they access visual information in children’s print books, and the potential of smart speakers in assisting their existing co-reading practices.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, blind or low vision, co-reading, parents/caregivers', 'numpages': '4', 'articleno': '88', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Co-reading, an activity where adults collaboratively read books with child(ren), is important for literacy learning and forming human connection. However, parents and guardians with visual impairments do not experience the same level of access to resources when co-reading with their child(ren) as their sighted counterparts, especially as regards images in children’s books. Through conducting an interview study with five visually impaired parents/guardians, we illuminate the importance parents place on images in children’s books, how they access visual information in children’s print books, and the potential of smart speakers in assisting their existing co-reading practices.', 'doi': '10.1145/3597638.3614488', 'url': 'https://doi.org/10.1145/3597638.3614488', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': '“It’s All About the Pictures:” Understanding How Parents/Guardians With Visual Impairments Co-Read With Their Child(ren)', 'author': 'Park, Sohyeon and Cassidy, Cameron Tyler and Branham, Stacy M.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614488'}"
"Deploying VizLens: Characterizing User Needs, Preferences, and Challenges of Physical Interfaces Usage in the Wild",10.1145/3597638.3614493,"Blind or Visually Impaired (BVI) people often encounter flat, inaccessible interfaces. Current solutions lack cost-effectiveness, portability, and robustness in real-world settings. We introduce VizLens, a fully-automated, full-stack mobile application powered by computer vision algorithms. The system is deployed and publicly available through the Apple App Store (https://vizlens.org/). From May to August 2023, we had 665 users, who uploaded 1,320 interface images. We aim to use it to study usage patterns and possible challenges BVI users may encounter with flat interfaces through a large-scale study in real-world settings. With in-depth analysis of user data and activity logs, our study will provide insights into BVI users’ interface interests, preferred assistance modes, and potential challenges due to system limitations or users’ diverse abilities. Our goal is to enhance the understanding of how BVI users interact with inaccessible, flat interfaces, and inform future assistive technology design.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, appliances, blind, computer vision, deployment, physical interfaces, visually impaired people', 'numpages': '4', 'articleno': '89', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blind or Visually Impaired (BVI) people often encounter flat, inaccessible interfaces. Current solutions lack cost-effectiveness, portability, and robustness in real-world settings. We introduce VizLens, a fully-automated, full-stack mobile application powered by computer vision algorithms. The system is deployed and publicly available through the Apple App Store (https://vizlens.org/). From May to August 2023, we had 665 users, who uploaded 1,320 interface images. We aim to use it to study usage patterns and possible challenges BVI users may encounter with flat interfaces through a large-scale study in real-world settings. With in-depth analysis of user data and activity logs, our study will provide insights into BVI users’ interface interests, preferred assistance modes, and potential challenges due to system limitations or users’ diverse abilities. Our goal is to enhance the understanding of how BVI users interact with inaccessible, flat interfaces, and inform future assistive technology design.', 'doi': '10.1145/3597638.3614493', 'url': 'https://doi.org/10.1145/3597638.3614493', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Deploying VizLens: Characterizing User Needs, Preferences, and Challenges of Physical Interfaces Usage in the Wild', 'author': 'Xu, Andi and Qazwini, Mahdi and Liang, Chen and Guo, Anhong', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614493'}"
A Demonstration of RASSAR: Room Accessibility and Safety Scanning in Augmented Reality,10.1145/3597638.3614504,"In this demo paper, we introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues using LiDAR and real-time computer vision. Our prototype supports four classes of detection problems: inaccessible object dimensions (e.g., table height), inaccessible object positions (e.g., a light switch out of reach), the presence of unsafe items (e.g., scissors), and the lack of proper assistive devices (e.g., grab bars). RASSAR’s design was informed by a formative interview study with 18 participants from five key stakeholder groups, including wheelchair users, blind and low vision participants, families with young children, and caregivers. Our envisioned use cases include vacation rental hosts, new caregivers, or people with disabilities themselves documenting issues in their homes or rental spaces and planning renovations. We present key findings from our formative interviews, the design of RASSAR, and results from an initial performance evaluation.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Computer Vision, Indoor Accessibility Auditing, Object Detection, Virtual/Augmented Reality', 'numpages': '4', 'articleno': '90', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this demo paper, we introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues using LiDAR and real-time computer vision. Our prototype supports four classes of detection problems: inaccessible object dimensions (e.g., table height), inaccessible object positions (e.g., a light switch out of reach), the presence of unsafe items (e.g., scissors), and the lack of proper assistive devices (e.g., grab bars). RASSAR’s design was informed by a formative interview study with 18 participants from five key stakeholder groups, including wheelchair users, blind and low vision participants, families with young children, and caregivers. Our envisioned use cases include vacation rental hosts, new caregivers, or people with disabilities themselves documenting issues in their homes or rental spaces and planning renovations. We present key findings from our formative interviews, the design of RASSAR, and results from an initial performance evaluation.', 'doi': '10.1145/3597638.3614504', 'url': 'https://doi.org/10.1145/3597638.3614504', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A Demonstration of RASSAR: Room Accessibility and Safety Scanning in Augmented Reality', 'author': 'Su, Xia and Cheng, Kaiming and Zhang, Han and Lee, Jaewook and Olson, Wyatt and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614504'}"
BusStopCV: A Real-time AI Assistant for Labeling Bus Stop Accessibility Features in Streetscape Imagery,10.1145/3597638.3614481,"Public transportation provides vital connectivity to people with disabilities, facilitating access to work, education, and health services. While modern navigation applications provide a suite of information about transit options—including real-time updates about bus or train arrivals—they lack data about the accessibility of the transit stops themselves. Bus stop features such as seatings, shelters, and landing areas are critical, but few cities provide this information. In this demo paper, we introduce BusStopCV, a Human+AI web prototype for scalably collecting data on bus stop features using real-time computer vision and human labeling. We describe BusStopCV’s design, custom training with the YOLOv8 model, and an evaluation of 100 randomly selected bus stops in Seattle, WA. Our findings demonstrate the potential of BusStopCV and highlight opportunities for future work.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessible transit systems, bus stops, computer vision, crowdsourcing, urban accessibility', 'numpages': '6', 'articleno': '91', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Public transportation provides vital connectivity to people with disabilities, facilitating access to work, education, and health services. While modern navigation applications provide a suite of information about transit options—including real-time updates about bus or train arrivals—they lack data about the accessibility of the transit stops themselves. Bus stop features such as seatings, shelters, and landing areas are critical, but few cities provide this information. In this demo paper, we introduce BusStopCV, a Human+AI web prototype for scalably collecting data on bus stop features using real-time computer vision and human labeling. We describe BusStopCV’s design, custom training with the YOLOv8 model, and an evaluation of 100 randomly selected bus stops in Seattle, WA. Our findings demonstrate the potential of BusStopCV and highlight opportunities for future work.', 'doi': '10.1145/3597638.3614481', 'url': 'https://doi.org/10.1145/3597638.3614481', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'BusStopCV: A Real-time AI Assistant for Labeling Bus Stop Accessibility Features in Streetscape Imagery', 'author': 'Kulkarni, Minchu and Li, Chu and Ahn, Jaye Jungmin and Ma, Katrina Oi Yau and Zhang, Zhihan and Saugstad, Michael and Wu, Kevin and Eisenberg, Yochai and Novack, Valerie and Chamberlain, Brent and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614481'}"
Sign Spotter: Design and Initial Evaluation of an Automatic Video-Based American Sign Language Dictionary System,10.1145/3597638.3614497,"Searching unfamiliar American Sign Language (ASL) words in a dictionary is challenging for learners, as it involves recalling signs from memory and providing specific linguistic details. Fortunately, the emergence of sign-recognition technology will soon enable users to search by submitting a video of themselves performing the word. Although previous research has independently addressed algorithmic enhancements and design aspects of ASL dictionaries, there has been limited effort to integrate both. This paper presents the design of an end-to-end sign language dictionary system, incorporating design recommendations from recent human–computer interaction (HCI) research. Additionally, we share preliminary findings from an interview-based user study with four ASL learners.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'American Sign Language (ASL), Dictionary, Search Evaluation, Search Interfaces, Search System Design, Sign Languages, User Satisfaction, Video Search', 'numpages': '5', 'articleno': '92', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Searching unfamiliar American Sign Language (ASL) words in a dictionary is challenging for learners, as it involves recalling signs from memory and providing specific linguistic details. Fortunately, the emergence of sign-recognition technology will soon enable users to search by submitting a video of themselves performing the word. Although previous research has independently addressed algorithmic enhancements and design aspects of ASL dictionaries, there has been limited effort to integrate both. This paper presents the design of an end-to-end sign language dictionary system, incorporating design recommendations from recent human–computer interaction (HCI) research. Additionally, we share preliminary findings from an interview-based user study with four ASL learners.', 'doi': '10.1145/3597638.3614497', 'url': 'https://doi.org/10.1145/3597638.3614497', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Sign Spotter: Design and Initial Evaluation of an Automatic Video-Based American Sign Language Dictionary System', 'author': 'Bohacek, Matyas and Hassan, Saad', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614497'}"
Designing a Human-Centered Intelligent System to Monitor \&amp; Explain Abnormal Patterns of Older Adults,10.1145/3597638.3614501,"Older adult care technologies are increasingly explored to support the independent living of older adults by monitoring their abnormal activities and informing caregivers to provide intervention if necessary. However, the adoption of these technologies remains challenging due to several factors (e.g. lack of usability). In this work, we present a human-centered, intelligent system for older adult care. Our proposed designs of the system were created based on the findings from a focus group session with caregivers. This system monitors the abnormal activities of an older adult using wireless motion sensors and machine learning models. In addition, unlike previous work that only notifies an outcome of activity recognition and abnormal detection models to a caregiver, the system supports interactive dialogue responses to explain the abnormal activities of an older adult to a caregiver and allow the caregiver to elicit additional information about the older adult and the older adult to proactively share his/her status with the caregiver for an adequate intervention.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'aging, caregivers, health/assistive technology, older adults, quality of life, remote monitoring', 'numpages': '5', 'articleno': '93', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Older adult care technologies are increasingly explored to support the independent living of older adults by monitoring their abnormal activities and informing caregivers to provide intervention if necessary. However, the adoption of these technologies remains challenging due to several factors (e.g. lack of usability). In this work, we present a human-centered, intelligent system for older adult care. Our proposed designs of the system were created based on the findings from a focus group session with caregivers. This system monitors the abnormal activities of an older adult using wireless motion sensors and machine learning models. In addition, unlike previous work that only notifies an outcome of activity recognition and abnormal detection models to a caregiver, the system supports interactive dialogue responses to explain the abnormal activities of an older adult to a caregiver and allow the caregiver to elicit additional information about the older adult and the older adult to proactively share his/her status with the caregiver for an adequate intervention.', 'doi': '10.1145/3597638.3614501', 'url': 'https://doi.org/10.1145/3597638.3614501', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Designing a Human-Centered Intelligent System to Monitor \\&amp; Explain Abnormal Patterns of Older Adults', 'author': 'Lee, Min Hun and Siewiorek, Daniel P. and Bernardino, Alexandre', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614501'}"
"Investigating Deaf and Hard-of-Hearing (DHH) Individuals' Online Dating Practices, Self-Presentation, and Disability Disclosure",10.1145/3597638.3614499,"The proliferation of online dating platforms has provided new opportunities for deaf and hard of hearing (DHH) individuals to seek, build, and maintain romantic relationships. However, challenges such as deception, harassment, and aggression widely exist, closely tied to their practices of self-presentation and disability disclosure. Yet, these practices of DHH individuals in online dating communities remain largely unknown. Our study presents a preliminary qualitative analysis of 1,200 posts from a leading DHH online dating community. Our findings reveal that DHH individuals hesitate to reveal their disability while actively disclosing their relationship challenges, especially with respect to hearing-DHH relationships. We have also identified significant research gaps in understanding online DHH dating activities, calling for future research to make online dating environments more inclusive for DHH individuals.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Deaf and hard of hearing, disability disclosure, online dating, self-presentation', 'numpages': '6', 'articleno': '94', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The proliferation of online dating platforms has provided new opportunities for deaf and hard of hearing (DHH) individuals to seek, build, and maintain romantic relationships. However, challenges such as deception, harassment, and aggression widely exist, closely tied to their practices of self-presentation and disability disclosure. Yet, these practices of DHH individuals in online dating communities remain largely unknown. Our study presents a preliminary qualitative analysis of 1,200 posts from a leading DHH online dating community. Our findings reveal that DHH individuals hesitate to reveal their disability while actively disclosing their relationship challenges, especially with respect to hearing-DHH relationships. We have also identified significant research gaps in understanding online DHH dating activities, calling for future research to make online dating environments more inclusive for DHH individuals.', 'doi': '10.1145/3597638.3614499', 'url': 'https://doi.org/10.1145/3597638.3614499', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""Investigating Deaf and Hard-of-Hearing (DHH) Individuals' Online Dating Practices, Self-Presentation, and Disability Disclosure"", 'author': 'Zhu, Xiangrong and Cao, Jiaxun and Niu, Zhe and Zhang, Jingze and Tong, Xin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614499'}"
Towards a System Architecture for Connected Physical and Digital Reminders Using Embodied Objects for People with Dementia,10.1145/3597638.3614506,"Numerous digital reminder solutions have been created on smartphones, tablets, and smart home devices, to support people with dementia. Yet, these digital reminder systems often overlook the importance of non-digital memory aids, such as paper calendars, diaries, and household objects, in the lives of people with dementia. Given recent work suggests that with progression of dementia, users are likely to minimize use of digital devices, accounting for non-digital practices around remembering presents an opportunity that can be leveraged when designing reminder technologies for people with dementia. This poster proposes a voice-based reminder system that connects both physical and digital reminders through embodied home objects and can be customized to the unique routines of the individual with dementia. We present the system architecture and usage scenarios that we implemented for feedback and iteration in future participatory design studies.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Dementia, reminder systems, voice interfaces', 'numpages': '5', 'articleno': '95', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Numerous digital reminder solutions have been created on smartphones, tablets, and smart home devices, to support people with dementia. Yet, these digital reminder systems often overlook the importance of non-digital memory aids, such as paper calendars, diaries, and household objects, in the lives of people with dementia. Given recent work suggests that with progression of dementia, users are likely to minimize use of digital devices, accounting for non-digital practices around remembering presents an opportunity that can be leveraged when designing reminder technologies for people with dementia. This poster proposes a voice-based reminder system that connects both physical and digital reminders through embodied home objects and can be customized to the unique routines of the individual with dementia. We present the system architecture and usage scenarios that we implemented for feedback and iteration in future participatory design studies.', 'doi': '10.1145/3597638.3614506', 'url': 'https://doi.org/10.1145/3597638.3614506', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Towards a System Architecture for Connected Physical and Digital Reminders Using Embodied Objects for People with Dementia', 'author': 'Pradhan, Alisha and Gallier, James and Domjan, Raymond and Dixon, Emma and Bao, Robert and Maddali, Hanuma Teja and Lazar, Amanda', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614506'}"
A Case for Improving the Accessibility of Electrical and Computer Engineering Education – Starting with a Blind Student's Autoethnography,10.1145/3597638.3614551,"Electrical and computer engineering (ECE) theory and tooling are foundational prerequisites for learners’ success in designing and building systems (so called “making” activities) and in several STEM majors in higher education. However, ECE education remains significantly inaccessible to students who are blind or have low vision (BLV), due to the field's weighty reliance on visual content. In this work, we reflect on the autoethnographic account of the first blind student to complete the introductory ECE and making course at our educational institution, identifying three main areas pertaining to the overarching theme of inequitable challenges, including the student's experiences with (1) conceptual learning and tooling, (2) translating theory into practice, and (3) experimenting with learning methods and approaches. We then bring together our findings, previous works, and existing tools to propose a preliminary list of design considerations to inform the development of future accessible tools for ECE education and reduce access barriers to the field.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '8', 'articleno': '96', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Electrical and computer engineering (ECE) theory and tooling are foundational prerequisites for learners’ success in designing and building systems (so called “making” activities) and in several STEM majors in higher education. However, ECE education remains significantly inaccessible to students who are blind or have low vision (BLV), due to the field's weighty reliance on visual content. In this work, we reflect on the autoethnographic account of the first blind student to complete the introductory ECE and making course at our educational institution, identifying three main areas pertaining to the overarching theme of inequitable challenges, including the student's experiences with (1) conceptual learning and tooling, (2) translating theory into practice, and (3) experimenting with learning methods and approaches. We then bring together our findings, previous works, and existing tools to propose a preliminary list of design considerations to inform the development of future accessible tools for ECE education and reduce access barriers to the field."", 'doi': '10.1145/3597638.3614551', 'url': 'https://doi.org/10.1145/3597638.3614551', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""A Case for Improving the Accessibility of Electrical and Computer Engineering Education – Starting with a Blind Student's Autoethnography"", 'author': 'Kulkarni, Trisha and Kim, Gene S-H and Mouallem, Aya', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614551'}"
Designing While Blind: Nonvisual Tools and Inclusive Workflows for Tactile Graphic Creation,10.1145/3597638.3614546,"Blind individuals encounter access barriers while leading the design of media they consume, particularly with tactile graphics. Blind designers are underrepresented in the field of tactile graphic design, where sighted designers routinely control the process and output on their behalf. This lack of representation is caused by inaccessible digital technology that prevents the nonvisual design of tactile graphics by Blind designers. We address these challenges by forming a Blind-led team of four Blind designers, one sighted designer, and one sighted researcher, designing tactile graphics to better understand areas of process improvement. We discuss navigating workflow challenges and reflect on the opportunities of a Blind-led design process. Based on our experience, we contribute recommendations on nonvisual tools and inclusive workflows for practitioners and researchers who wish to engage in accessible design practices. Understanding the challenges and opportunities in Blind-led design spaces supports other disability communities of design practice.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Blind, Co-Design, Design, Low Vision, Tactile Graphics', 'numpages': '8', 'articleno': '97', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blind individuals encounter access barriers while leading the design of media they consume, particularly with tactile graphics. Blind designers are underrepresented in the field of tactile graphic design, where sighted designers routinely control the process and output on their behalf. This lack of representation is caused by inaccessible digital technology that prevents the nonvisual design of tactile graphics by Blind designers. We address these challenges by forming a Blind-led team of four Blind designers, one sighted designer, and one sighted researcher, designing tactile graphics to better understand areas of process improvement. We discuss navigating workflow challenges and reflect on the opportunities of a Blind-led design process. Based on our experience, we contribute recommendations on nonvisual tools and inclusive workflows for practitioners and researchers who wish to engage in accessible design practices. Understanding the challenges and opportunities in Blind-led design spaces supports other disability communities of design practice.', 'doi': '10.1145/3597638.3614546', 'url': 'https://doi.org/10.1145/3597638.3614546', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Designing While Blind: Nonvisual Tools and Inclusive Workflows for Tactile Graphic Creation', 'author': 'Race, Lauren and Fleet, Chancey and Montour, Danielle and Yazzolino, Lindsay and Salsiccia, Marco and Kearney-Volpe, Claire and Wells-Jensen, Sheri and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614546'}"
Co-designing new keyboard and mouse solutions with people living with motor impairments,10.1145/3597638.3614549,"In this report we share a co-design process for developing more accessible alternatives to traditional keyboard and mouse interfaces, involving individuals with motor impairments. We describe our methodology, including initial discovery phases that inspired three subsequent co-design workshops with three individuals with motor impairments and 26 designers. Based on our experience, we highlight the importance of creating an equitable and effective dialogue between designers and individuals with motor impairments, emphasizing the personal nature of each participant’s experiences and the potential of technology as an enabler rather than a generic solution. Guided simulations and hands-on prototyping were employed to trigger meaningful conversations. We underscore the significance of “being with” during the co-design process and the importance of a transparent prototyping and development process for creating genuinely accessible and inclusive interactive systems. By sharing our findings and recommendations, we aim to assist researchers running future co-design workshops that involve prototyping with technology and people of diverse backgrounds.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'co-design, human interface devices, motor impairments, physical disabilities, rapid prototyping', 'numpages': '7', 'articleno': '98', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this report we share a co-design process for developing more accessible alternatives to traditional keyboard and mouse interfaces, involving individuals with motor impairments. We describe our methodology, including initial discovery phases that inspired three subsequent co-design workshops with three individuals with motor impairments and 26 designers. Based on our experience, we highlight the importance of creating an equitable and effective dialogue between designers and individuals with motor impairments, emphasizing the personal nature of each participant’s experiences and the potential of technology as an enabler rather than a generic solution. Guided simulations and hands-on prototyping were employed to trigger meaningful conversations. We underscore the significance of “being with” during the co-design process and the importance of a transparent prototyping and development process for creating genuinely accessible and inclusive interactive systems. By sharing our findings and recommendations, we aim to assist researchers running future co-design workshops that involve prototyping with technology and people of diverse backgrounds.', 'doi': '10.1145/3597638.3614549', 'url': 'https://doi.org/10.1145/3597638.3614549', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Co-designing new keyboard and mouse solutions with people living with motor impairments', 'author': 'Cossovich, Rodolfo and Hodges, Steve and Kang, Jin and Girouard, Audrey', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614549'}"
An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility,10.1145/3597638.3614548,"With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'ableism, accessibility, auto-ethnography, generative artificial intelligence', 'numpages': '8', 'articleno': '99', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'With the recent rapid rise in Generative Artificial Intelligence (GAI) tools, it is imperative that we understand their impact on people with disabilities, both positive and negative. However, although we know that AI in general poses both risks and opportunities for people with disabilities, little is known specifically about GAI in particular. To address this, we conducted a three-month autoethnography of our use of GAI to meet personal and professional needs as a team of researchers with and without disabilities. Our findings demonstrate a wide variety of potential accessibility-related uses for GAI while also highlighting concerns around verifiability, training data, ableism, and false promises.', 'doi': '10.1145/3597638.3614548', 'url': 'https://doi.org/10.1145/3597638.3614548', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': ""An Autoethnographic Case Study of Generative Artificial Intelligence's Utility for Accessibility"", 'author': 'Glazko, Kate S and Yamagami, Momona and Desai, Aashaka and Mack, Kelly Avery and Potluri, Venkatesh and Xu, Xuhai and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614548'}"
Maintaining the Accessibility Ecosystem: a Multi-Stakeholder Analysis of Accessibility in Higher Education,10.1145/3597638.3614547,"People with disabilities face extra hardship in institutions of higher education because of accessibility barriers built into the educational system. While prior work investigates the needs of individual stakeholders, this work offers insights into the communication and collaboration between key stakeholders in creating access in institutions of higher education. The authors present reflections from their experiences working with disability service offices to meet their access needs and the results from interviewing 6 professors and 6 other disabled students about their experience in achieving access. Our results indicate that there are rich opportunities for technological solutions to support these stakeholders in communicating about and creating access.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, accommodations, disability, higher education', 'numpages': '6', 'articleno': '100', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People with disabilities face extra hardship in institutions of higher education because of accessibility barriers built into the educational system. While prior work investigates the needs of individual stakeholders, this work offers insights into the communication and collaboration between key stakeholders in creating access in institutions of higher education. The authors present reflections from their experiences working with disability service offices to meet their access needs and the results from interviewing 6 professors and 6 other disabled students about their experience in achieving access. Our results indicate that there are rich opportunities for technological solutions to support these stakeholders in communicating about and creating access.', 'doi': '10.1145/3597638.3614547', 'url': 'https://doi.org/10.1145/3597638.3614547', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Maintaining the Accessibility Ecosystem: a Multi-Stakeholder Analysis of Accessibility in Higher Education', 'author': 'Mack, Kelly Avery and Sidik, Natasha A and Desai, Aashaka and McDonnell, Emma J and Mehta, Kunal and Zhang, Christina and Mankoff, Jennifer', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614547'}"
Banal Autistic Social Media: A Found Footage Autoethnography,10.1145/3597638.3614552,"In this position paper, we argue for designing autistic social media for thriving neurodiverse online communities. We discuss emerging work in the field of Human-Computer Interaction around autism and social media design, and explore how a Banal Autistic Design approach can lead to a less deficit-focused design paradigm by conducting a Found Footage Autoethnography, a method we introduce as an autoethnography variant suited to autistic embodied ways of knowing. Finally, we juxtapose our design insights with existing design guidelines for autistic social media, and argue for design for customisation, anti-normativity, non-prescripitiveness, granular publics, and interest-centricity.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Autism, Design Power, Neurodivergence, Social Media', 'numpages': '7', 'articleno': '101', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this position paper, we argue for designing autistic social media for thriving neurodiverse online communities. We discuss emerging work in the field of Human-Computer Interaction around autism and social media design, and explore how a Banal Autistic Design approach can lead to a less deficit-focused design paradigm by conducting a Found Footage Autoethnography, a method we introduce as an autoethnography variant suited to autistic embodied ways of knowing. Finally, we juxtapose our design insights with existing design guidelines for autistic social media, and argue for design for customisation, anti-normativity, non-prescripitiveness, granular publics, and interest-centricity.', 'doi': '10.1145/3597638.3614552', 'url': 'https://doi.org/10.1145/3597638.3614552', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Banal Autistic Social Media: A Found Footage Autoethnography', 'author': 'Kender, Kay and Spiel, Katta', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614552'}"
Coding Non-Visually in Visual Studio Code: Collaboration Towards Accessible Development Environment for Blind Programmers,10.1145/3597638.3614550,"This paper delineates a fruitful collaboration between blind and sighted developers, aiming to augment the accessibility of Visual Studio Code (VSCode). Our shared journey is portrayed through examples drawn from our interaction with GitHub issues, pull requests, review processes, and insider’s releases, each contributing to an improved VSCode experience for blind developers. One key milestone of our co-design process is the establishment of an accessible terminal buffer, a significant enhancement for blind developers using VSCode. Other innovative outcomes include Git Diff audio cues, adaptable verbosity settings, intuitive help menus, and a targeted accessibility testing initiative. These tailored improvements not only uplift the accessibility standards of VSCode but also provide a valuable blueprint for open-source developers at large. Through our shared dedication to promoting inclusivity in software development, we aim for the strategies and successes shared in this paper to inspire and guide the open-source community towards crafting more accessible software environments.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, integrated development environment, nonvisual programming, visual studio code', 'numpages': '9', 'articleno': '102', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper delineates a fruitful collaboration between blind and sighted developers, aiming to augment the accessibility of Visual Studio Code (VSCode). Our shared journey is portrayed through examples drawn from our interaction with GitHub issues, pull requests, review processes, and insider’s releases, each contributing to an improved VSCode experience for blind developers. One key milestone of our co-design process is the establishment of an accessible terminal buffer, a significant enhancement for blind developers using VSCode. Other innovative outcomes include Git Diff audio cues, adaptable verbosity settings, intuitive help menus, and a targeted accessibility testing initiative. These tailored improvements not only uplift the accessibility standards of VSCode but also provide a valuable blueprint for open-source developers at large. Through our shared dedication to promoting inclusivity in software development, we aim for the strategies and successes shared in this paper to inspire and guide the open-source community towards crafting more accessible software environments.', 'doi': '10.1145/3597638.3614550', 'url': 'https://doi.org/10.1145/3597638.3614550', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Coding Non-Visually in Visual Studio Code: Collaboration Towards Accessible Development Environment for Blind Programmers', 'author': 'Seo, JooYoung and Rogge, Megan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3614550'}"
Designing Voice-Assisted Technology (VAT) Training for Activities of Daily Living (ADLs) for Adults with Cognitive-Communication Needs (CCNs) at Home,10.1145/3597638.3615656,"Due to challenges in communication and cognition (e.g., attention, memory, organization, etc.), individuals with cognitive-communication needs (CCNs) experience difficulties performing activities of daily living, or ADLs (e.g., self-care, meal preparation, managing different routines and appointments). Voice-assisted technology (VAT) has been shown to support these individuals for ADLs; however, there are limited clinician-guided VAT training programs available for individuals with CCNs to use VAT for ADLs. This study aims to design and implement a 6-week virtual VAT training program focused on using Amazon Alexa voice commands to improve independence in ADL tasks for three adults with CCNs. This research study describes the collaborative design process of the VAT Training program by a clinical team and the 6-week delivery of the training program. By analyzing training session videos and participant interviews, the benefits and challenges from the training, as well as future plans to improve the implementation of VAT for telehealth are discussed.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '16', 'articleno': '103', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Due to challenges in communication and cognition (e.g., attention, memory, organization, etc.), individuals with cognitive-communication needs (CCNs) experience difficulties performing activities of daily living, or ADLs (e.g., self-care, meal preparation, managing different routines and appointments). Voice-assisted technology (VAT) has been shown to support these individuals for ADLs; however, there are limited clinician-guided VAT training programs available for individuals with CCNs to use VAT for ADLs. This study aims to design and implement a 6-week virtual VAT training program focused on using Amazon Alexa voice commands to improve independence in ADL tasks for three adults with CCNs. This research study describes the collaborative design process of the VAT Training program by a clinical team and the 6-week delivery of the training program. By analyzing training session videos and participant interviews, the benefits and challenges from the training, as well as future plans to improve the implementation of VAT for telehealth are discussed.', 'doi': '10.1145/3597638.3615656', 'url': 'https://doi.org/10.1145/3597638.3615656', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Designing Voice-Assisted Technology (VAT) Training for Activities of Daily Living (ADLs) for Adults with Cognitive-Communication Needs (CCNs) at Home', 'author': ""O'Connor, Claire and Kim, Lauren H and Byun, Ginna and Vora, Priyal and Du, Yao"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615656'}"
Analyzing Walking with Ankle Foot Orthoses Using Shank-mounted Wearable Movement Sensors,10.1145/3597638.3615654,"An Ankle Foot Orthosis (AFO) is a device that supports the ankle joint to help improve gait. When fitting AFOs, clinicians tune the AFO, and the success of the tuning is often determined by observation, e.g. of a patient walking up and down a corridor. As a result, clinicians do not get a true picture of the patient's gait, therefore this study investigates the use of Inertial Measurement Units (IMU) for gait analysis. The aim is to provide clinicians with quantitative data about the patient's gait in real-world situations, helping them to determine how well the AFOs are performing. In this study, data were collected from one adult volunteer who was a regular wearer of two AFOs. The volunteer performed a series of activities while wearing the AFOs with different heel wedge heights and also while not wearing the AFOs. Data were collected from six IMU sensors but only the data from the shank sensors was used to perform gait analysis. The results from the gait analysis show that the IMU is sensitive to changes in angle- and time-based metrics whilst the volunteer was and was not wearing AFOs. This paper proposes a novel metric called negative momentum, which captures the continual progression of the shank forward which can be an indicator to show if the AFO is tuned correctly. This study suggests that two shank-mounted IMUs can be sufficient to perform gait analysis without the need of a full gait analysis lab.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Ankle Foot Orthoses (AFO), Inertial Measurement Unit (IMU), gait, gait analysis', 'numpages': '6', 'articleno': '104', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""An Ankle Foot Orthosis (AFO) is a device that supports the ankle joint to help improve gait. When fitting AFOs, clinicians tune the AFO, and the success of the tuning is often determined by observation, e.g. of a patient walking up and down a corridor. As a result, clinicians do not get a true picture of the patient's gait, therefore this study investigates the use of Inertial Measurement Units (IMU) for gait analysis. The aim is to provide clinicians with quantitative data about the patient's gait in real-world situations, helping them to determine how well the AFOs are performing. In this study, data were collected from one adult volunteer who was a regular wearer of two AFOs. The volunteer performed a series of activities while wearing the AFOs with different heel wedge heights and also while not wearing the AFOs. Data were collected from six IMU sensors but only the data from the shank sensors was used to perform gait analysis. The results from the gait analysis show that the IMU is sensitive to changes in angle- and time-based metrics whilst the volunteer was and was not wearing AFOs. This paper proposes a novel metric called negative momentum, which captures the continual progression of the shank forward which can be an indicator to show if the AFO is tuned correctly. This study suggests that two shank-mounted IMUs can be sufficient to perform gait analysis without the need of a full gait analysis lab."", 'doi': '10.1145/3597638.3615654', 'url': 'https://doi.org/10.1145/3597638.3615654', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Analyzing Walking with Ankle Foot Orthoses Using Shank-mounted Wearable Movement Sensors', 'author': 'Blackwell, Cliona', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615654'}"
AudioQ: A Debugging Extension for Visually Impaired Developers,10.1145/3597638.3615655,"Lately, integrated development environments (IDEs) for software development, such as Visual Studio Code, have seen an increase in usage to enhance efficiency while coding. Unfortunately, the lack of accessibility, specifically for visually impaired developers, does not allow for universal participation. Previous research demonstrates how current accessibility tools such as screen readers simply read out the code and errors but fail to acknowledge the error location and type. To address this issue, our team used Visual Studio Code's Extension Builder to design and develop a debugging extension for visually impaired developers. This extension utilizes the NVDA screen reader to read out the line number and category of the error the user is fixing. This extension was tested by visually impaired developers through surveys and timing how long it takes the developer to solve coding problems with and without our extension. Testing resulted in average times of around 5 minutes to locate all possible errors and debug. The results obtained from the study can be used as a beacon in the future development on the topic of software programming accessibility. This project solves an extensive problem not only seen by visually impaired people, but by all software developers in general. With an efficient tool to aid the debugging process, developers can execute programs with more time in hand and greater productivity. As a result, large scale projects can be developed at a faster pace while still maintaining greater efficiency.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'numpages': '3', 'articleno': '105', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Lately, integrated development environments (IDEs) for software development, such as Visual Studio Code, have seen an increase in usage to enhance efficiency while coding. Unfortunately, the lack of accessibility, specifically for visually impaired developers, does not allow for universal participation. Previous research demonstrates how current accessibility tools such as screen readers simply read out the code and errors but fail to acknowledge the error location and type. To address this issue, our team used Visual Studio Code's Extension Builder to design and develop a debugging extension for visually impaired developers. This extension utilizes the NVDA screen reader to read out the line number and category of the error the user is fixing. This extension was tested by visually impaired developers through surveys and timing how long it takes the developer to solve coding problems with and without our extension. Testing resulted in average times of around 5 minutes to locate all possible errors and debug. The results obtained from the study can be used as a beacon in the future development on the topic of software programming accessibility. This project solves an extensive problem not only seen by visually impaired people, but by all software developers in general. With an efficient tool to aid the debugging process, developers can execute programs with more time in hand and greater productivity. As a result, large scale projects can be developed at a faster pace while still maintaining greater efficiency."", 'doi': '10.1145/3597638.3615655', 'url': 'https://doi.org/10.1145/3597638.3615655', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'AudioQ: A Debugging Extension for Visually Impaired Developers', 'author': 'Kumar, Sehej and Kotla, Shreyas', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615655'}"
Tackling the Lack of a Practical Guide in Disability-Centered Research,10.1145/3597638.3615650,"Accessibility research strives to develop technology that is useful for disabled people, but the research processes that we engage in do not always center disabled people in a way that allows us to shape artifacts so that they benefit disabled communities. In this workshop, we want to address core questions that are relevant in this context: How can research questions be defined in a way that shares power between research teams and technology users? How should research processes be designed to be broadly accessible for disabled people? And what are equitable ways of summarizing and sharing research findings in a way that allows disabled communities to critically appraise findings with us? Through discussion among all attendees, we want to develop a practical guide in disability-centered research that will be made available and further developed as a community resource when engaging in accessibility research.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Access, Disability Justice, Research Methods', 'numpages': '5', 'articleno': '106', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Accessibility research strives to develop technology that is useful for disabled people, but the research processes that we engage in do not always center disabled people in a way that allows us to shape artifacts so that they benefit disabled communities. In this workshop, we want to address core questions that are relevant in this context: How can research questions be defined in a way that shares power between research teams and technology users? How should research processes be designed to be broadly accessible for disabled people? And what are equitable ways of summarizing and sharing research findings in a way that allows disabled communities to critically appraise findings with us? Through discussion among all attendees, we want to develop a practical guide in disability-centered research that will be made available and further developed as a community resource when engaging in accessibility research.', 'doi': '10.1145/3597638.3615650', 'url': 'https://doi.org/10.1145/3597638.3615650', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Tackling the Lack of a Practical Guide in Disability-Centered Research', 'author': 'McDonnell, Emma J. and Mack, Kelly Avery and Gerling, Kathrin and Spiel, Katta and Bennett, Cynthia L. and Brewer, Robin N. and Williams, Rua Mae and Tigwell, Garreth W.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615650'}"
A11yFutures: Envisioning the Future of Accessibility Research,10.1145/3597638.3615652,"The future of accessibility research is a topic we take up every day as researchers; yet it is important also to step back and ask ourselves about the most important, and overlooked, areas for inquiry in our field. With the rapid pace of change in both computational capabilities and the environmental, social, and political context in which disability plays out, we believe this is a critical time for such inquiry. This is even more important given the relatively narrow set of topics that accessibility research has focused on for most of our field’s history. We invite our community to come together to define what the next generation of accessibility research should engage with.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility', 'numpages': '4', 'articleno': '107', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The future of accessibility research is a topic we take up every day as researchers; yet it is important also to step back and ask ourselves about the most important, and overlooked, areas for inquiry in our field. With the rapid pace of change in both computational capabilities and the environmental, social, and political context in which disability plays out, we believe this is a critical time for such inquiry. This is even more important given the relatively narrow set of topics that accessibility research has focused on for most of our field’s history. We invite our community to come together to define what the next generation of accessibility research should engage with.', 'doi': '10.1145/3597638.3615652', 'url': 'https://doi.org/10.1145/3597638.3615652', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'A11yFutures: Envisioning the Future of Accessibility Research', 'author': 'Mankoff, Jennifer and Mack, Kelly Avery and Wiese, Jason and Crawford, Kirk Andrew and Hamidi, Foad', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615652'}"
Bridging the Gap: Towards Advancing Privacy and Accessibility,10.1145/3597638.3615653,"The privacy dimensions of accessibility technologies are often understudied and overlooked. Very little prior research has investigated the privacy concerns of disabled people, and much less has studied the barriers of privacy-preserving techniques. In order to address this gap and bridge between two separate communities (accessibility and privacy), our one-day workshop explores how researchers might design and build technologies that are both accessible and privacy-preserving.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'accessibility, disability, privacy, security', 'numpages': '4', 'articleno': '108', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The privacy dimensions of accessibility technologies are often understudied and overlooked. Very little prior research has investigated the privacy concerns of disabled people, and much less has studied the barriers of privacy-preserving techniques. In order to address this gap and bridge between two separate communities (accessibility and privacy), our one-day workshop explores how researchers might design and build technologies that are both accessible and privacy-preserving.', 'doi': '10.1145/3597638.3615653', 'url': 'https://doi.org/10.1145/3597638.3615653', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Bridging the Gap: Towards Advancing Privacy and Accessibility', 'author': 'Alharbi, Rahaf and Brewer, Robin N. and India, Gesu and Zhang, Lotus and Findlater, Leah and Zou, Yixin and Stangl, Abigale', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615653'}"
Accessibility Research and Users with Multiple Disabilities or Complex Needs,10.1145/3597638.3615651,"Conventionally, the accessibility research community centers most of its efforts on designing assistive technologies and systems related to single categories of impairments. Although this approach has contributed to valuable progress and advancements in the field, there is a growing consensus among accessibility researchers that focusing on designing technologies for single impairments oversimplifies disability since this approach may fail to adequately address the real-world experiences of a significant population of users with complex needs. Despite challenges related to conducting research with users living with multiple, profound, or complex disabilities, it is essential in terms of future work that the accessibility research community adopts a more inclusive approach where users with lived experience of multiple disabilities are directly informing and shaping the design of assistive systems and accessible technologies. Therefore, we propose the 2nd International Workshop on Accessibility Research and Users with Multiple Disabilities or Complex Needs. The workshop will act as a forum for participants to share their perspectives related to challenges and opportunities in designing accessible systems that consider the multidimensional needs of users living with multiple disabilities. This workshop intends to challenge current paradigms in the accessibility field, share latest work and foster future collaborations.","{'series': ""ASSETS '23"", 'location': 'New York, NY, USA', 'keywords': 'Accessibility, Assistive Technologies, Complex Needs, Disability Studies, Multiple Impairments', 'numpages': '6', 'articleno': '109', 'booktitle': 'Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Conventionally, the accessibility research community centers most of its efforts on designing assistive technologies and systems related to single categories of impairments. Although this approach has contributed to valuable progress and advancements in the field, there is a growing consensus among accessibility researchers that focusing on designing technologies for single impairments oversimplifies disability since this approach may fail to adequately address the real-world experiences of a significant population of users with complex needs. Despite challenges related to conducting research with users living with multiple, profound, or complex disabilities, it is essential in terms of future work that the accessibility research community adopts a more inclusive approach where users with lived experience of multiple disabilities are directly informing and shaping the design of assistive systems and accessible technologies. Therefore, we propose the 2nd International Workshop on Accessibility Research and Users with Multiple Disabilities or Complex Needs. The workshop will act as a forum for participants to share their perspectives related to challenges and opportunities in designing accessible systems that consider the multidimensional needs of users living with multiple disabilities. This workshop intends to challenge current paradigms in the accessibility field, share latest work and foster future collaborations.', 'doi': '10.1145/3597638.3615651', 'url': 'https://doi.org/10.1145/3597638.3615651', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9798400702204', 'year': '2023', 'title': 'Accessibility Research and Users with Multiple Disabilities or Complex Needs', 'author': 'Theil, Arthur and Anderton, Craig and Creed, Chris and Olson, Nasrine and Holt, Raymond John and Sarcar, Sayan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3597638.3615651'}"