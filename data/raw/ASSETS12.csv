Title,DOI,Abstract,BibTeX
Session details: Screen reader usage,10.1145/3245891,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245891', 'url': 'https://doi.org/10.1145/3245891', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Screen reader usage', 'author': 'Bigham, Jeff', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245891'}"
Back navigation shortcuts for screen reader users,10.1145/2384916.2384918,"When screen reader users need to back track pages to re-find previously visited content, they are forced to listen to some portion of each unwanted page to recognize it. This makes aural back navigation inefficient, especially on large websites. To address this problem, we introduce topic- and list-based back: two navigation strategies that provide back browsing shortcuts by leveraging the conceptual structure of content-rich websites. Both are manifested in Webtime, an accessible website on the history of the Web. A controlled study (N=10) conducted at the Indiana School for the Blind and Visually Impaired compared topic- and list-based back to traditional back mechanisms while participants completed fact-finding tasks. Topic- and list-based back significantly decreased time-on-task and number of backtracked pages; the navigation shortcuts were also associated with positive improvements in perceived cognitive effort and navigation experience. The proposed strategies can operate as a supplement to current back mechanisms in information-rich websites.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'screen reader users, information architecture, back navigation, assistive technology', 'numpages': '8', 'pages': '1–8', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'When screen reader users need to back track pages to re-find previously visited content, they are forced to listen to some portion of each unwanted page to recognize it. This makes aural back navigation inefficient, especially on large websites. To address this problem, we introduce topic- and list-based back: two navigation strategies that provide back browsing shortcuts by leveraging the conceptual structure of content-rich websites. Both are manifested in Webtime, an accessible website on the history of the Web. A controlled study (N=10) conducted at the Indiana School for the Blind and Visually Impaired compared topic- and list-based back to traditional back mechanisms while participants completed fact-finding tasks. Topic- and list-based back significantly decreased time-on-task and number of backtracked pages; the navigation shortcuts were also associated with positive improvements in perceived cognitive effort and navigation experience. The proposed strategies can operate as a supplement to current back mechanisms in information-rich websites.', 'doi': '10.1145/2384916.2384918', 'url': 'https://doi.org/10.1145/2384916.2384918', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Back navigation shortcuts for screen reader users', 'author': 'Rohani Ghahari, Romisa and Ferati, Mexhid and Yang, Tao and Bolchini, Davide', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384918'}"
Capture: a desktop display-centric text recorder,10.1145/2384916.2384919,"As more and more information is designed for human visual consumption through computer displays, the need to capture and process display-centric content is becoming increasingly important, especially for visually impaired users. We present Capture, a novel display-centric text recorder that facilitates real-time access to onscreen text and its structure and contextual information, including data associated with both foreground and background windows. Capture provides an intelligent caching architecture that integrates with the standard accessibility framework available on modern operating systems to continuously track onscreen text and metadata. This enables fast, semantic information recording without any modifications to applications, window systems, or operating system kernels. The recorded data is useful for a variety of problem domains, including assistive technologies, desktop search, auditing, and predictive graphical user interfaces. We have implemented a Capture prototype on Linux with the GNOME Accessibility Toolkit. Our results on real desktop applications demonstrate that Capture provides low runtime overhead and much more complete recording of onscreen text than modern desktop screen readers used for visually impaired users.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'screen capture, assistive technology, accessibility', 'numpages': '8', 'pages': '9–16', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'As more and more information is designed for human visual consumption through computer displays, the need to capture and process display-centric content is becoming increasingly important, especially for visually impaired users. We present Capture, a novel display-centric text recorder that facilitates real-time access to onscreen text and its structure and contextual information, including data associated with both foreground and background windows. Capture provides an intelligent caching architecture that integrates with the standard accessibility framework available on modern operating systems to continuously track onscreen text and metadata. This enables fast, semantic information recording without any modifications to applications, window systems, or operating system kernels. The recorded data is useful for a variety of problem domains, including assistive technologies, desktop search, auditing, and predictive graphical user interfaces. We have implemented a Capture prototype on Linux with the GNOME Accessibility Toolkit. Our results on real desktop applications demonstrate that Capture provides low runtime overhead and much more complete recording of onscreen text than modern desktop screen readers used for visually impaired users.', 'doi': '10.1145/2384916.2384919', 'url': 'https://doi.org/10.1145/2384916.2384919', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Capture: a desktop display-centric text recorder', 'author': 'Laadan, Oren and Shu, Andrew and Nieh, Jason', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384919'}"
Thematic organization of web content for distraction-free text-to-speech narration,10.1145/2384916.2384920,"People with visual disabilities, especially those who are blind, have digital content narrated to them by text-to-speech (TTS) engines (e.g., with the help of screen readers). Naively narrating web pages, particularly the ones consisting of several diverse pieces (e.g., news summaries, opinion pieces, taxonomy, ads), with TTS engines without organizing them into thematic segments will make it very difficult for the blind user to mentally separate out and comprehend the essential elements in a segment, and the effort to do so can cause significant cognitive stress. One can alleviate this difficulty by segmenting web pages into thematic pieces and then narrating each of them separately. Extant segmentation methods typically segment web pages using visual and structural cues. The use of such cues without taking into account the semantics of the content, tends to produce ""impure"" segments containing extraneous material interspersed with the essential elements. In this paper, we describe a new technique for identifying thematic segments by tightly coupling visual, structural, and linguistic features present in the content. A notable aspect of the technique is that it produces segments with very little irrelevant content. Another interesting aspect is that the clutter-free main content of a web page, that is produced by the Readability tool and the ""Reader"" feature of the Safari browser, emerges as a special case of the thematic segments created by our technique. We provide experimental evidence of the effectiveness of our technique in reducing clutter. We also describe a user study with 23 blind subjects of its impact on web accessibility.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'singular value decomposition, segmentation, screen readers, clustering, blind users', 'numpages': '8', 'pages': '17–24', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People with visual disabilities, especially those who are blind, have digital content narrated to them by text-to-speech (TTS) engines (e.g., with the help of screen readers). Naively narrating web pages, particularly the ones consisting of several diverse pieces (e.g., news summaries, opinion pieces, taxonomy, ads), with TTS engines without organizing them into thematic segments will make it very difficult for the blind user to mentally separate out and comprehend the essential elements in a segment, and the effort to do so can cause significant cognitive stress. One can alleviate this difficulty by segmenting web pages into thematic pieces and then narrating each of them separately. Extant segmentation methods typically segment web pages using visual and structural cues. The use of such cues without taking into account the semantics of the content, tends to produce ""impure"" segments containing extraneous material interspersed with the essential elements. In this paper, we describe a new technique for identifying thematic segments by tightly coupling visual, structural, and linguistic features present in the content. A notable aspect of the technique is that it produces segments with very little irrelevant content. Another interesting aspect is that the clutter-free main content of a web page, that is produced by the Readability tool and the ""Reader"" feature of the Safari browser, emerges as a special case of the thematic segments created by our technique. We provide experimental evidence of the effectiveness of our technique in reducing clutter. We also describe a user study with 23 blind subjects of its impact on web accessibility.', 'doi': '10.1145/2384916.2384920', 'url': 'https://doi.org/10.1145/2384916.2384920', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Thematic organization of web content for distraction-free text-to-speech narration', 'author': 'Islam, Muhammad Asiful and Ahmed, Faisal and Borodin, Yevgen and Ramakrishnan, I.V.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384920'}"
Session details: Designing for older adults,10.1145/3245892,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245892', 'url': 'https://doi.org/10.1145/3245892', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Designing for older adults', 'author': 'Hanson, Vicki', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245892'}"
Basic senior personas: a representative design tool covering the spectrum of European older adults,10.1145/2384916.2384922,"The persona method is a powerful approach to focus on needs and characteristics of target users, keeping complex user data,numbers and diagrams alive during the whole design cycle.However, the development of prosperous personas requires a considerable amount of time, effort and specific skills. This paper introduces the development of a set of 30 basic senior personas, covering a broad range of characteristics of European older adults, following a quantitative development approach. The aim of this tool is to support researchers and developers in extending empathy for their target users when developing ICT solutions for the benefit of older adults. The main innovation lies in the representativeness of the basic senior personas. The personas build on multifaceted quantitative data from a single source including micro-level information from roughly 12,500 older individuals living in different European countries. The resulting personas may be applied in their basic form but are extendable to specific contexts. Also, the suggested tool addresses the drawbacks of current existing personas describing older adults: being representative and cost-efficient. The basic senior personas, a filter tool, a manual and templates for ""persona marketing"" articles are available for free online under http://elderlypersonas.cure.at.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user requirements, user centered design methods, personas layering, persona re-usage, persona method, older adults with diverse capabilities, basic personas, ambient assisted living', 'numpages': '8', 'pages': '25–32', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The persona method is a powerful approach to focus on needs and characteristics of target users, keeping complex user data,numbers and diagrams alive during the whole design cycle.However, the development of prosperous personas requires a considerable amount of time, effort and specific skills. This paper introduces the development of a set of 30 basic senior personas, covering a broad range of characteristics of European older adults, following a quantitative development approach. The aim of this tool is to support researchers and developers in extending empathy for their target users when developing ICT solutions for the benefit of older adults. The main innovation lies in the representativeness of the basic senior personas. The personas build on multifaceted quantitative data from a single source including micro-level information from roughly 12,500 older individuals living in different European countries. The resulting personas may be applied in their basic form but are extendable to specific contexts. Also, the suggested tool addresses the drawbacks of current existing personas describing older adults: being representative and cost-efficient. The basic senior personas, a filter tool, a manual and templates for ""persona marketing"" articles are available for free online under http://elderlypersonas.cure.at.', 'doi': '10.1145/2384916.2384922', 'url': 'https://doi.org/10.1145/2384916.2384922', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Basic senior personas: a representative design tool covering the spectrum of European older adults', 'author': 'W\\""{o}ckl, Bernhard and Yildizoglu, Ulcay and Buber, Isabella and Aparicio Diaz, Belinda and Kruijff, Ernst and Tscheligi, Manfred', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384922'}"
Considerations for technology that support physical activity by older adults,10.1145/2384916.2384923,"Barriers to physical activity prevent older adults from meeting recommended physical activity levels necessary for maintaining quality of life. As technology becomes more advanced, we have the opportunity and the responsibility to address concerns faced by the aging population. We seek opportunities for technology to empower older adults to overcome barriers on their own by interviewing and learning from older adults who have successfully overcome these barriers. In this paper, we present a set of needs that technology can address, and considerations for designing technology interventions that support physical activity by older adults.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'technology interventions, physical activity, older adults, barriers', 'numpages': '8', 'pages': '33–40', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Barriers to physical activity prevent older adults from meeting recommended physical activity levels necessary for maintaining quality of life. As technology becomes more advanced, we have the opportunity and the responsibility to address concerns faced by the aging population. We seek opportunities for technology to empower older adults to overcome barriers on their own by interviewing and learning from older adults who have successfully overcome these barriers. In this paper, we present a set of needs that technology can address, and considerations for designing technology interventions that support physical activity by older adults.', 'doi': '10.1145/2384916.2384923', 'url': 'https://doi.org/10.1145/2384916.2384923', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Considerations for technology that support physical activity by older adults', 'author': 'Fan, Chloe and Forlizzi, Jodi and Dey, Anind', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384923'}"
Design recommendations for tv user interfaces for older adults: findings from the eCAALYX project,10.1145/2384916.2384924,"While guidelines for designing websites and iTV applications for older adults exist, no previous work has suggested how to best design TV user interfaces (UIs) that are accessible to older adults. Building upon pertinent guidelines from related areas, this paper presents thirteen recommendations for designing UIs for TV applications for older adults. These recommendations are the result of iterative design, testing, and development of a TV-based health system for older adults that aims to provide a holistic solution to improve quality of life for older adults with chronic conditions by fostering their autonomy and reducing hospitalization costs. The authors' work and experience shows that widely known UI design guidelines unsurprisingly apply to the design of TV-based applications for older adults, but acquire a crucial importance in this context.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user interface design, tv, older adults, design recommendations', 'numpages': '8', 'pages': '41–48', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""While guidelines for designing websites and iTV applications for older adults exist, no previous work has suggested how to best design TV user interfaces (UIs) that are accessible to older adults. Building upon pertinent guidelines from related areas, this paper presents thirteen recommendations for designing UIs for TV applications for older adults. These recommendations are the result of iterative design, testing, and development of a TV-based health system for older adults that aims to provide a holistic solution to improve quality of life for older adults with chronic conditions by fostering their autonomy and reducing hospitalization costs. The authors' work and experience shows that widely known UI design guidelines unsurprisingly apply to the design of TV-based applications for older adults, but acquire a crucial importance in this context."", 'doi': '10.1145/2384916.2384924', 'url': 'https://doi.org/10.1145/2384916.2384924', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Design recommendations for tv user interfaces for older adults: findings from the eCAALYX project', 'author': 'Nunes, Francisco and Kerwin, Maureen and Silva, Paula Alexandra', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384924'}"
Session details: Communication aids,10.1145/3245893,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245893', 'url': 'https://doi.org/10.1145/3245893', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Communication aids', 'author': 'Sporka, Adam', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245893'}"
What we talk about: designing a context-aware communication tool for people with aphasia,10.1145/2384916.2384926,"Many people with aphasia experience difficulty recalling words extemporaneously, but can recognize those words when given an image, text, or audio prompt. Augmented and alternative communication (AAC) systems can help address this problem by enabling people with aphasia to browse and select from a list of vocabulary words. However, these systems can be difficult to navigate, especially when they contain large amounts of content. In this paper, we describe the design of TalkAbout, a context-aware, adaptive AAC system that provides users with a word list that is adapted to their current location and conversation partner. We describe the design and development of TalkAbout, which we conducted in collaboration with 5 adults with aphasia. We then present guidelines for developing and evaluating context-aware technology for people with aphasia.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'participatory design, context-aware computing, augmented and alternative communication, aphasia, accessibility', 'numpages': '8', 'pages': '49–56', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Many people with aphasia experience difficulty recalling words extemporaneously, but can recognize those words when given an image, text, or audio prompt. Augmented and alternative communication (AAC) systems can help address this problem by enabling people with aphasia to browse and select from a list of vocabulary words. However, these systems can be difficult to navigate, especially when they contain large amounts of content. In this paper, we describe the design of TalkAbout, a context-aware, adaptive AAC system that provides users with a word list that is adapted to their current location and conversation partner. We describe the design and development of TalkAbout, which we conducted in collaboration with 5 adults with aphasia. We then present guidelines for developing and evaluating context-aware technology for people with aphasia.', 'doi': '10.1145/2384916.2384926', 'url': 'https://doi.org/10.1145/2384916.2384926', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'What we talk about: designing a context-aware communication tool for people with aphasia', 'author': 'Kane, Shaun K. and Linam-Church, Barbara and Althoff, Kyle and McCall, Denise', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384926'}"
iSCAN: a phoneme-based predictive communication aid for nonspeaking individuals,10.1145/2384916.2384927,"The high incidence of literacy deficits among people with severe speech impairments (SSI) has been well documented. Without literacy skills, people with SSI are unable to effectively use orthographic-based communication systems to generate novel linguistic items in spontaneous conversation. To address this problem, phoneme-based communication systems have been proposed which enable users to create spoken output from phoneme sequences. In this paper, we investigate whether prediction techniques can be employed to improve the usability of such systems. We have developed iSCAN, a phoneme-based predictive communication system, which offers phoneme prediction and phoneme-based word prediction. A pilot study with 16 able-bodied participants showed that our predictive methods led to a 108.4\% increase in phoneme entry speed and a 79.0\% reduction in phoneme error rate. The benefits of the predictive methods were also demonstrated in a case study with a cerebral palsied participant. Moreover, results of a comparative evaluation conducted with the same participant after 16 sessions using iSCAN indicated that our system outperformed an orthographic-based predictive communication device that the participant has used for over 4 years.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'word prediction, phoneme-based communication, phoneme prediction, augmentative and alternative communication', 'numpages': '8', 'pages': '57–64', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The high incidence of literacy deficits among people with severe speech impairments (SSI) has been well documented. Without literacy skills, people with SSI are unable to effectively use orthographic-based communication systems to generate novel linguistic items in spontaneous conversation. To address this problem, phoneme-based communication systems have been proposed which enable users to create spoken output from phoneme sequences. In this paper, we investigate whether prediction techniques can be employed to improve the usability of such systems. We have developed iSCAN, a phoneme-based predictive communication system, which offers phoneme prediction and phoneme-based word prediction. A pilot study with 16 able-bodied participants showed that our predictive methods led to a 108.4\\% increase in phoneme entry speed and a 79.0\\% reduction in phoneme error rate. The benefits of the predictive methods were also demonstrated in a case study with a cerebral palsied participant. Moreover, results of a comparative evaluation conducted with the same participant after 16 sessions using iSCAN indicated that our system outperformed an orthographic-based predictive communication device that the participant has used for over 4 years.', 'doi': '10.1145/2384916.2384927', 'url': 'https://doi.org/10.1145/2384916.2384927', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'iSCAN: a phoneme-based predictive communication aid for nonspeaking individuals', 'author': 'Trinh, Ha and Waller, Annalu and Vertanen, Keith and Kristensson, Per Ola and Hanson, Vicki L.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384927'}"
Detecting linguistic HCI markers in an online aphasia support group,10.1145/2384916.2384928,"Aphasia is an acquired language disorder resulting from trauma or injury to language areas of the brain. Despite extensive research on the impact of aphasia on traditional forms of communication, little is known about the impact of aphasia on computer-mediated communication (CMC). In this study we asked whether the well-documented language deficits associated with aphasia can be detected in online writing of people with aphasia. We analyzed 150 messages (14,754 words) posted to an online aphasia support forum, by six people with aphasia and by four controls. Significant linguistic differences between people with aphasia and controls were detected, suggesting five putative linguistic HCI markers for aphasia. These findings suggest that interdisciplinary research on communication disorders and CMC has both applied and theoretical implications.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user modeling, unobtrusive monitoring, online support groups, human factors, hci markers, computer-mediated communication, aphasia', 'numpages': '6', 'pages': '65–70', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Aphasia is an acquired language disorder resulting from trauma or injury to language areas of the brain. Despite extensive research on the impact of aphasia on traditional forms of communication, little is known about the impact of aphasia on computer-mediated communication (CMC). In this study we asked whether the well-documented language deficits associated with aphasia can be detected in online writing of people with aphasia. We analyzed 150 messages (14,754 words) posted to an online aphasia support forum, by six people with aphasia and by four controls. Significant linguistic differences between people with aphasia and controls were detected, suggesting five putative linguistic HCI markers for aphasia. These findings suggest that interdisciplinary research on communication disorders and CMC has both applied and theoretical implications.', 'doi': '10.1145/2384916.2384928', 'url': 'https://doi.org/10.1145/2384916.2384928', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Detecting linguistic HCI markers in an online aphasia support group', 'author': 'Kalman, Yoram M. and Geraghty, Kathleen and Thompson, Cynthia K. and Gergle, Darren', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384928'}"
Session details: Accessibility at large,10.1145/3245894,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245894', 'url': 'https://doi.org/10.1145/3245894', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Accessibility at large', 'author': 'Harper, Simon', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245894'}"
A readability evaluation of real-time crowd captions in the classroom,10.1145/2384916.2384930,"Deaf and hard of hearing individuals need accommodations that transform aural to visual information, such as captions that are generated in real-time to enhance their access to spoken information in lectures and other live events. The captions produced by professional captionists work well in general events such as community or legal meetings, but is often unsatisfactory in specialized content events such as higher education classrooms. In addition, it is hard to hire professional captionists, especially those that have experience in specialized content areas, as they are scarce and expensive. The captions produced by commercial automatic speech recognition (ASR) software are far cheaper, but is often perceived as unreadable due to ASR's sensitivity to accents, background noise and slow response time. We ran a study to evaluate the readability of captions generated by a new crowd captioning approach versus professional captionists and ASR. In this approach, captions are typed by classmates into a system that aligns and merges the multiple incomplete caption streams into a single, comprehensive real-time transcript. Our study asked 48 deaf and hearing readers to evaluate transcripts produced by a professional captionist, ASR and crowd captioning software respectively and found the readers preferred crowd captions over professional captions and ASR.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'human factors, experimentation, design', 'numpages': '8', 'pages': '71–78', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Deaf and hard of hearing individuals need accommodations that transform aural to visual information, such as captions that are generated in real-time to enhance their access to spoken information in lectures and other live events. The captions produced by professional captionists work well in general events such as community or legal meetings, but is often unsatisfactory in specialized content events such as higher education classrooms. In addition, it is hard to hire professional captionists, especially those that have experience in specialized content areas, as they are scarce and expensive. The captions produced by commercial automatic speech recognition (ASR) software are far cheaper, but is often perceived as unreadable due to ASR's sensitivity to accents, background noise and slow response time. We ran a study to evaluate the readability of captions generated by a new crowd captioning approach versus professional captionists and ASR. In this approach, captions are typed by classmates into a system that aligns and merges the multiple incomplete caption streams into a single, comprehensive real-time transcript. Our study asked 48 deaf and hearing readers to evaluate transcripts produced by a professional captionist, ASR and crowd captioning software respectively and found the readers preferred crowd captions over professional captions and ASR."", 'doi': '10.1145/2384916.2384930', 'url': 'https://doi.org/10.1145/2384916.2384930', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'A readability evaluation of real-time crowd captions in the classroom', 'author': 'Kushalnagar, Raja S. and Lasecki, Walter S. and Bigham, Jeffrey P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384930'}"
Web accessibility as a side effect,10.1145/2384916.2384931,"This paper explores evidence for the conjecture that improvements in Web accessibility have arisen, in part, as side effects of changes in Web technology and associated shifts in the way Web pages are designed and coded. Drawing on an earlier study of Web accessibility trends over the past 14 years, it discusses several possible indirect contributors to improving accessibility including the use of new browser capabilities to create more sophisticated page layouts, a growing concern with improved page rank in search results, and a shift toward cross-device content design. Understanding these examples may inspire the creation of additional technologies with incidental accessibility benefits.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'web accessibility', 'numpages': '8', 'pages': '79–86', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper explores evidence for the conjecture that improvements in Web accessibility have arisen, in part, as side effects of changes in Web technology and associated shifts in the way Web pages are designed and coded. Drawing on an earlier study of Web accessibility trends over the past 14 years, it discusses several possible indirect contributors to improving accessibility including the use of new browser capabilities to create more sophisticated page layouts, a growing concern with improved page rank in search results, and a shift toward cross-device content design. Understanding these examples may inspire the creation of additional technologies with incidental accessibility benefits.', 'doi': '10.1145/2384916.2384931', 'url': 'https://doi.org/10.1145/2384916.2384931', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Web accessibility as a side effect', 'author': 'Richards, John T. and Montague, Kyle and Hanson, Vicki L.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384931'}"
How do professionals who create computing technologies consider accessibility?,10.1145/2384916.2384932,"In this paper, we present survey findings about how user experience (UX) and human-computer interaction (HCI) professionals, who create information and communication technologies (ICTs), reported considering accessibility in their work. Participants (N = 199) represented a wide range of job titles and nationalities. We found that most respondents (87\%, N = 173) reported that accessibility was important or very important in their work; however, when considerations for accessibility were discussed in an open-ended question (N =185) the scope was limited. Additionally, we found that aspects of empathy and professional experience were associated with how accessibility considerations were reported. We also found that many respondents indicated that decisions about accessibility were not in their control. We argue that a better understanding about how accessibility is considered by professionals has implications for academic programs in HCI and UX as to how well programs are preparing students to consider and advocate for inclusive design.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'professions, inclusive design, diverse users, accessibility', 'numpages': '8', 'pages': '87–94', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this paper, we present survey findings about how user experience (UX) and human-computer interaction (HCI) professionals, who create information and communication technologies (ICTs), reported considering accessibility in their work. Participants (N = 199) represented a wide range of job titles and nationalities. We found that most respondents (87\\%, N = 173) reported that accessibility was important or very important in their work; however, when considerations for accessibility were discussed in an open-ended question (N =185) the scope was limited. Additionally, we found that aspects of empathy and professional experience were associated with how accessibility considerations were reported. We also found that many respondents indicated that decisions about accessibility were not in their control. We argue that a better understanding about how accessibility is considered by professionals has implications for academic programs in HCI and UX as to how well programs are preparing students to consider and advocate for inclusive design.', 'doi': '10.1145/2384916.2384932', 'url': 'https://doi.org/10.1145/2384916.2384932', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'How do professionals who create computing technologies consider accessibility?', 'author': 'Putnam, Cynthia and Wozniak, Kathryn and Zefeldt, Mary Jo and Cheng, Jinghui and Caputo, Morgan and Duffield, Carl', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384932'}"
Session details: Interactions without sight,10.1145/3245895,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245895', 'url': 'https://doi.org/10.1145/3245895', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Interactions without sight', 'author': 'Barreto, Armando', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245895'}"
Helping visually impaired users properly aim a camera,10.1145/2384916.2384934,"We evaluate three interaction modes to assist visually impaired users during the camera aiming process: speech, tone, and silent feedback. Our main assumption is that users are able to spatially localize what they want to photograph, and roughly aim the camera in the appropriate direction. Thus, small camera motions are sufficient for obtaining a good composition. Results in the context of documenting accessibility barriers related to public transportation show that audio feedback is valuable. Visually impaired users were not affected by audio feedback in terms of social comfort. Furthermore, we observed trends in favor of speech over tone, including higher ratings for ease of use. This study reinforces earlier work that suggests users who are blind or low vision find assisted photography appealing and useful.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'visually impaired, transit, photography, accessibility', 'numpages': '8', 'pages': '95–102', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We evaluate three interaction modes to assist visually impaired users during the camera aiming process: speech, tone, and silent feedback. Our main assumption is that users are able to spatially localize what they want to photograph, and roughly aim the camera in the appropriate direction. Thus, small camera motions are sufficient for obtaining a good composition. Results in the context of documenting accessibility barriers related to public transportation show that audio feedback is valuable. Visually impaired users were not affected by audio feedback in terms of social comfort. Furthermore, we observed trends in favor of speech over tone, including higher ratings for ease of use. This study reinforces earlier work that suggests users who are blind or low vision find assisted photography appealing and useful.', 'doi': '10.1145/2384916.2384934', 'url': 'https://doi.org/10.1145/2384916.2384934', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Helping visually impaired users properly aim a camera', 'author': ""V\\'{a}zquez, Marynel and Steinfeld, Aaron"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384934'}"
Learning non-visual graphical information using a touch-based vibro-audio interface,10.1145/2384916.2384935,"This paper evaluates an inexpensive and intuitive approach for providing non-visual access to graphic material, called a vibro-audio interface. The system works by allowing users to freely explore graphical information on the touchscreen of a commercially available tablet and synchronously triggering vibration patterns and auditory information whenever an on-screen visual element is touched. Three studies were conducted that assessed legibility and comprehension of the relative relations and global structure of a bar graph (Exp 1), Pattern recognition via a letter identification task (Exp 2), and orientation discrimination of geometric shapes (Exp 3). Performance with the touch-based device was compared to the same tasks performed using standard hardcopy tactile graphics. Results showed similar error performance between modes for all measures, indicating that the vibro-audio interface is a viable multimodal solution for providing access to dynamic visual information and supporting accurate spatial learning and the development of mental representations of graphical material.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'information graphics, haptic cues, graphs and diagrams, audio cues, assistive technology, android programming, accessibility (blind and visually-impaired)', 'numpages': '8', 'pages': '103–110', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper evaluates an inexpensive and intuitive approach for providing non-visual access to graphic material, called a vibro-audio interface. The system works by allowing users to freely explore graphical information on the touchscreen of a commercially available tablet and synchronously triggering vibration patterns and auditory information whenever an on-screen visual element is touched. Three studies were conducted that assessed legibility and comprehension of the relative relations and global structure of a bar graph (Exp 1), Pattern recognition via a letter identification task (Exp 2), and orientation discrimination of geometric shapes (Exp 3). Performance with the touch-based device was compared to the same tasks performed using standard hardcopy tactile graphics. Results showed similar error performance between modes for all measures, indicating that the vibro-audio interface is a viable multimodal solution for providing access to dynamic visual information and supporting accurate spatial learning and the development of mental representations of graphical material.', 'doi': '10.1145/2384916.2384935', 'url': 'https://doi.org/10.1145/2384916.2384935', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Learning non-visual graphical information using a touch-based vibro-audio interface', 'author': 'Giudice, Nicholas A. and Palani, Hari Prasath and Brenner, Eric and Kramer, Kevin M.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384935'}"
Exploration and avoidance of surrounding obstacles for the visually impaired,10.1145/2384916.2384936,Proximity-based interaction through a long cane is essential for the blind and the visually impaired. We designed and implemented an obstacle detector consisting of a 3D Time-of-Flight (TOF) camera and a planar tactile display to extend the interaction range and provide rich non-visual information about the environment. Users choose a better path after acquiring the spatial layout of obstacles than with a white cane alone. A user study with 6 blind people was analyzed and showed extra time is needed to ensure safe walking while reading the layout. Both hanging and ground-based obstacles were circumvented. Tactile mapping information has been designed for representation of precise spatial information around a blind user.,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tactile symbol, obstacle avoidance, haptic user interface, 3D TOF camera', 'numpages': '8', 'pages': '111–118', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Proximity-based interaction through a long cane is essential for the blind and the visually impaired. We designed and implemented an obstacle detector consisting of a 3D Time-of-Flight (TOF) camera and a planar tactile display to extend the interaction range and provide rich non-visual information about the environment. Users choose a better path after acquiring the spatial layout of obstacles than with a white cane alone. A user study with 6 blind people was analyzed and showed extra time is needed to ensure safe walking while reading the layout. Both hanging and ground-based obstacles were circumvented. Tactile mapping information has been designed for representation of precise spatial information around a blind user.', 'doi': '10.1145/2384916.2384936', 'url': 'https://doi.org/10.1145/2384916.2384936', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Exploration and avoidance of surrounding obstacles for the visually impaired', 'author': 'Zeng, Limin and Prescher, Denise and Weber, Gerhard', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384936'}"
Session details: Understanding aging performance,10.1145/3245896,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245896', 'url': 'https://doi.org/10.1145/3245896', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Understanding aging performance', 'author': 'Hwang, Faustina', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245896'}"
Understanding the role of age and fluid intelligence in information search,10.1145/2384916.2384938,"In this study, we explore the role of age and fluid intelligence on the behavior of people looking for information in a real-world search space. Analyses of mouse moves, clicks, and eye movements provide a window into possible differences in both task strategy and performance, and allow us to begin to separate the influence of age from the correlated but isolable influence of cognitive ability. We found little evidence of differences in strategy between younger and older participants matched on fluid intelligence. Both performance and strategy differences were found between older participants having higher versus lower fluid intelligence, however, suggesting that cognitive factors, rather than age per se, exert the dominant influence. This underscores the importance of measuring and controlling for cognitive abilities in studies involving older adults.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'information search, fluid intelligence, eye-gaze, cognition, age', 'numpages': '8', 'pages': '119–126', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this study, we explore the role of age and fluid intelligence on the behavior of people looking for information in a real-world search space. Analyses of mouse moves, clicks, and eye movements provide a window into possible differences in both task strategy and performance, and allow us to begin to separate the influence of age from the correlated but isolable influence of cognitive ability. We found little evidence of differences in strategy between younger and older participants matched on fluid intelligence. Both performance and strategy differences were found between older participants having higher versus lower fluid intelligence, however, suggesting that cognitive factors, rather than age per se, exert the dominant influence. This underscores the importance of measuring and controlling for cognitive abilities in studies involving older adults.', 'doi': '10.1145/2384916.2384938', 'url': 'https://doi.org/10.1145/2384916.2384938', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Understanding the role of age and fluid intelligence in information search', 'author': 'Trewin, Shari and Richards, John T. and Hanson, Vicki L. and Sloan, David and John, Bonnie E. and Swart, Cal and Thomas, John C.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384938'}"
Elderly text-entry performance on touchscreens,10.1145/2384916.2384939,"Touchscreen devices have become increasingly popular. Yet they lack of tactile feedback and motor stability, making it difficult effectively typing on virtual keyboards. This is even worse for elderly users and their declining motor abilities, particularly hand tremor. In this paper we examine text-entry performance and typing patterns of elderly users on touch-based devices. Moreover, we analyze users' hand tremor profile and its relationship to typing behavior. Our main goal is to inform future designs of touchscreen keyboards for elderly people. To this end, we asked 15 users to enter text under two device conditions (mobile and tablet) and measured their performance, both speed- and accuracy-wise. Additionally, we thoroughly analyze different types of errors (insertions, substitutions, and omissions) looking at touch input features and their main causes. Results show that omissions are the most common error type, mainly due to cognitive errors, followed by substitutions and insertions. While tablet devices can compensate for about 9\% of typing errors, omissions are similar across conditions. Measured hand tremor largely correlates with text-entry errors, suggesting that it should be approached to improve input accuracy. Finally, we assess the effect of simple touch models and provide implications to design.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tremor, touchscreen, text-entry, tablet, mobile, elderly', 'numpages': '8', 'pages': '127–134', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Touchscreen devices have become increasingly popular. Yet they lack of tactile feedback and motor stability, making it difficult effectively typing on virtual keyboards. This is even worse for elderly users and their declining motor abilities, particularly hand tremor. In this paper we examine text-entry performance and typing patterns of elderly users on touch-based devices. Moreover, we analyze users' hand tremor profile and its relationship to typing behavior. Our main goal is to inform future designs of touchscreen keyboards for elderly people. To this end, we asked 15 users to enter text under two device conditions (mobile and tablet) and measured their performance, both speed- and accuracy-wise. Additionally, we thoroughly analyze different types of errors (insertions, substitutions, and omissions) looking at touch input features and their main causes. Results show that omissions are the most common error type, mainly due to cognitive errors, followed by substitutions and insertions. While tablet devices can compensate for about 9\\% of typing errors, omissions are similar across conditions. Measured hand tremor largely correlates with text-entry errors, suggesting that it should be approached to improve input accuracy. Finally, we assess the effect of simple touch models and provide implications to design."", 'doi': '10.1145/2384916.2384939', 'url': 'https://doi.org/10.1145/2384916.2384939', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Elderly text-entry performance on touchscreens', 'author': 'Nicolau, Hugo and Jorge, Joaquim', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384939'}"
Session details: Shared work,10.1145/3245899,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245899', 'url': 'https://doi.org/10.1145/3245899', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Shared work', 'author': 'Ladner, Richard', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245899'}"
Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities,10.1145/2384916.2384941,"Fashion is a language. How we dress signals to others who we are and how we want to be perceived. However, this language is primarily visual, making it inaccessible to people with vision impairments. Someone who is low-vision or completely blind cannot see what others are wearing or readily know what constitutes the norms and extremes of fashion, but most everyone they encounter can see (and judge) their fashion choices. We describe our findings of a diary study with people with vision impairments that revealed the many accessibility barriers fashion presents, and how an online survey revealed that clothing decisions are often made collaboratively, regardless of visual ability. Based on these findings, we identified a need for a collaborative and real-time environment for fashion advice. We have tested the feasibility of providing this advice through crowdsourcing using VizWiz, a mobile phone application where participants receive nearly real-time answers to visual questions. Our pilot study results show that this application has the potential to address a great need within the blind community, but remaining challenges include improving photo capture and assembling a set of crowd workers with the requisite expertise. More broadly our research highlights the feasibility of using crowdsourcing for subjective, opinion-based advice.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'fashion, crowdsourcing, blind users', 'numpages': '8', 'pages': '135–142', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Fashion is a language. How we dress signals to others who we are and how we want to be perceived. However, this language is primarily visual, making it inaccessible to people with vision impairments. Someone who is low-vision or completely blind cannot see what others are wearing or readily know what constitutes the norms and extremes of fashion, but most everyone they encounter can see (and judge) their fashion choices. We describe our findings of a diary study with people with vision impairments that revealed the many accessibility barriers fashion presents, and how an online survey revealed that clothing decisions are often made collaboratively, regardless of visual ability. Based on these findings, we identified a need for a collaborative and real-time environment for fashion advice. We have tested the feasibility of providing this advice through crowdsourcing using VizWiz, a mobile phone application where participants receive nearly real-time answers to visual questions. Our pilot study results show that this application has the potential to address a great need within the blind community, but remaining challenges include improving photo capture and assembling a set of crowd workers with the requisite expertise. More broadly our research highlights the feasibility of using crowdsourcing for subjective, opinion-based advice.', 'doi': '10.1145/2384916.2384941', 'url': 'https://doi.org/10.1145/2384916.2384941', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities', 'author': 'Burton, Michele A. and Brady, Erin and Brewer, Robin and Neylan, Callie and Bigham, Jeffrey P. and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384941'}"
Online quality control for real-time crowd captioning,10.1145/2384916.2384942,"Approaches for real-time captioning of speech are either expensive (professional stenographers) or error-prone (automatic speech recognition). As an alternative approach, we have been exploring whether groups of non-experts can collectively caption speech in real-time. In this approach, each worker types as much as they can and the partial captions are merged together in real-time automatically. This approach works best when partial captions are correct and received within a few seconds of when they were spoken, but these assumptions break down when engaging workers on-demand from existing sources of crowd work like Amazon's Mechanical Turk. In this paper, we present methods for quickly identifying workers who are producing good partial captions and estimating the quality of their input. We evaluate these methods in experiments run on Mechanical Turk in which a total of 42 workers captioned 20 minutes of audio. The methods introduced in this paper were able to raise overall accuracy from 57.8\% to 81.22\% while keeping coverage of the ground truth signal nearly unchanged.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'human computation, hard of hearing, deaf, captioning', 'numpages': '8', 'pages': '143–150', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Approaches for real-time captioning of speech are either expensive (professional stenographers) or error-prone (automatic speech recognition). As an alternative approach, we have been exploring whether groups of non-experts can collectively caption speech in real-time. In this approach, each worker types as much as they can and the partial captions are merged together in real-time automatically. This approach works best when partial captions are correct and received within a few seconds of when they were spoken, but these assumptions break down when engaging workers on-demand from existing sources of crowd work like Amazon's Mechanical Turk. In this paper, we present methods for quickly identifying workers who are producing good partial captions and estimating the quality of their input. We evaluate these methods in experiments run on Mechanical Turk in which a total of 42 workers captioned 20 minutes of audio. The methods introduced in this paper were able to raise overall accuracy from 57.8\\% to 81.22\\% while keeping coverage of the ground truth signal nearly unchanged."", 'doi': '10.1145/2384916.2384942', 'url': 'https://doi.org/10.1145/2384916.2384942', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Online quality control for real-time crowd captioning', 'author': 'Lasecki, Walter S. and Bigham, Jeffrey P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384942'}"
Designing for individuals: usable touch-screen interaction through shared user models,10.1145/2384916.2384943,"Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'shared user modelling, mobile touch screens, adaptive interfaces', 'numpages': '8', 'pages': '151–158', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Mobile touch-screen devices are becoming increasingly popular across a diverse range of users. Whilst there is a wealth of information and utilities available via downloadable apps, there is still a large proportion of users with visual and motor impairments who are unable to use the technology fully due to their interaction needs. In this paper we present an evaluation of the use of shared user modelling and adaptive interfaces to improve the accessibility of mobile touch-screen technologies. By using abilities based information collected through application use and continually updating the user model and interface adaptations, it is easy for users to make applications aware of their needs and preferences. Three smart phone apps were created for this study and tested with 12 adults who had diverse visual and motor impairments. Results indicated significant benefits from the shared user models that can automatically adapt interfaces, across applications, to address usability needs.', 'doi': '10.1145/2384916.2384943', 'url': 'https://doi.org/10.1145/2384916.2384943', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Designing for individuals: usable touch-screen interaction through shared user models', 'author': 'Montague, Kyle and Hanson, Vicki L. and Cobley, Andy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384943'}"
Session details: Visual impairment simulation,10.1145/3245897,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245897', 'url': 'https://doi.org/10.1145/3245897', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Visual impairment simulation', 'author': 'Trewin, Shari', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245897'}"
PassChords: secure multi-touch authentication for blind people,10.1145/2384916.2384945,"Blind mobile device users face security risks such as inaccessible authentication methods, and aural and visual eavesdropping. We interviewed 13 blind smartphone users and found that most participants were unaware of or not concerned about potential security threats. Not a single participant used optional authentication methods such as a password-protected screen lock. We addressed the high risk of unauthorized user access by developing PassChords, a non-visual authentication method for touch surfaces that is robust to aural and visual eavesdropping. A user enters a PassChord by tapping several times on a touch surface with one or more fingers. The set of fingers used in each tap defines the password. We give preliminary evidence that a four-tap PassChord has about the same entropy, a measure of password strength, as a four-digit personal identification number (PIN) used in the iPhone's Passcode Lock. We conducted a study with 16 blind participants that showed that PassChords were nearly three times as fast as iPhone's Passcode Lock with VoiceOver, suggesting that PassChords are a viable accessible authentication method for touch screens.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'touch screens, security, privacy, mobile devices, blind', 'numpages': '8', 'pages': '159–166', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Blind mobile device users face security risks such as inaccessible authentication methods, and aural and visual eavesdropping. We interviewed 13 blind smartphone users and found that most participants were unaware of or not concerned about potential security threats. Not a single participant used optional authentication methods such as a password-protected screen lock. We addressed the high risk of unauthorized user access by developing PassChords, a non-visual authentication method for touch surfaces that is robust to aural and visual eavesdropping. A user enters a PassChord by tapping several times on a touch surface with one or more fingers. The set of fingers used in each tap defines the password. We give preliminary evidence that a four-tap PassChord has about the same entropy, a measure of password strength, as a four-digit personal identification number (PIN) used in the iPhone's Passcode Lock. We conducted a study with 16 blind participants that showed that PassChords were nearly three times as fast as iPhone's Passcode Lock with VoiceOver, suggesting that PassChords are a viable accessible authentication method for touch screens."", 'doi': '10.1145/2384916.2384945', 'url': 'https://doi.org/10.1145/2384916.2384945', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'PassChords: secure multi-touch authentication for blind people', 'author': 'Azenkot, Shiri and Rector, Kyle and Ladner, Richard and Wobbrock, Jacob', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384945'}"
"""So that's what you see"": building understanding with personalized simulations of colour vision deficiency",10.1145/2384916.2384946,"Colour vision deficiencies (CVD) affect the everyday lives of a large number of people, but it is difficult for others - even friends and family members - to understand the experience of having CVD. Simulation tools can help provide this experience; however, current simulations are based on general models that have several limitations, and therefore cannot accurately reflect the perceptual capabilities of most individuals with reduced colour vision. To address this problem, we have developed a new simulation approach that is based on a specific empirical model of the actual colour perception abilities of a person with CVD. The resulting simulation is therefore a more exact representation of what a particular person with CVD actually sees. We tested the new approach in two ways. First, we compared its accuracy with that of the existing models, and found that the personalized simulations were significantly more accurate than the old method. Second, we asked pairs of participants (one with CVD, and one close friend or family member without CVD) to discuss images of everyday scenes that had been simulated with the CVD person's particular model. We found that the personalized simulations provided new insights into the details of the CVD person's experience. The personalized-simulation approach shows great promise for improving understanding of CVD (and potentially other conditions) for people with ordinary perceptual abilities.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'colour vision simulation, colour vision deficiency', 'numpages': '8', 'pages': '167–174', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Colour vision deficiencies (CVD) affect the everyday lives of a large number of people, but it is difficult for others - even friends and family members - to understand the experience of having CVD. Simulation tools can help provide this experience; however, current simulations are based on general models that have several limitations, and therefore cannot accurately reflect the perceptual capabilities of most individuals with reduced colour vision. To address this problem, we have developed a new simulation approach that is based on a specific empirical model of the actual colour perception abilities of a person with CVD. The resulting simulation is therefore a more exact representation of what a particular person with CVD actually sees. We tested the new approach in two ways. First, we compared its accuracy with that of the existing models, and found that the personalized simulations were significantly more accurate than the old method. Second, we asked pairs of participants (one with CVD, and one close friend or family member without CVD) to discuss images of everyday scenes that had been simulated with the CVD person's particular model. We found that the personalized simulations provided new insights into the details of the CVD person's experience. The personalized-simulation approach shows great promise for improving understanding of CVD (and potentially other conditions) for people with ordinary perceptual abilities."", 'doi': '10.1145/2384916.2384946', 'url': 'https://doi.org/10.1145/2384916.2384946', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': '""So that\'s what you see"": building understanding with personalized simulations of colour vision deficiency', 'author': 'Flatla, David R. and Gutwin, Carl', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384946'}"
Evaluation of dynamic image pre-compensation forcomputer users with severe refractive error,10.1145/2384916.2384947,"Visual distortion and blurring impede the efficient interaction between computers and their users. Visual problems can be caused by eye diseases, severe refractive errors or combinations of both. Several image enhancement methods based on contrast sensitivity have been used to help people with eye diseases (e.g., age-related macular degeneration and cataracts), whereas few methods have been designed for people with severe refractive errors. This paper describes a new pre-compensation method to counter the visual blurring caused by the severe refractive errors of a specific computer user. It preprocesses the pictorial information through dynamic pre-compensation in advance, aiming to present customized images on the basis of the ocular aberrations of the specific computer user. The new method improves the previous static pre-compensation method by updating the aberration data according to pupil size variations, in real-time. The real-time aberration data enable us to generate better suited pre-compensated images, as the pre-compensation model is updated dynamically. An empirical study was conducted to evaluate the efficiency of the new pre-compensation method, through an icon recognition test. From the results of statistical analysis, we found that participants achieved significantly higher accuracy levels in recognizing the icons with dynamic pre-compensation, than when viewing the original icons. The accuracy is also significantly boosted when the icons were processed with dynamic pre-compensation method, in comparison with the previous static pre-compensation method.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'refractive error, ocular aberration, image pre-compensation, image enhancement, icon recognition', 'numpages': '8', 'pages': '175–182', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Visual distortion and blurring impede the efficient interaction between computers and their users. Visual problems can be caused by eye diseases, severe refractive errors or combinations of both. Several image enhancement methods based on contrast sensitivity have been used to help people with eye diseases (e.g., age-related macular degeneration and cataracts), whereas few methods have been designed for people with severe refractive errors. This paper describes a new pre-compensation method to counter the visual blurring caused by the severe refractive errors of a specific computer user. It preprocesses the pictorial information through dynamic pre-compensation in advance, aiming to present customized images on the basis of the ocular aberrations of the specific computer user. The new method improves the previous static pre-compensation method by updating the aberration data according to pupil size variations, in real-time. The real-time aberration data enable us to generate better suited pre-compensated images, as the pre-compensation model is updated dynamically. An empirical study was conducted to evaluate the efficiency of the new pre-compensation method, through an icon recognition test. From the results of statistical analysis, we found that participants achieved significantly higher accuracy levels in recognizing the icons with dynamic pre-compensation, than when viewing the original icons. The accuracy is also significantly boosted when the icons were processed with dynamic pre-compensation method, in comparison with the previous static pre-compensation method.', 'doi': '10.1145/2384916.2384947', 'url': 'https://doi.org/10.1145/2384916.2384947', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Evaluation of dynamic image pre-compensation forcomputer users with severe refractive error', 'author': 'Huang, Jian and Barreto, Armando and Adjouadi, Malek', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384947'}"
Session details: Sign language,10.1145/3245898,Abstract not available,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3245898', 'url': 'https://doi.org/10.1145/3245898', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Session details: Sign language', 'author': 'Kushalnagar, Raja', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3245898'}"
Effect of presenting video as a baseline during an american sign language animation user study,10.1145/2384916.2384949,"Animations of American Sign Language (ASL) have accessibility benefits for many signers with lower levels of written language literacy. Our lab has conducted several prior studies to evaluate synthesized ASL animations by asking native signers to watch different versions of animations and to answer comprehension and subjective questions about them. As an upper baseline, we used an animation of a virtual human carefully created by a human animator who is a native ASL signer. Considering whether to instead use videos of human signers as an upper baseline, we wanted to quantify how including a video upper baseline would affect how participants evaluate the ASL animations presented in a study. In this paper, we replicate a user study we conducted two years ago, with one difference: replacing our original animation upper baseline with a video of a human signer. We found that adding a human video upper baseline depressed the subjective Likert-scale scores that participants assign to the other stimuli (the synthesized animations) in the study when viewed side-by-side. This paper provides methodological guidance for how to design user studies evaluating sign language animations and facilitates comparison of studies that have used different upper baselines.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user study, baseline, animation, american sign language, accessibility technology for people who are deaf', 'numpages': '8', 'pages': '183–190', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Animations of American Sign Language (ASL) have accessibility benefits for many signers with lower levels of written language literacy. Our lab has conducted several prior studies to evaluate synthesized ASL animations by asking native signers to watch different versions of animations and to answer comprehension and subjective questions about them. As an upper baseline, we used an animation of a virtual human carefully created by a human animator who is a native ASL signer. Considering whether to instead use videos of human signers as an upper baseline, we wanted to quantify how including a video upper baseline would affect how participants evaluate the ASL animations presented in a study. In this paper, we replicate a user study we conducted two years ago, with one difference: replacing our original animation upper baseline with a video of a human signer. We found that adding a human video upper baseline depressed the subjective Likert-scale scores that participants assign to the other stimuli (the synthesized animations) in the study when viewed side-by-side. This paper provides methodological guidance for how to design user studies evaluating sign language animations and facilitates comparison of studies that have used different upper baselines.', 'doi': '10.1145/2384916.2384949', 'url': 'https://doi.org/10.1145/2384916.2384949', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Effect of presenting video as a baseline during an american sign language animation user study', 'author': 'Lu, Pengfei and Kacorri, Hernisa', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384949'}"
Design and evaluation of classifier for identifying sign language videos in video sharing sites,10.1145/2384916.2384950,"Video sharing sites provide an opportunity for the collection and use of sign language presentations about a wide range of topics. Currently, locating sign language videos (SL videos) in such sharing sites relies on the existence and accuracy of tags, titles or other metadata indicating the content is in sign language. In this paper, we describe the design and evaluation of a classifier for distinguishing between sign language videos and other videos. A test collection of SL videos and videos likely to be incorrectly recognized as SL videos (likely false positives) was created for evaluating alternative classifiers. Five video features thought to be potentially valuable for this task were developed based on common video analysis techniques. A comparison of the relative value of the five video features shows that a measure of the symmetry of movement relative to the face is the best feature for distinguishing sign language videos. Overall, an SVM classifier provided with all five features achieves 82\% precision and 90\% recall when tested on the challenging test collection. The performance would be considerably higher when applied to the more varied collections of large video sharing sites.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'video sharing, video analysis, sign language, metadata extraction, asl', 'numpages': '8', 'pages': '191–198', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Video sharing sites provide an opportunity for the collection and use of sign language presentations about a wide range of topics. Currently, locating sign language videos (SL videos) in such sharing sites relies on the existence and accuracy of tags, titles or other metadata indicating the content is in sign language. In this paper, we describe the design and evaluation of a classifier for distinguishing between sign language videos and other videos. A test collection of SL videos and videos likely to be incorrectly recognized as SL videos (likely false positives) was created for evaluating alternative classifiers. Five video features thought to be potentially valuable for this task were developed based on common video analysis techniques. A comparison of the relative value of the five video features shows that a measure of the symmetry of movement relative to the face is the best feature for distinguishing sign language videos. Overall, an SVM classifier provided with all five features achieves 82\\% precision and 90\\% recall when tested on the challenging test collection. The performance would be considerably higher when applied to the more varied collections of large video sharing sites.', 'doi': '10.1145/2384916.2384950', 'url': 'https://doi.org/10.1145/2384916.2384950', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Design and evaluation of classifier for identifying sign language videos in video sharing sites', 'author': 'Monteiro, Caio D.D. and Gutierrez-Osuna, Ricardo and Shipman, Frank M.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384950'}"
Turning off-the-shelf games into biofeedback games,10.1145/2384916.2384952,"Biofeedback games help users maintain specific mental or physical states and are useful to help people with cognitive impairments learn to self-regulate their brain function. However, biofeedback games are expensive and difficult to create and are not sufficiently appealing to hold a user's interest over the long term. We present two systems that turn off-the-shelf games into biofeedback games. Our desktop approach uses visual feedback via texture-based graphical overlays that vary in their obfuscation of an underlying game based on the user's physiological state. Our mobile approach presents multi-modal feedback (audio or vibration) of a user's physiological state on an iPhone.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'neurofeedback, game, fasd, eeg, biofeedback', 'numpages': '2', 'pages': '199–200', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Biofeedback games help users maintain specific mental or physical states and are useful to help people with cognitive impairments learn to self-regulate their brain function. However, biofeedback games are expensive and difficult to create and are not sufficiently appealing to hold a user's interest over the long term. We present two systems that turn off-the-shelf games into biofeedback games. Our desktop approach uses visual feedback via texture-based graphical overlays that vary in their obfuscation of an underlying game based on the user's physiological state. Our mobile approach presents multi-modal feedback (audio or vibration) of a user's physiological state on an iPhone."", 'doi': '10.1145/2384916.2384952', 'url': 'https://doi.org/10.1145/2384916.2384952', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Turning off-the-shelf games into biofeedback games', 'author': 'Mandryk, Regan L. and Kalyn, Michael and Dang, Yichen and Doucette, Andre and Taylor, Brett and Dielschneider, Shane', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384952'}"
Smartphone application for indoor scene localization,10.1145/2384916.2384953,"Blind people are unable to navigate easily in unfamiliar indoor environments without assistance. Knowing the current location is a particularly important aspect of indoor navigation. Scene identification in indoor buildings without any Global Positioning System (GPS) is a challenging problem. We present a smart phone based assistive technology which uses computer vision techniques to localize the indoor location from a scene image. The aim of our work is to guide blind people during navigation inside buildings where GPS is not effective. Our current system uses a client-server model where the user takes a photo from their current location, the image is sent to the server, the location is sent back to the mobile device, and a voice message is used to convey the location information.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'voice synthesis, visual bag of words, sift features, indoor navigation, fundamental matrix, blind people', 'numpages': '2', 'pages': '201–202', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blind people are unable to navigate easily in unfamiliar indoor environments without assistance. Knowing the current location is a particularly important aspect of indoor navigation. Scene identification in indoor buildings without any Global Positioning System (GPS) is a challenging problem. We present a smart phone based assistive technology which uses computer vision techniques to localize the indoor location from a scene image. The aim of our work is to guide blind people during navigation inside buildings where GPS is not effective. Our current system uses a client-server model where the user takes a photo from their current location, the image is sent to the server, the location is sent back to the mobile device, and a voice message is used to convey the location information.', 'doi': '10.1145/2384916.2384953', 'url': 'https://doi.org/10.1145/2384916.2384953', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Smartphone application for indoor scene localization', 'author': 'Khan, Nabeel Younus and McCane, Brendan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384953'}"
Specialized DVD player to render audio description and its usability performance,10.1145/2384916.2384954,"Our DVD Player was designed based on the needs identified in our prior in situ study with blind and visually impaired individuals. In this study we identified three major issues that would significantly improve the audio-visual content with audio description «AD). Beside the regular accessible functions found in existing video player, we added three specialized functions to enhance the user experience. Thus, the player provides context information on the content, basic and an augmented quantity of AD and recall functions on key information (scene identification, actions in the scene and the actors in the scene). We propose to demonstrate the CRIM DVDPlayer and to discuss the results of our seven months usability study.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'usability, dvd player, blind and low vision people, audio description', 'numpages': '2', 'pages': '203–204', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Our DVD Player was designed based on the needs identified in our prior in situ study with blind and visually impaired individuals. In this study we identified three major issues that would significantly improve the audio-visual content with audio description «AD). Beside the regular accessible functions found in existing video player, we added three specialized functions to enhance the user experience. Thus, the player provides context information on the content, basic and an augmented quantity of AD and recall functions on key information (scene identification, actions in the scene and the actors in the scene). We propose to demonstrate the CRIM DVDPlayer and to discuss the results of our seven months usability study.', 'doi': '10.1145/2384916.2384954', 'url': 'https://doi.org/10.1145/2384916.2384954', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Specialized DVD player to render audio description and its usability performance', 'author': 'Chapdelaine, Claude', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384954'}"
IDEAL: a dyslexic-friendly ebook reader,10.1145/2384916.2384955,"We present an ebook reader for Android which displays ebooks in a more accessible manner for users with dyslexia. The ebook reader combines features that other related tools already have, such as text-to-speech technology, and new features, such as displaying the text with an adapted text layout based on the results of a user study with participants with dyslexia. Since there is no universal profile of a user with dyslexia, the layout settings are customizable and users can override the special layout setting according to their reading preferences.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'text-to-speech, readability, e-book reader, dyslexia, assistive technologies, android', 'numpages': '2', 'pages': '205–206', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present an ebook reader for Android which displays ebooks in a more accessible manner for users with dyslexia. The ebook reader combines features that other related tools already have, such as text-to-speech technology, and new features, such as displaying the text with an adapted text layout based on the results of a user study with participants with dyslexia. Since there is no universal profile of a user with dyslexia, the layout settings are customizable and users can override the special layout setting according to their reading preferences.', 'doi': '10.1145/2384916.2384955', 'url': 'https://doi.org/10.1145/2384916.2384955', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'IDEAL: a dyslexic-friendly ebook reader', 'author': 'Kanvinde, Gaurang and Rello, Luz and Baeza-Yates, Ricardo', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384955'}"
FEPS: a sensory substitution system for the blind to perceive facial expressions,10.1145/2384916.2384956,"This work demonstrates a visual-to-auditory Sensory Substitution System (SSD) called Facial Expression Perception through Sound (FEPS). It is designed to enable the visually impaired people to participate in a more effective social communication by perceiving their interlocutor's facial expressions. The earlier SSDs provided feedback on inferred emotions, where as, this system responds to the facial movements. This is a better method than emotion inference due to complexities in expression-to-emotion mapping, the problem of capturing multitude of possible emotions derived from a limited facial movements and the difficulty to correctly predict emotions due to lack in ground truth data. In this work, the user's ability to understand the facial expressions has been ensured by a usability study.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'sonification, sensory substitution system, facial expressions, blind, assistive technology', 'numpages': '2', 'pages': '207–208', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""This work demonstrates a visual-to-auditory Sensory Substitution System (SSD) called Facial Expression Perception through Sound (FEPS). It is designed to enable the visually impaired people to participate in a more effective social communication by perceiving their interlocutor's facial expressions. The earlier SSDs provided feedback on inferred emotions, where as, this system responds to the facial movements. This is a better method than emotion inference due to complexities in expression-to-emotion mapping, the problem of capturing multitude of possible emotions derived from a limited facial movements and the difficulty to correctly predict emotions due to lack in ground truth data. In this work, the user's ability to understand the facial expressions has been ensured by a usability study."", 'doi': '10.1145/2384916.2384956', 'url': 'https://doi.org/10.1145/2384916.2384956', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'FEPS: a sensory substitution system for the blind to perceive facial expressions', 'author': 'Tanveer, Md. Iftekhar and Anam, A.S.M. Iftekhar and Rahman, A.K.M Mahbubur and Ghosh, Sreya and Yeasin, Mohammed', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384956'}"
SymbolPath: a continuous motion overlay module for icon-based assistive communication,10.1145/2384916.2384957,"Augmentative and alternative communication (AAC) systems are often used by individuals with severe speech impairments. Icon-based AAC systems typically present users with arrays of icons that are sequentially selected to construct utterances, which are then spoken aloud using text-to-speech (TTS) synthesis. For touch-screen devices, users must lift their finger or hand to select individual icons and avoid selecting multiple icons at once. Because many individuals with severe speech impairments have concomitant limb impairments, repetitive and precise movements can be slow and effortful. The current work aims to enhance message formulation ease and speed by using continuous motion icon selection rather than discrete input. SymbolPath is an overlay module that can be integrated with existing icon-based AAC systems to enable continuous motion icon selection. Message formulation using SymbolPath consists of drawing a continuous path through a set of desired icons. The system then determines the most likely subset of desired icons on that path and rearranges them to form a meaningful and grammatical sentence. In addition to demonstrating the SymbolPath module, we plan to present usability data and discuss iterative modifications to the software.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'icons, continuous motion, aac', 'numpages': '2', 'pages': '209–210', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Augmentative and alternative communication (AAC) systems are often used by individuals with severe speech impairments. Icon-based AAC systems typically present users with arrays of icons that are sequentially selected to construct utterances, which are then spoken aloud using text-to-speech (TTS) synthesis. For touch-screen devices, users must lift their finger or hand to select individual icons and avoid selecting multiple icons at once. Because many individuals with severe speech impairments have concomitant limb impairments, repetitive and precise movements can be slow and effortful. The current work aims to enhance message formulation ease and speed by using continuous motion icon selection rather than discrete input. SymbolPath is an overlay module that can be integrated with existing icon-based AAC systems to enable continuous motion icon selection. Message formulation using SymbolPath consists of drawing a continuous path through a set of desired icons. The system then determines the most likely subset of desired icons on that path and rearranges them to form a meaningful and grammatical sentence. In addition to demonstrating the SymbolPath module, we plan to present usability data and discuss iterative modifications to the software.', 'doi': '10.1145/2384916.2384957', 'url': 'https://doi.org/10.1145/2384916.2384957', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'SymbolPath: a continuous motion overlay module for icon-based assistive communication', 'author': 'Wiegand, Karl and Patel, Rupal', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384957'}"
Automated description generation for indoor floor maps,10.1145/2384916.2384958,"People with visual impairment generally suffer from diminished freedom in navigating an environment. A practical need is to navigate through unfamiliar indoor environments such as school buildings, hotels, etc., for which commonly-used existing tools like canes, seeing-eye dogs and GPS devices cannot provide adequate support. We demonstrate a prototype system that aims at addressing this practical need. The input to the system is the name of the building/establishment supplied by a user, which is used by a web crawler to determine the availability of a floor map on the corresponding website. If available, the map is downloaded and used by the proposed system to generate a verbal description giving an overview of the locations of key landmarks inside the map with respect to one another. Our preliminary survey and experiments indicate that this is a promising direction to pursue in supporting indoor navigation for the visually impaired.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'visually impaired, verbal description, indoor floor plan', 'numpages': '2', 'pages': '211–212', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People with visual impairment generally suffer from diminished freedom in navigating an environment. A practical need is to navigate through unfamiliar indoor environments such as school buildings, hotels, etc., for which commonly-used existing tools like canes, seeing-eye dogs and GPS devices cannot provide adequate support. We demonstrate a prototype system that aims at addressing this practical need. The input to the system is the name of the building/establishment supplied by a user, which is used by a web crawler to determine the availability of a floor map on the corresponding website. If available, the map is downloaded and used by the proposed system to generate a verbal description giving an overview of the locations of key landmarks inside the map with respect to one another. Our preliminary survey and experiments indicate that this is a promising direction to pursue in supporting indoor navigation for the visually impaired.', 'doi': '10.1145/2384916.2384958', 'url': 'https://doi.org/10.1145/2384916.2384958', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Automated description generation for indoor floor maps', 'author': 'Paladugu, Devi A. and Maguluri, Hima Bindu and Tian, Qiongjie and Li, Baoxin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384958'}"
Non-visual-cueing-based sensing and understanding of nearby entities in aided navigation.,10.1145/2384916.2384959,"Exploring unfamiliar environments is a challenging task in which additionally, unsighted individuals frequently fail to gain perception of obstacles and make serendipitous discoveries. This is because the mental depiction of the context is drastically lessened due to the absence of visual information. It is still not clear in neuroscience, whether stimuli elicited by visual cueing can be replicated by other senses (cross-model transfer). In the practice, however, everyone recognizes a key, whether it is felt in a pocket or seen on a table. We present a context-aware aid system for the blind that merges three levels of assistance enhancing the intelligibility of the nearby entities: an exploration module to help gain awareness of the surrounding context, an alerting method for warning the user when a stumble is likely, and, finally, a recognition engine that retrieves natural targets previously learned. Practical experiences with our system show that in the absence of visual cueing, the audio and haptic trajectory playback coupled with computer-vision methods is a promising approach to depict dynamic information of the immediate environment.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'visually impaired, visual cueing, context-aware aid, assistance', 'numpages': '2', 'pages': '213–214', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Exploring unfamiliar environments is a challenging task in which additionally, unsighted individuals frequently fail to gain perception of obstacles and make serendipitous discoveries. This is because the mental depiction of the context is drastically lessened due to the absence of visual information. It is still not clear in neuroscience, whether stimuli elicited by visual cueing can be replicated by other senses (cross-model transfer). In the practice, however, everyone recognizes a key, whether it is felt in a pocket or seen on a table. We present a context-aware aid system for the blind that merges three levels of assistance enhancing the intelligibility of the nearby entities: an exploration module to help gain awareness of the surrounding context, an alerting method for warning the user when a stumble is likely, and, finally, a recognition engine that retrieves natural targets previously learned. Practical experiences with our system show that in the absence of visual cueing, the audio and haptic trajectory playback coupled with computer-vision methods is a promising approach to depict dynamic information of the immediate environment.', 'doi': '10.1145/2384916.2384959', 'url': 'https://doi.org/10.1145/2384916.2384959', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Non-visual-cueing-based sensing and understanding of nearby entities in aided navigation.', 'author': 'Gomez, Juan Diego and Bologna, Guido and Pun, Thierry', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384959'}"
EZ ballot with multimodal inputs and outputs,10.1145/2384916.2384960,"Current accessible voting machines require many voters with visual, cognitive and dexterity limitations to vote with assistance, if they can vote at all. To address accessibility problems, we developed the EZ Ballot. The linear layout of the EZ ballot structure fundamentally re-conceptualizes ballot design to provide the same simple and intuitive voting experience for all voters, regardless of ability or input/output (I/O) device used. Further, multimodal I/O interfaces were seamlessly integrated with the ballot structure to provide flexibility in accommodating voters with different abilities.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'multimodal output, multimodal input, gestural input, accessible voting', 'numpages': '2', 'pages': '215–216', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Current accessible voting machines require many voters with visual, cognitive and dexterity limitations to vote with assistance, if they can vote at all. To address accessibility problems, we developed the EZ Ballot. The linear layout of the EZ ballot structure fundamentally re-conceptualizes ballot design to provide the same simple and intuitive voting experience for all voters, regardless of ability or input/output (I/O) device used. Further, multimodal I/O interfaces were seamlessly integrated with the ballot structure to provide flexibility in accommodating voters with different abilities.', 'doi': '10.1145/2384916.2384960', 'url': 'https://doi.org/10.1145/2384916.2384960', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'EZ ballot with multimodal inputs and outputs', 'author': 'Lee, Seunghyun and Xiong, Xiao and Yilin, Liu Elaine and Sanford, Jon', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384960'}"
Breath mobile: a software-based hands-free and voice-free breathing controlled mobile phone interface,10.1145/2384916.2384961,"This work proposes the use of a low-cost software based breathing interface for mobile phones as an alternative interaction technology for people with motor disabilities. It attempts to explore the processing of the audio from the microphone in mobile phones to trigger and launch software events. A proof of concept of this work is demonstrated by the implementation and experimentation of a mobile application prototype that enables users to perform a basic operation on the phone, such as calling through ""puffing"" interaction.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'mobile, breathing, alternative hci, accessibility', 'numpages': '2', 'pages': '217–218', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This work proposes the use of a low-cost software based breathing interface for mobile phones as an alternative interaction technology for people with motor disabilities. It attempts to explore the processing of the audio from the microphone in mobile phones to trigger and launch software events. A proof of concept of this work is demonstrated by the implementation and experimentation of a mobile application prototype that enables users to perform a basic operation on the phone, such as calling through ""puffing"" interaction.', 'doi': '10.1145/2384916.2384961', 'url': 'https://doi.org/10.1145/2384916.2384961', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Breath mobile: a software-based hands-free and voice-free breathing controlled mobile phone interface', 'author': ""Feij\\'{o} Filho, Jackson and Valle, Thiago and Prata, Wilson"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384961'}"
What is wrong with this word? dyseggxia: a game for children with dyslexia,10.1145/2384916.2384962,"We present Dyseggxia, a game application with word exercises for children with dyslexia. We design the content of the game combining linguistic and pedagogical criteria as well as corpus analysis. The main contributions are (i) designing exercises by using the analysis of errors written by people with dyslexia and (i) presenting Spanish reinforcement exercises in the form of a computer game. The game is available for free on iOS and Android.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'pedagogical exercises, ios, game, dyslexia, android', 'numpages': '2', 'pages': '219–220', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present Dyseggxia, a game application with word exercises for children with dyslexia. We design the content of the game combining linguistic and pedagogical criteria as well as corpus analysis. The main contributions are (i) designing exercises by using the analysis of errors written by people with dyslexia and (i) presenting Spanish reinforcement exercises in the form of a computer game. The game is available for free on iOS and Android.', 'doi': '10.1145/2384916.2384962', 'url': 'https://doi.org/10.1145/2384916.2384962', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'What is wrong with this word? dyseggxia: a game for children with dyslexia', 'author': 'Rello, Luz and Bayarri, Clara and Gorriz, Azuki', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384962'}"
Tapulator: a non-visual calculator using natural prefix-free codes,10.1145/2384916.2384963,"A new non-visual method of numeric entry into a smartphone is designed, implemented, and tested. Users tap the smartphone screen with one to three fingers or swipe the screen in order to enter numbers. No buttons are used--only simple, easy-to-remember gestures. A preliminary valuation with sighted users compares the method to a standard accessible numeric keyboard with a VoiceOver-like screen reader interface for non-visual entry. We found that users entered numbers faster and with higher accuracy with our number entry method than with a VoiceOver-like interface, showing there is potential for use among blind people as well. The Tapulator, a complete calculator based on this non-visual numeric entry that uses simple gestures for arithmetic operations and other calculator actions is described.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'number entry, non-visual interface, low-vision, calculator, blind', 'numpages': '2', 'pages': '221–222', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'A new non-visual method of numeric entry into a smartphone is designed, implemented, and tested. Users tap the smartphone screen with one to three fingers or swipe the screen in order to enter numbers. No buttons are used--only simple, easy-to-remember gestures. A preliminary valuation with sighted users compares the method to a standard accessible numeric keyboard with a VoiceOver-like screen reader interface for non-visual entry. We found that users entered numbers faster and with higher accuracy with our number entry method than with a VoiceOver-like interface, showing there is potential for use among blind people as well. The Tapulator, a complete calculator based on this non-visual numeric entry that uses simple gestures for arithmetic operations and other calculator actions is described.', 'doi': '10.1145/2384916.2384963', 'url': 'https://doi.org/10.1145/2384916.2384963', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Tapulator: a non-visual calculator using natural prefix-free codes', 'author': 'Ruamviboonsuk, Vaspol and Azenkot, Shiri and Ladner, Richard E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384963'}"
Design goals for a system for enhancing AAC with personalized video,10.1145/2384916.2384964,"Enabling end-users of Augmentative and Alternative Communication (AAC) systems to add personalized video content at runtime holds promise for improving communication, but the requirements for such systems are as yet unclear. To explore this issue, we present Vid2Speech, a prototype AAC system for children with complex communication needs (CCN) that uses personalized video to enhance representations of action words. We describe three design goals that guided the integration of personalized video to enhance AAC in our early-stage prototype: 1) Providing social-temporal navigation; 2) Enhancing comprehension; and 3) Enabling customization in real time. Our system concept represents one approach to realizing these goals, however, we contribute the goals and the system as a starting point for future innovations in personalized video-based AAC.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'video, complex communication needs, aac', 'numpages': '2', 'pages': '223–224', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Enabling end-users of Augmentative and Alternative Communication (AAC) systems to add personalized video content at runtime holds promise for improving communication, but the requirements for such systems are as yet unclear. To explore this issue, we present Vid2Speech, a prototype AAC system for children with complex communication needs (CCN) that uses personalized video to enhance representations of action words. We describe three design goals that guided the integration of personalized video to enhance AAC in our early-stage prototype: 1) Providing social-temporal navigation; 2) Enhancing comprehension; and 3) Enabling customization in real time. Our system concept represents one approach to realizing these goals, however, we contribute the goals and the system as a starting point for future innovations in personalized video-based AAC.', 'doi': '10.1145/2384916.2384964', 'url': 'https://doi.org/10.1145/2384916.2384964', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Design goals for a system for enhancing AAC with personalized video', 'author': ""O'Leary, Katie and Delahunt, Charles and Dowden, Patricia and Darmansya, Ivan and Heng, Jiaqi and Riskin, Eve A. and Ladner, Richard E. and Wobbrock, Jacob O."", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384964'}"
Liberi and the racer bike: exergaming technology for children with cerebral palsy,10.1145/2384916.2384965,"Children with cerebral palsy (CP) often have limited opportunities to engage in physical exercise and to interact with other children. We report on the design of a multiplayer exercise video game and a novel cycling-based exergaming station that allow children with CP to perform vigorous exercise while playing with other children. The game and the station were designed through an iterative and incremental participatory design process involving medical professionals, game designers, computer scientists, kinesiologists, physiotherapists, and eight children with CP. The station combines a physical platform allowing children with CP to provide pedaling input into a game, and a standard PC gamepad. With this station seven of eight children could play a cycling-based game effectively. The game is a virtual world featuring several minigames, group play, and an in-game money-based reward system. Abilities and limitations associated with CP were considered when designing the game. The data collected during the design sessions shows that the games are fun, engaging and allow the children to reach exertion levels recommended by the American College of Sports Medicine.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'exergaming station, exergames, children, cerebral palsy', 'numpages': '2', 'pages': '225–226', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Children with cerebral palsy (CP) often have limited opportunities to engage in physical exercise and to interact with other children. We report on the design of a multiplayer exercise video game and a novel cycling-based exergaming station that allow children with CP to perform vigorous exercise while playing with other children. The game and the station were designed through an iterative and incremental participatory design process involving medical professionals, game designers, computer scientists, kinesiologists, physiotherapists, and eight children with CP. The station combines a physical platform allowing children with CP to provide pedaling input into a game, and a standard PC gamepad. With this station seven of eight children could play a cycling-based game effectively. The game is a virtual world featuring several minigames, group play, and an in-game money-based reward system. Abilities and limitations associated with CP were considered when designing the game. The data collected during the design sessions shows that the games are fun, engaging and allow the children to reach exertion levels recommended by the American College of Sports Medicine.', 'doi': '10.1145/2384916.2384965', 'url': 'https://doi.org/10.1145/2384916.2384965', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Liberi and the racer bike: exergaming technology for children with cerebral palsy', 'author': 'Ye, Zi and Hernandez, Hamilton A. and Graham, T.C. Nicholas and Fehlings, Darcy and Switzer, Lauren and Hamza, Md Ameer and Schumann, Irina', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384965'}"
Blue herd: automated captioning for videoconferences,10.1145/2384916.2384966,"Blue Herd is a project in IBM Research to investigate automated captioning for videoconferences. Today videoconferences are held among meeting participants connected with a variety of devices: personal computers, mobile devices, and multi-participant meeting rooms. Blue Herd is charged with studying automated real-time captioning in that context. This poster explains the system that was developed for personal computers and describes our experiments to include mobile devices and multi-participant meeting rooms.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'transcription, speech recognition, automated captioning, accessibility', 'numpages': '2', 'pages': '227–228', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blue Herd is a project in IBM Research to investigate automated captioning for videoconferences. Today videoconferences are held among meeting participants connected with a variety of devices: personal computers, mobile devices, and multi-participant meeting rooms. Blue Herd is charged with studying automated real-time captioning in that context. This poster explains the system that was developed for personal computers and describes our experiments to include mobile devices and multi-participant meeting rooms.', 'doi': '10.1145/2384916.2384966', 'url': 'https://doi.org/10.1145/2384916.2384966', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Blue herd: automated captioning for videoconferences', 'author': 'Forman, Ira R. and Fletcher, Ben and Hartley, John and Rippon, Bill and Wilson, Allen', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384966'}"
Toward the development of a BCI and gestural interface to support individuals with physical disabilities,10.1145/2384916.2384967,"In this paper, we describe a first step towards the development of a solution to support the movement and repositioning of an individual's limbs. Limb repositioning is particularly valuable for individuals with physical disabilities who are either bed or chair-bound, to help reduce the occurrence of contractures and pressure ulcers. A data gathering study has been performed examining attitudes towards using BCI and gestural devices to control a robotic aid to assist with the repositioning process. Findings from a preliminary study evaluating a controller interface prototype suggest that while BCI and gestural technologies may play a valuable role in limiting fatigue from interacting with a mouse or other input device, challenges are faced accurately identifying specific facial expressions (e.g. blinks). Future work would aim to refine algorithms to detect gestures, with a view to augmenting the experience when using a BCI and gestural device to control a robotic aid.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'physical disabilities, gestural interfaces, bci', 'numpages': '2', 'pages': '229–230', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""In this paper, we describe a first step towards the development of a solution to support the movement and repositioning of an individual's limbs. Limb repositioning is particularly valuable for individuals with physical disabilities who are either bed or chair-bound, to help reduce the occurrence of contractures and pressure ulcers. A data gathering study has been performed examining attitudes towards using BCI and gestural devices to control a robotic aid to assist with the repositioning process. Findings from a preliminary study evaluating a controller interface prototype suggest that while BCI and gestural technologies may play a valuable role in limiting fatigue from interacting with a mouse or other input device, challenges are faced accurately identifying specific facial expressions (e.g. blinks). Future work would aim to refine algorithms to detect gestures, with a view to augmenting the experience when using a BCI and gestural device to control a robotic aid."", 'doi': '10.1145/2384916.2384967', 'url': 'https://doi.org/10.1145/2384916.2384967', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Toward the development of a BCI and gestural interface to support individuals with physical disabilities', 'author': 'Krishnaswamy, Kavita and Kuber, Ravi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384967'}"
An electronic-textile wearable communication board for individuals with autism engaged in horse therapy,10.1145/2384916.2384968,"Horse therapy is becoming an increasingly popular physical therapy activity for individuals with social, communication, or cognitive impairments as a means to help enhance social and interpersonal skills [1]. However, horse therapy and other very physically engaging therapies pose a challenge for those who rely on a communication board to communicate as the highly unstable nature of such activities impedes device operation. In such an instance, users are typically forced to abandon their communication board [2], rendering them unable to convey vital pieces of information throughout the duration of the physical therapy activity. This poster presents the Electronic-Textile (E-Textile) Wearable Communication Board - a device that was developed specifically to fill this void and support the communication needs of individuals with autism during horse therapy.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'wearable technology, horse therapy, electronic textiles, communication board, autism, assistive technology', 'numpages': '2', 'pages': '231–232', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Horse therapy is becoming an increasingly popular physical therapy activity for individuals with social, communication, or cognitive impairments as a means to help enhance social and interpersonal skills [1]. However, horse therapy and other very physically engaging therapies pose a challenge for those who rely on a communication board to communicate as the highly unstable nature of such activities impedes device operation. In such an instance, users are typically forced to abandon their communication board [2], rendering them unable to convey vital pieces of information throughout the duration of the physical therapy activity. This poster presents the Electronic-Textile (E-Textile) Wearable Communication Board - a device that was developed specifically to fill this void and support the communication needs of individuals with autism during horse therapy.', 'doi': '10.1145/2384916.2384968', 'url': 'https://doi.org/10.1145/2384916.2384968', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'An electronic-textile wearable communication board for individuals with autism engaged in horse therapy', 'author': 'Profita, Halley P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384968'}"
Tongible: a non-contact tongue-based interaction technique,10.1145/2384916.2384969,"Using tongue to access computer for people with none or minimal upper limb function has been studied in recent years. These studies mainly focus on utilizing mechanical or electromagnetic devices. These devices, however, must contact to people's oral cavity and cause hygiene problems or accidental ingestion. This work presents an interaction technique named Tongible that employs tongue as input without any mechanical or electromagnetic assistive device. In Tongible, six gestures of tongue are captured by an RGB camera and used as basic controlling gestures. Preliminary usability testing suggests that Tongible is effective in pointing and text entry for people with dexterity impairment.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tongue-computer interface, tile, non-contacted, interaction', 'numpages': '2', 'pages': '233–234', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Using tongue to access computer for people with none or minimal upper limb function has been studied in recent years. These studies mainly focus on utilizing mechanical or electromagnetic devices. These devices, however, must contact to people's oral cavity and cause hygiene problems or accidental ingestion. This work presents an interaction technique named Tongible that employs tongue as input without any mechanical or electromagnetic assistive device. In Tongible, six gestures of tongue are captured by an RGB camera and used as basic controlling gestures. Preliminary usability testing suggests that Tongible is effective in pointing and text entry for people with dexterity impairment."", 'doi': '10.1145/2384916.2384969', 'url': 'https://doi.org/10.1145/2384916.2384969', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Tongible: a non-contact tongue-based interaction technique', 'author': 'Liu, Li and Niu, Shuo and Ren, Jingjing and Zhang, Jingyuan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384969'}"
Effectiveness of the haptic chair in speech training,10.1145/2384916.2384970,"The 'Haptic Chair' [3] delivers vibrotactile stimulation to several parts of the body including the palmar surface of the hand (palm and fingers), and has been shown to have a significant positive effect on the enjoyment of music even by the profoundly deaf. In this paper, we explore the effectiveness of using the Haptic Chair during speech therapy for the deaf. We conducted a 24-week study with 20 profoundly deaf users to validate our initial observations. The improvements in word clarity observed over the duration of this study indicate that the Haptic Chair has the potential to make a significant contribution to speech therapy for the deaf.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'speech therapy, haptic feedback', 'numpages': '2', 'pages': '235–236', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""The 'Haptic Chair' [3] delivers vibrotactile stimulation to several parts of the body including the palmar surface of the hand (palm and fingers), and has been shown to have a significant positive effect on the enjoyment of music even by the profoundly deaf. In this paper, we explore the effectiveness of using the Haptic Chair during speech therapy for the deaf. We conducted a 24-week study with 20 profoundly deaf users to validate our initial observations. The improvements in word clarity observed over the duration of this study indicate that the Haptic Chair has the potential to make a significant contribution to speech therapy for the deaf."", 'doi': '10.1145/2384916.2384970', 'url': 'https://doi.org/10.1145/2384916.2384970', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Effectiveness of the haptic chair in speech training', 'author': 'Nanayakkara, Suranga and Wyse, Lonce and Taylor, Elizabeth A.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384970'}"
Access to UML diagrams with the HUTN,10.1145/2384916.2384971,"Modern software development includes the usage of UML for (model-driven) analysis and design, customer communication etc. Since UML is a graphical notation, alternative forms of representation are needed to avoid barriers for developers and other users with low vision. Here, Human-usable Textual Notation (HUTN) is tested and evaluated in a user interface modeling concept to provide accessible model-driven software design.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'unified modeling language (uml), modeling, human-usable textual notation (hutn), accessibility', 'numpages': '2', 'pages': '237–238', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Modern software development includes the usage of UML for (model-driven) analysis and design, customer communication etc. Since UML is a graphical notation, alternative forms of representation are needed to avoid barriers for developers and other users with low vision. Here, Human-usable Textual Notation (HUTN) is tested and evaluated in a user interface modeling concept to provide accessible model-driven software design.', 'doi': '10.1145/2384916.2384971', 'url': 'https://doi.org/10.1145/2384916.2384971', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Access to UML diagrams with the HUTN', 'author': 'Vieritz, Helmut and Schilberg, Daniel and Jeschke, Sabina', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384971'}"
An interactive play mat for deaf-blind infants,10.1145/2384916.2384972,"There is a great need for the development of interactive toys for deaf-blind infants (1-3 year olds) to motivate their exploration of their environment, and develop their motor and cognitive skills. We describe relevant design criteria, gleaned from the literature and a discussion with professionals who work with deaf-blind children. We then present a toy consisting of a play mat with three activity areas: one for remembering and repeating vibration patterns and two for matching textures. Vibrators which turn on as the infant moves in the direction of an activity area, measured by pressure sensors, are used to encourage the infant to explore in that direction.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'deaf-blind, cognitive development, assistive technology', 'numpages': '2', 'pages': '239–240', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'There is a great need for the development of interactive toys for deaf-blind infants (1-3 year olds) to motivate their exploration of their environment, and develop their motor and cognitive skills. We describe relevant design criteria, gleaned from the literature and a discussion with professionals who work with deaf-blind children. We then present a toy consisting of a play mat with three activity areas: one for remembering and repeating vibration patterns and two for matching textures. Vibrators which turn on as the infant moves in the direction of an activity area, measured by pressure sensors, are used to encourage the infant to explore in that direction.', 'doi': '10.1145/2384916.2384972', 'url': 'https://doi.org/10.1145/2384916.2384972', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'An interactive play mat for deaf-blind infants', 'author': ""O'Bryan, Crystal and Parvez, Amina and Pawluk, Dianne"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384972'}"
Investigating authentication methods used by individuals with down syndrome,10.1145/2384916.2384973,"Although there have been numerous studies investigating password usage by neurotypical users, a paucity of research has been conducted to examine the use of authentication methods used by individuals with cognitive impairment. In this paper, we report a longitudinal study that investigates how individuals with Down syndrome interact with three user authentication mechanisms. It confirms that many individuals with DS are capable of using traditional alphanumeric passwords as well as learning other authentication methods. Contrary to previous belief, the result suggests that mnemonic passwords may not be easier to remember for individuals with DS during initial usage.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'down syndrome, cognitive impairment, authentication', 'numpages': '2', 'pages': '241–242', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Although there have been numerous studies investigating password usage by neurotypical users, a paucity of research has been conducted to examine the use of authentication methods used by individuals with cognitive impairment. In this paper, we report a longitudinal study that investigates how individuals with Down syndrome interact with three user authentication mechanisms. It confirms that many individuals with DS are capable of using traditional alphanumeric passwords as well as learning other authentication methods. Contrary to previous belief, the result suggests that mnemonic passwords may not be easier to remember for individuals with DS during initial usage.', 'doi': '10.1145/2384916.2384973', 'url': 'https://doi.org/10.1145/2384916.2384973', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Investigating authentication methods used by individuals with down syndrome', 'author': 'Ma, Yao and Feng, Jinjuan Heidi and Kumin, Libby and Lazar, Jonathan and Sreeramareddy, Lakshmidevi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384973'}"
WatchMe: wrist-worn interface that makes remote monitoring seamless,10.1145/2384916.2384974,"Remote monitoring allows us to understand the regular living behaviors of the elderly and alert their loved ones in emergency situations. In this paper, we describe WatchMe, a software and hardware platform that focuses on making ambient monitoring intuitive and seamless. WatchMe system consists of the WatchMe server application and a WatchMe client application implemented on a regular wristwatch. Thus, it requires minimal effort to monitor and is less disruptive to the user. We hope that the WatchMe system will contribute to improving the lives of the elderly by creating a healthy link between them and their loved ones.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'health monitoring, assistive interface, ambient display', 'numpages': '2', 'pages': '243–244', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Remote monitoring allows us to understand the regular living behaviors of the elderly and alert their loved ones in emergency situations. In this paper, we describe WatchMe, a software and hardware platform that focuses on making ambient monitoring intuitive and seamless. WatchMe system consists of the WatchMe server application and a WatchMe client application implemented on a regular wristwatch. Thus, it requires minimal effort to monitor and is less disruptive to the user. We hope that the WatchMe system will contribute to improving the lives of the elderly by creating a healthy link between them and their loved ones.', 'doi': '10.1145/2384916.2384974', 'url': 'https://doi.org/10.1145/2384916.2384974', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'WatchMe: wrist-worn interface that makes remote monitoring seamless', 'author': 'Ransiri, Shanaka and Nanayakkara, Suranga', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384974'}"
Musica Parlata: a methodology to teach music to blind people,10.1145/2384916.2384975,"Music education for blind people heavily relies on Braille. The use of Braille for music causes difficulties for the blind student: new meanings for the Braille symbols have to be learned and the reading of the music is not immediate. More-over, in the majority of the cases, music teachers don't know Braille. Although Braille remains the primary means for music education for blind people, alternative methods can help. We propose a new methodology that helps the reading of music scores by means of a software that sings the name of the notes. Singing the name of the notes provides to a blind user a direct perception of the score. Moreover the information is directly conveyed to the student through the ear. Although the method has several limitations we believe that it is effective. The methodology is not intended to ""replace"" Braille, but only to offer a different approach to the study of music.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user interfaces for blind people, music learning, accessibility', 'numpages': '2', 'pages': '245–246', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Music education for blind people heavily relies on Braille. The use of Braille for music causes difficulties for the blind student: new meanings for the Braille symbols have to be learned and the reading of the music is not immediate. More-over, in the majority of the cases, music teachers don\'t know Braille. Although Braille remains the primary means for music education for blind people, alternative methods can help. We propose a new methodology that helps the reading of music scores by means of a software that sings the name of the notes. Singing the name of the notes provides to a blind user a direct perception of the score. Moreover the information is directly conveyed to the student through the ear. Although the method has several limitations we believe that it is effective. The methodology is not intended to ""replace"" Braille, but only to offer a different approach to the study of music.', 'doi': '10.1145/2384916.2384975', 'url': 'https://doi.org/10.1145/2384916.2384975', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Musica Parlata: a methodology to teach music to blind people', 'author': 'Capozzi, Alfredo and De Prisco, Roberto and Nasti, Michele and Zaccagnino, Rocco', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384975'}"
Supporting employment matching with mobile interfaces,10.1145/2384916.2384976,"People with cognitive disabilities need careful matching to find appropriate employment. However, it can be difficult for them to articulate their worries and concerns in a timely and useful manner. This work demonstrates how appropriately-designed technology can assist in accommodating cognitive disabilities, providing avenues to assess their concerns about work environments. A mobile application was developed with targeted repeated multimedia surveying to assess work concerns, with temporal- and geo-tagged answers stored for review by a personal assistant, employer, job coach, or person with a cognitive disability. An expert review provided feedback to ensure an appropriate application.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'mobile computing, cognitive disabilities', 'numpages': '2', 'pages': '247–248', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'People with cognitive disabilities need careful matching to find appropriate employment. However, it can be difficult for them to articulate their worries and concerns in a timely and useful manner. This work demonstrates how appropriately-designed technology can assist in accommodating cognitive disabilities, providing avenues to assess their concerns about work environments. A mobile application was developed with targeted repeated multimedia surveying to assess work concerns, with temporal- and geo-tagged answers stored for review by a personal assistant, employer, job coach, or person with a cognitive disability. An expert review provided feedback to ensure an appropriate application.', 'doi': '10.1145/2384916.2384976', 'url': 'https://doi.org/10.1145/2384916.2384976', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Supporting employment matching with mobile interfaces', 'author': 'Zhang, Ziyi and McCrickard, Scott and Tanis, Shea and Lewis, Clayton', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384976'}"
Combining emotion and facial nonmanual signals in synthesized american sign language,10.1145/2384916.2384977,"Translating from English to American Sign Language (ASL) requires an avatar to display synthesized ASL. Essential to the language are nonmanual signals that appear on the face. Previous avatars were hampered by an inability to portray emotion and facial nonmanual signals that occur at the same time. A new animation system addresses this challenge. Animations produced by the new system were tested with 40 members of the Deaf community in the United States. For each animation, participants were able to identify both nonmanual signals and emotional states. Co-occurring question nonmanuals and affect information were distinguishable, which is particularly striking because the two processes can move an avatar's brows in opposing directions.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'sign language synthesis, deaf, avatar, asl', 'numpages': '2', 'pages': '249–250', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Translating from English to American Sign Language (ASL) requires an avatar to display synthesized ASL. Essential to the language are nonmanual signals that appear on the face. Previous avatars were hampered by an inability to portray emotion and facial nonmanual signals that occur at the same time. A new animation system addresses this challenge. Animations produced by the new system were tested with 40 members of the Deaf community in the United States. For each animation, participants were able to identify both nonmanual signals and emotional states. Co-occurring question nonmanuals and affect information were distinguishable, which is particularly striking because the two processes can move an avatar's brows in opposing directions."", 'doi': '10.1145/2384916.2384977', 'url': 'https://doi.org/10.1145/2384916.2384977', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Combining emotion and facial nonmanual signals in synthesized american sign language', 'author': 'Schnepp, Jerry C. and Wolfe, Rosalee J. and McDonald, John C. and Toro, Jorge A.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384977'}"
"Displaying braille and graphics with a ""tactile mouse""",10.1145/2384916.2384978,"Refreshable tactile displays that move with the hand, such as those that resemble computer mice, can be utilized to display tactile graphics faster and more cost effectively to individuals who are blind and visually impaired than traditional paper methods of creating tactile diagrams. However, in tactile diagrams, the word labels can be as important as the diagram itself and so it is important that these displays can present Braille. In this work, we present and discuss findings from a study which used three methods of displaying Braille and tactile graphics simultaneously with a tactile mouse: Braille and graphics at the same amplitude level, Braille and graphics at different amplitude levels, and Braille with a box around it, The simplest method, Braille and graphics at the same amplitude, surprisingly proved to be the most effective.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tactile graphics, haptics, haptic mouse visually impaired, braille', 'numpages': '2', 'pages': '251–252', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Refreshable tactile displays that move with the hand, such as those that resemble computer mice, can be utilized to display tactile graphics faster and more cost effectively to individuals who are blind and visually impaired than traditional paper methods of creating tactile diagrams. However, in tactile diagrams, the word labels can be as important as the diagram itself and so it is important that these displays can present Braille. In this work, we present and discuss findings from a study which used three methods of displaying Braille and tactile graphics simultaneously with a tactile mouse: Braille and graphics at the same amplitude level, Braille and graphics at different amplitude levels, and Braille with a box around it, The simplest method, Braille and graphics at the same amplitude, surprisingly proved to be the most effective.', 'doi': '10.1145/2384916.2384978', 'url': 'https://doi.org/10.1145/2384916.2384978', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Displaying braille and graphics with a ""tactile mouse""', 'author': 'Hribar, Victoria E. and Deal, Laura G. and Pawluk, Dianne T.V.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384978'}"
A participatory design workshop on accessible apps and games with students with learning differences,10.1145/2384916.2384979,"This paper describes a Science-Technology-Engineering-Mathematics (STEM) outreach workshop conducted with post-secondary students diagnosed with learning differences, including Learning Disabilities (LD), Attention Deficit / Hyperactivity Disorders (AD/HD), and/or Autism Spectrum Disorders (ASD). In this workshop, students were actively involved in participatory design exercises such as data gathering, identifying accessible design requirements, and evaluating mobile applications and games targeted for diverse users. This hands-on experience broadened students' understanding of STEM areas, provided them with an opportunity to see themselves as computer scientists, and demonstrated how they might succeed in computing careers, especially in human-centered computing and interface design. Lessons learned from the workshop also offer useful insight on conducting participatory design with this unique population.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'stem, participatory design, learning differences, human-computer interaction, education, cognitive disabilities, accessibility', 'numpages': '2', 'pages': '253–254', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""This paper describes a Science-Technology-Engineering-Mathematics (STEM) outreach workshop conducted with post-secondary students diagnosed with learning differences, including Learning Disabilities (LD), Attention Deficit / Hyperactivity Disorders (AD/HD), and/or Autism Spectrum Disorders (ASD). In this workshop, students were actively involved in participatory design exercises such as data gathering, identifying accessible design requirements, and evaluating mobile applications and games targeted for diverse users. This hands-on experience broadened students' understanding of STEM areas, provided them with an opportunity to see themselves as computer scientists, and demonstrated how they might succeed in computing careers, especially in human-centered computing and interface design. Lessons learned from the workshop also offer useful insight on conducting participatory design with this unique population."", 'doi': '10.1145/2384916.2384979', 'url': 'https://doi.org/10.1145/2384916.2384979', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'A participatory design workshop on accessible apps and games with students with learning differences', 'author': 'Anthony, Lisa and Prasad, Sapna and Hurst, Amy and Kuber, Ravi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384979'}"
Hybrid auditory feedback: a new method for mobility assistance of the visually impaired,10.1145/2384916.2384980,"In this paper we present a novel concept of hybrid auditory feedback in mobility assistance for people with visual disabilities in indoor environment. Hybrid auditory feedback is a gradual con-version of sound from speech-only to non-speech (i.e., spearcons) based on the sound repetitiveness and the users' frequency of the travelled route. Using a within-subject design, eight participants carried out a task using a mobility assistant application and followed a same route for few days. Preliminary results suggest that hybrid sounds in auditory feedback are more effective than non-speech and are pleasant compared to speech-only.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'visually impaired, speech, non-speech, mobility assistance, hybrid, auditory feedback', 'numpages': '2', 'pages': '255–256', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""In this paper we present a novel concept of hybrid auditory feedback in mobility assistance for people with visual disabilities in indoor environment. Hybrid auditory feedback is a gradual con-version of sound from speech-only to non-speech (i.e., spearcons) based on the sound repetitiveness and the users' frequency of the travelled route. Using a within-subject design, eight participants carried out a task using a mobility assistant application and followed a same route for few days. Preliminary results suggest that hybrid sounds in auditory feedback are more effective than non-speech and are pleasant compared to speech-only."", 'doi': '10.1145/2384916.2384980', 'url': 'https://doi.org/10.1145/2384916.2384980', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Hybrid auditory feedback: a new method for mobility assistance of the visually impaired', 'author': 'Hussain, Ibrar and Chen, Ling and Mirza, Hamid Turab and Majid, Abdul and Chen, Gencai', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384980'}"
ClickerAID: a tool for efficient clicking using intentional muscle contractions,10.1145/2384916.2384981,"This is to propose a demo and poster about a tool designed to assist persons who are temporarily or permanently unable to reliably operate the buttons of a physical pointing device, for example because of tenosynovitis (TSV). It monitors a dedicated muscle of the user and emulates a click event at the current position of the mouse pointer in response to a contraction of that muscle (as small as raising the eyebrow). The ClickType (= type of the click) - left, right, single, double, drag - is selected by the user (who is also responsible for moving the mouse pointer) and stays valid until the selection of a new one.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tenosynovitis, mouse clicks, layered windows, intentional muscle contractions, human-computer interaction, eye tracker', 'numpages': '2', 'pages': '257–258', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This is to propose a demo and poster about a tool designed to assist persons who are temporarily or permanently unable to reliably operate the buttons of a physical pointing device, for example because of tenosynovitis (TSV). It monitors a dedicated muscle of the user and emulates a click event at the current position of the mouse pointer in response to a contraction of that muscle (as small as raising the eyebrow). The ClickType (= type of the click) - left, right, single, double, drag - is selected by the user (who is also responsible for moving the mouse pointer) and stays valid until the selection of a new one.', 'doi': '10.1145/2384916.2384981', 'url': 'https://doi.org/10.1145/2384916.2384981', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'ClickerAID: a tool for efficient clicking using intentional muscle contractions', 'author': 'Felzer, Torsten and Rinderknecht, Stephan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384981'}"
Assistive system experiment designer ASED: a toolkit for the quantitative evaluation of enhanced assistive systems for impaired persons in production,10.1145/2384916.2384982,"This paper introduces the toolkit ASED: Assistive System Experiment Designer. Combining a specially constructed assembly table and new software it allows measuring the performance of impaired persons when using assistive systems for production environments (ASiPE). The ASiPE design tested using ASED transgresses the state of the art by three enhancements. With the help of ASED we are able to quantify and rank their effects on work quality and performance. The ASED toolkit, however, is not confined to the design tested but can be used for the experimental analysis of every kind of manual process.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'user-centered design, motion recognition, human computer interaction (hci), evaluation, disabled, assistive technology', 'numpages': '2', 'pages': '259–260', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper introduces the toolkit ASED: Assistive System Experiment Designer. Combining a specially constructed assembly table and new software it allows measuring the performance of impaired persons when using assistive systems for production environments (ASiPE). The ASiPE design tested using ASED transgresses the state of the art by three enhancements. With the help of ASED we are able to quantify and rank their effects on work quality and performance. The ASED toolkit, however, is not confined to the design tested but can be used for the experimental analysis of every kind of manual process.', 'doi': '10.1145/2384916.2384982', 'url': 'https://doi.org/10.1145/2384916.2384982', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Assistive system experiment designer ASED: a toolkit for the quantitative evaluation of enhanced assistive systems for impaired persons in production', 'author': 'Korn, Oliver and Schmidt, Albrecht and H\\""{o}rz, Thomas and Kaupp, Daniel', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384982'}"
Optimizing gaze typing for people with severe motor disabilities: the iWriter arabic interface,10.1145/2384916.2384983,"Communication in the Arabic language with gaze using dwell time has been made possible by the development of eye typing interfaces. This paper describes the design process for developing iWriter, an Arabic gaze communication system. Design considerations for the optimization of the gaze typing interfaces for Arabic script are discussed.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'severe motor disability, locked-in syndrome, gaze communication, eye typing, eye tracking, augmentative, alternative communication, aac', 'numpages': '2', 'pages': '261–262', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Communication in the Arabic language with gaze using dwell time has been made possible by the development of eye typing interfaces. This paper describes the design process for developing iWriter, an Arabic gaze communication system. Design considerations for the optimization of the gaze typing interfaces for Arabic script are discussed.', 'doi': '10.1145/2384916.2384983', 'url': 'https://doi.org/10.1145/2384916.2384983', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Optimizing gaze typing for people with severe motor disabilities: the iWriter arabic interface', 'author': 'Al-Wabil, Areej and Al-Issa, Arwa and Hazzaa, Itisam and Al-Humaimeedi, May and Al-Tamimi, Lujain and Al-Kadhi, Bushra', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384983'}"
Toward a design of word processing environment for people with disabilities,10.1145/2384916.2384984,"The study presented in this paper is aimed at identifying text editing actions that are routinely performed by the users when composing a text document and that are not directly supported by common word processors. The results of this study will help a design of a novel word processing interface controlled by device with a limited number of input signals, operated by people with certain motor disabilities.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'work in progress, word processing, text input, motor impairments', 'numpages': '2', 'pages': '263–264', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The study presented in this paper is aimed at identifying text editing actions that are routinely performed by the users when composing a text document and that are not directly supported by common word processors. The results of this study will help a design of a novel word processing interface controlled by device with a limited number of input signals, operated by people with certain motor disabilities.', 'doi': '10.1145/2384916.2384984', 'url': 'https://doi.org/10.1145/2384916.2384984', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Toward a design of word processing environment for people with disabilities', 'author': 'Sporka, Adam J. and Polacek, Ondrej', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384984'}"
Preliminary evaluation of three eyes-free interfaces for point-and-click computer games,10.1145/2384916.2384985,This paper presents a preliminary evaluation of the perceived entertainment value and ease of use of three eyes-free interfaces for point-and-click games. Interface 1 (I1) uses a web-like cyclical navigation system to change the focused interactive element. Interface 2 (I2) uses a sonar to help the user locate interactive elements with the mouse. Interface 3 (I3) interprets natural language commands typed in by the player. Results suggest that I2 adds more entertainment value and is appropriate for experienced players. Players find I1 is the easiest to use while I3 seems more adequate for users with little gaming experience.,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'eyes-free games, audio 3d, accessibility', 'numpages': '2', 'pages': '265–266', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper presents a preliminary evaluation of the perceived entertainment value and ease of use of three eyes-free interfaces for point-and-click games. Interface 1 (I1) uses a web-like cyclical navigation system to change the focused interactive element. Interface 2 (I2) uses a sonar to help the user locate interactive elements with the mouse. Interface 3 (I3) interprets natural language commands typed in by the player. Results suggest that I2 adds more entertainment value and is appropriate for experienced players. Players find I1 is the easiest to use while I3 seems more adequate for users with little gaming experience.', 'doi': '10.1145/2384916.2384985', 'url': 'https://doi.org/10.1145/2384916.2384985', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Preliminary evaluation of three eyes-free interfaces for point-and-click computer games', 'author': ""Torrente, Javier and Marchiori, Eugenio J. and Vallejo-Pinto, Jos\\'{e} \\'{A}ngel and Ortega-Moral, Manuel and Moreno-Ger, Pablo and Fern\\'{a}ndez-Manj\\'{o}n, Baltasar"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384985'}"
Accessible collaborative writing for persons who are blind: a usability study,10.1145/2384916.2384986,"Collaborative writing applications are widely utilized in organizations to co-author documents and jointly exchange ideas. Unfortunately, for persons who are blind, collaborative writing applications are often difficult to access and use. Therefore, this paper presents the results from several usability studies that examined how visually able persons and persons who are blind interact with collaborative writing applications, and the accessibility and usability issues they encounter.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'usability, microsoft word, human-computer interaction, google docs, collaborative writing, blind, accessibility', 'numpages': '2', 'pages': '267–268', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Collaborative writing applications are widely utilized in organizations to co-author documents and jointly exchange ideas. Unfortunately, for persons who are blind, collaborative writing applications are often difficult to access and use. Therefore, this paper presents the results from several usability studies that examined how visually able persons and persons who are blind interact with collaborative writing applications, and the accessibility and usability issues they encounter.', 'doi': '10.1145/2384916.2384986', 'url': 'https://doi.org/10.1145/2384916.2384986', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Accessible collaborative writing for persons who are blind: a usability study', 'author': 'Schoeberlein, John G. and Wang, Yuanqiong', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384986'}"
E-Arithmetic: non-visual arithmetic manipulation for students with impaired vision,10.1145/2384916.2384987,"In this paper we present a web-based system that enables children with impaired vision to handle basic arithmetic knowledge: addition, subtraction, multiplication, and division. Taking into consideration the accommodation of varied levels of vision disability - minor to severe - the new system provides an electronic auditory alternative to the currently used tools.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'math accessibility, assistive technology, arithmetic skills', 'numpages': '2', 'pages': '269–270', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this paper we present a web-based system that enables children with impaired vision to handle basic arithmetic knowledge: addition, subtraction, multiplication, and division. Taking into consideration the accommodation of varied levels of vision disability - minor to severe - the new system provides an electronic auditory alternative to the currently used tools.', 'doi': '10.1145/2384916.2384987', 'url': 'https://doi.org/10.1145/2384916.2384987', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'E-Arithmetic: non-visual arithmetic manipulation for students with impaired vision', 'author': 'Alajarmeh, Nancy and Pontelli, Enrico', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384987'}"
A music application for visually impaired people using daily goods and stationeries on the table,10.1145/2384916.2384988,"Music applications, like a GarageBand, have become more popular today because they afford intuitive comfortable visual interface for users to play, remix, and compose music. But visually impaired people have difficulties to use such a software application. Therefore, this paper introduces a novel music interface for visually impaired people using daily goods and stationery on the table. An experimental system was developed with Kinect for AR marker and gesture recognition, and with sounds of three instruments, the piano, the guitar and the percussion. Five blind young people participated in the evaluation of right combination of the goods. The results shows that the proposed interface is effective both single use and collaborative work.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'visually impaired people, music application, kinect, ar marker', 'numpages': '2', 'pages': '271–272', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Music applications, like a GarageBand, have become more popular today because they afford intuitive comfortable visual interface for users to play, remix, and compose music. But visually impaired people have difficulties to use such a software application. Therefore, this paper introduces a novel music interface for visually impaired people using daily goods and stationery on the table. An experimental system was developed with Kinect for AR marker and gesture recognition, and with sounds of three instruments, the piano, the guitar and the percussion. Five blind young people participated in the evaluation of right combination of the goods. The results shows that the proposed interface is effective both single use and collaborative work.', 'doi': '10.1145/2384916.2384988', 'url': 'https://doi.org/10.1145/2384916.2384988', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'A music application for visually impaired people using daily goods and stationeries on the table', 'author': 'Yairi, Ikuko Eguchi and Takeda, Takuya', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384988'}"
A feasibility study of crowdsourcing and google street view to determine sidewalk accessibility,10.1145/2384916.2384989,"We explore the feasibility of using crowd workers from Amazon Mechanical Turk to identify and rank sidewalk accessibility issues from a manually curated database of 100 Google Street View images. We examine the effect of three different interactive labeling interfaces (Point, Rectangle, and Outline) on task accuracy and duration. We close the paper by discussing limitations and opportunities for future work.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'mechanical turk, google street view, crowdsourcing accessibility, accessible urban navigation', 'numpages': '2', 'pages': '273–274', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We explore the feasibility of using crowd workers from Amazon Mechanical Turk to identify and rank sidewalk accessibility issues from a manually curated database of 100 Google Street View images. We examine the effect of three different interactive labeling interfaces (Point, Rectangle, and Outline) on task accuracy and duration. We close the paper by discussing limitations and opportunities for future work.', 'doi': '10.1145/2384916.2384989', 'url': 'https://doi.org/10.1145/2384916.2384989', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'A feasibility study of crowdsourcing and google street view to determine sidewalk accessibility', 'author': 'Hara, Kotaro and Le, Victoria and Froehlich, Jon', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384989'}"
Replicating semantic connections made by visual readers for a scanning system for nonvisual readers,10.1145/2384916.2384990,"When scanning through a text document for the answer to a question, visual readers are able to quickly locate text within the document related to the answer while simultaneously getting a general sense of the document's content. For nonvisual readers, however, this poses a challenge, especially when the relevant text is spread out or worded in a way that can't be searched for directly. Our goal is to make the scanning experience quicker for nonvisual readers by giving them an experience similar to that of visual readers. To do this we first determined what visual scanners focused on by using an eye-tracker while they scanned for answers to complex questions. Resulting data revealed that text with loose semantic connections to the question are important. This paper reports on our efforts to develop a method that automatically replicates the connections made by visual scanners. Ultimately, our goal is a system that replicates the visual scanning experience, allowing nonvisual readers to quickly glean information in a manner similar to how visual readers glean information when scanning. This work stems from work with students who are nonvisual readers and is aimed at making their school experience more equitable with students who scan visually.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'word clustering, text scanning, natural language processing, assistive technology', 'numpages': '2', 'pages': '275–276', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""When scanning through a text document for the answer to a question, visual readers are able to quickly locate text within the document related to the answer while simultaneously getting a general sense of the document's content. For nonvisual readers, however, this poses a challenge, especially when the relevant text is spread out or worded in a way that can't be searched for directly. Our goal is to make the scanning experience quicker for nonvisual readers by giving them an experience similar to that of visual readers. To do this we first determined what visual scanners focused on by using an eye-tracker while they scanned for answers to complex questions. Resulting data revealed that text with loose semantic connections to the question are important. This paper reports on our efforts to develop a method that automatically replicates the connections made by visual scanners. Ultimately, our goal is a system that replicates the visual scanning experience, allowing nonvisual readers to quickly glean information in a manner similar to how visual readers glean information when scanning. This work stems from work with students who are nonvisual readers and is aimed at making their school experience more equitable with students who scan visually."", 'doi': '10.1145/2384916.2384990', 'url': 'https://doi.org/10.1145/2384916.2384990', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Replicating semantic connections made by visual readers for a scanning system for nonvisual readers', 'author': 'Yarrington, Debra and McCoy, Kathleen F.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384990'}"
It is not a talking book;: it is more like really reading a book!,10.1145/2384916.2384991,"In this research we designed, developed, and tested a reading system that enables Individuals with Blindness or Severe Visual Impairment (IBSVI) to fuse audio, tactile landmarks, and spatial information in order to read. This system renders electronic text documents on iPad-type devices, and reads aloud each word touched by the user's finger. A tactile overlay on the iPad screen helps IBSVI to navigate a page, furnishing a framework of tactile landmarks to give IBSVI a sense of place on the page. As the user moves her finger along the tangible pattern of the overlay, the text on the iPad screen that is touched is rendered audibly using a text-to-speech bsynthesizer.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'touch devices, spatial capabilities, reading process, blindness', 'numpages': '2', 'pages': '277–278', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""In this research we designed, developed, and tested a reading system that enables Individuals with Blindness or Severe Visual Impairment (IBSVI) to fuse audio, tactile landmarks, and spatial information in order to read. This system renders electronic text documents on iPad-type devices, and reads aloud each word touched by the user's finger. A tactile overlay on the iPad screen helps IBSVI to navigate a page, furnishing a framework of tactile landmarks to give IBSVI a sense of place on the page. As the user moves her finger along the tangible pattern of the overlay, the text on the iPad screen that is touched is rendered audibly using a text-to-speech bsynthesizer."", 'doi': '10.1145/2384916.2384991', 'url': 'https://doi.org/10.1145/2384916.2384991', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'It is not a talking book;: it is more like really reading a book!', 'author': 'El-Glaly, Yasmine N. and Quek, Francis and Smith-Jackson, Tonya L. and Dhillon, Gurjot', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384991'}"
MeetUp: a universally designed smartphone application to find another,10.1145/2384916.2384992,"A universally designed mobile application, MeetUp, that assists users to meet up is designed and implemented for blind and sighted users. The Android application MeetUp was originally designed for blind users, but a visual interface was added so it could become usable by sighted users as well. The system design and user interactions with the application are described.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'way-finding, navigation, blind, accessibility', 'numpages': '2', 'pages': '279–280', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'A universally designed mobile application, MeetUp, that assists users to meet up is designed and implemented for blind and sighted users. The Android application MeetUp was originally designed for blind users, but a visual interface was added so it could become usable by sighted users as well. The system design and user interactions with the application are described.', 'doi': '10.1145/2384916.2384992', 'url': 'https://doi.org/10.1145/2384916.2384992', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'MeetUp: a universally designed smartphone application to find another', 'author': 'Kim, Nara and Moyers, Matthew I.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384992'}"
Gesture interface magnifiers for low-vision users,10.1145/2384916.2384993,"This study compared different types of magnification and navigation methods on low-vision handheld magnifiers to determine the feasibility of a touch screen gesture interface. The results show that despite the fact that participants had no experience using gestures for magnification or navigation, participants were more satisfied with them. Gestures were faster and more preferred than the indirect input methods for pushing a button or rotating a knob, which had previously been familiar to participants from other electronic device interfaces. The study suggests that the use of gestures may afford an alternative and more natural magnification and navigation method for a new user-centric low vision magnifier.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'low vision video magnifier, gesture', 'numpages': '2', 'pages': '281–282', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This study compared different types of magnification and navigation methods on low-vision handheld magnifiers to determine the feasibility of a touch screen gesture interface. The results show that despite the fact that participants had no experience using gestures for magnification or navigation, participants were more satisfied with them. Gestures were faster and more preferred than the indirect input methods for pushing a button or rotating a knob, which had previously been familiar to participants from other electronic device interfaces. The study suggests that the use of gestures may afford an alternative and more natural magnification and navigation method for a new user-centric low vision magnifier.', 'doi': '10.1145/2384916.2384993', 'url': 'https://doi.org/10.1145/2384916.2384993', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Gesture interface magnifiers for low-vision users', 'author': 'Lee, Seunghyun (Tina) and Sanford, Jon', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384993'}"
3D point of gaze estimation using head-mounted RGB-D cameras,10.1145/2384916.2384994,"This paper presents a low-cost, wearable headset for 3D Point of Gaze (PoG) estimation in assistive applications. The device consists of an eye tracking camera and forward facing RGB-D scene camera which, together, provide an estimate of the user gaze vector and its intersection with a 3D point in space. The resulting system is able to compute the 3D PoG in real-time using inexpensive and readily available hardware components.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'multimodal systems, human-computer interaction, eyetracking, assistive environments', 'numpages': '2', 'pages': '283–284', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper presents a low-cost, wearable headset for 3D Point of Gaze (PoG) estimation in assistive applications. The device consists of an eye tracking camera and forward facing RGB-D scene camera which, together, provide an estimate of the user gaze vector and its intersection with a 3D point in space. The resulting system is able to compute the 3D PoG in real-time using inexpensive and readily available hardware components.', 'doi': '10.1145/2384916.2384994', 'url': 'https://doi.org/10.1145/2384916.2384994', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': '3D point of gaze estimation using head-mounted RGB-D cameras', 'author': 'McMurrough, Christopher and Conly, Christopher and Athitsos, Vassilis and Makedon, Fillia', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384994'}"
Displaying error \&amp; uncertainty in auditory graphs,10.1145/2384916.2384995,"Clear representation of uncertainty or error is crucial in graphs and other displays of data. Error bars are quite common in visual graphs, even though they are not necessarily well-designed, and often are not well understood, even by those who use them often (e.g., scientists, engineers). There has been little study of how to represent uncertainty in auditory graphs, such as those used increasingly by students and scientists with vision impairment. This study used conceptual magnitude estimation to determine how well different auditory dimensions (frequency, tempo) can represent error and uncertainty. The results will lead to more effective auditory displays of quantitative information and data.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'sonification, magnitude estimation, auditory display', 'numpages': '2', 'pages': '285–286', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Clear representation of uncertainty or error is crucial in graphs and other displays of data. Error bars are quite common in visual graphs, even though they are not necessarily well-designed, and often are not well understood, even by those who use them often (e.g., scientists, engineers). There has been little study of how to represent uncertainty in auditory graphs, such as those used increasingly by students and scientists with vision impairment. This study used conceptual magnitude estimation to determine how well different auditory dimensions (frequency, tempo) can represent error and uncertainty. The results will lead to more effective auditory displays of quantitative information and data.', 'doi': '10.1145/2384916.2384995', 'url': 'https://doi.org/10.1145/2384916.2384995', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Displaying error \\&amp; uncertainty in auditory graphs', 'author': 'Batterman, Jared M. and Walker, Bruce N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384995'}"
Visualizations for self-reflection on mouse pointer performance for older adults,10.1145/2384916.2384996,"Aging causes physical and cognitive changes that influence how we interact with the world around us. As personal data becomes increasingly available from a variety of sources, older adults can use this information to better understand these changes and adapt. Our project explores information visualization as a tool to help older adults interpret and understand their own personal data. To test this concept, we created visualizations of user's pointer performance metrics to help demystify problems in real-world mouse use. In a user study conducted with older adults with a range of computing experience, we learned that visualizations such as these can be a highly engaging information medium for this population. This paper presents our value-driven design process and recommendations for visualizations for older adults.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'reflection, pointing, personal informatics, older adults', 'numpages': '2', 'pages': '287–288', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Aging causes physical and cognitive changes that influence how we interact with the world around us. As personal data becomes increasingly available from a variety of sources, older adults can use this information to better understand these changes and adapt. Our project explores information visualization as a tool to help older adults interpret and understand their own personal data. To test this concept, we created visualizations of user's pointer performance metrics to help demystify problems in real-world mouse use. In a user study conducted with older adults with a range of computing experience, we learned that visualizations such as these can be a highly engaging information medium for this population. This paper presents our value-driven design process and recommendations for visualizations for older adults."", 'doi': '10.1145/2384916.2384996', 'url': 'https://doi.org/10.1145/2384916.2384996', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Visualizations for self-reflection on mouse pointer performance for older adults', 'author': 'Jones, Jasmine and Hall, Steven and Gentis, Mieke and Reynolds, Carrie and Gadwal, Chitra and Hurst, Amy and Ronch, Judah and Neylan, Callie', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384996'}"
Accessible skimming: faster screen reading of web pages,10.1145/2384916.2384998,"Sighted people know how to quickly glance over the headlines and news articles online to get the gist of information. On the other hand, people who are blind use screen-readers to listen through the content narrated by a serial audio interface. This interface does not give them an opportunity to know what content to skip and what to listen to. In this work, I present an automated approach to facilitate non-visual skimming of web pages.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'web browser, user interface, skimming, screen reader, blind, audio interface, assistive technology, accessibility', 'numpages': '2', 'pages': '289–290', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sighted people know how to quickly glance over the headlines and news articles online to get the gist of information. On the other hand, people who are blind use screen-readers to listen through the content narrated by a serial audio interface. This interface does not give them an opportunity to know what content to skip and what to listen to. In this work, I present an automated approach to facilitate non-visual skimming of web pages.', 'doi': '10.1145/2384916.2384998', 'url': 'https://doi.org/10.1145/2384916.2384998', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Accessible skimming: faster screen reading of web pages', 'author': 'Ahmed, Faisal', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384998'}"
Accessible web automation interface: a user study,10.1145/2384916.2384999,"With the growth of the Web as a platform for performing many useful daily tasks, such as shopping and paying bills, and as an important vehicle for doing business, the Web's potential to improve the quality of life of blind and low-vision users is greater than ever. However, the growth of sophistication of Web applications continues to outpace the capabilities of tools that help make the Web more accessible. Web automation has the potential to bridge the divide between the ways visually impaired users and sighted users access the Web, and enable visually impaired users to breeze through Web browsing tasks that beforehand were slow, hard, or even impossible to achieve. Typical automation interfaces require that the user record a macro, a useful sequence of browsing steps, so that these steps can be replayed in the future. In this paper, I present the results of evaluation of two web automation user interfaces that enable web automation without having to record macros. The experiments suggest that the approach has the potential to significantly increase accessibility and usability of web pages by reducing interaction time, and by enhancing user experience.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'web browser, web accessibility, screen reader, non-visual, macro recorder, macro player, low-vision users, blind users, audio interface', 'numpages': '2', 'pages': '291–292', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""With the growth of the Web as a platform for performing many useful daily tasks, such as shopping and paying bills, and as an important vehicle for doing business, the Web's potential to improve the quality of life of blind and low-vision users is greater than ever. However, the growth of sophistication of Web applications continues to outpace the capabilities of tools that help make the Web more accessible. Web automation has the potential to bridge the divide between the ways visually impaired users and sighted users access the Web, and enable visually impaired users to breeze through Web browsing tasks that beforehand were slow, hard, or even impossible to achieve. Typical automation interfaces require that the user record a macro, a useful sequence of browsing steps, so that these steps can be replayed in the future. In this paper, I present the results of evaluation of two web automation user interfaces that enable web automation without having to record macros. The experiments suggest that the approach has the potential to significantly increase accessibility and usability of web pages by reducing interaction time, and by enhancing user experience."", 'doi': '10.1145/2384916.2384999', 'url': 'https://doi.org/10.1145/2384916.2384999', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Accessible web automation interface: a user study', 'author': 'Puzis, Yury', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2384999'}"
Detecting hunchback behavior in autistic children with smart phone assistive devices,10.1145/2384916.2385000,"The research target in this case study was an autistic student at a special education school, who often unconsciously became hunchbacked during group activities or when not talking to people. We designed a system for hunchback detection using smart phone. Combined with an assistive T-shirt, the system could detect whether his/her back was hunched. Through this demonstration, we showcased the potential of using smart phones to develop simpler and more effective assistive devices for people with disabilities.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'hunchback, autism, assistive technology, accelerometer', 'numpages': '2', 'pages': '293–294', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The research target in this case study was an autistic student at a special education school, who often unconsciously became hunchbacked during group activities or when not talking to people. We designed a system for hunchback detection using smart phone. Combined with an assistive T-shirt, the system could detect whether his/her back was hunched. Through this demonstration, we showcased the potential of using smart phones to develop simpler and more effective assistive devices for people with disabilities.', 'doi': '10.1145/2384916.2385000', 'url': 'https://doi.org/10.1145/2384916.2385000', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Detecting hunchback behavior in autistic children with smart phone assistive devices', 'author': 'Lin, Shu-Hsien', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385000'}"
Detecting the hand-mouthing behavior of children with intellectual disability using Kinect imaging technology,10.1145/2384916.2385001,"Research indicates that approximately 17\% of individuals with intellectual disability engage in hand-mouthing behavior. The proportion is even higher among those with extremely severe intellectual disability. Stereotypic and excessive hand-mouthing behavior may lead to an unpleasant odor, lesions of the skin and muscular tissues, and infections. Typically, a substantial amount of staff intervention is required for a special education teacher to correct hand-mouthing behavior. However, this results in prolonged treatment periods and has negative effects on the students' learning and interaction with their peers, which leads to barriers in their social integration. In this study, we applied Kinect imaging technology to detect children's hand-mouthing behavior. This method enables rapid verification of the hand mouthing intervention strategies proposed by special education teachers, thereby reducing students' hand-mouthing behavior and facilitating individual learning.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'severe intellectual disabilities, kinect, hand mouthing behavior', 'numpages': '2', 'pages': '295–296', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Research indicates that approximately 17\\% of individuals with intellectual disability engage in hand-mouthing behavior. The proportion is even higher among those with extremely severe intellectual disability. Stereotypic and excessive hand-mouthing behavior may lead to an unpleasant odor, lesions of the skin and muscular tissues, and infections. Typically, a substantial amount of staff intervention is required for a special education teacher to correct hand-mouthing behavior. However, this results in prolonged treatment periods and has negative effects on the students' learning and interaction with their peers, which leads to barriers in their social integration. In this study, we applied Kinect imaging technology to detect children's hand-mouthing behavior. This method enables rapid verification of the hand mouthing intervention strategies proposed by special education teachers, thereby reducing students' hand-mouthing behavior and facilitating individual learning."", 'doi': '10.1145/2384916.2385001', 'url': 'https://doi.org/10.1145/2384916.2385001', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Detecting the hand-mouthing behavior of children with intellectual disability using Kinect imaging technology', 'author': 'Wei, Tzu-Wei', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385001'}"
Face tracking user interfaces using vision-based consumer devices,10.1145/2384916.2385002,One form of natural user interaction with a personal computer is based on natural face movements. This is especially helpful for users who cannot effectively use common input devices with their hands but have sufficient control of their heads. Using vision-based consumer devices makes such a user interface readily available and allows its use to be non-intrusive. This user interface presents some significant challenges particularly with accuracy and design. This research aims to investigate such problems and discover solutions to creating a usable and robust face tracking user interface using currently available technology. Design requirements were set and different design options were implemented and evaluated.,"{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'tracking, perceptual user interface, face, detection, depth, consumer devices, assistive technology, accessibility', 'numpages': '2', 'pages': '297–298', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'One form of natural user interaction with a personal computer is based on natural face movements. This is especially helpful for users who cannot effectively use common input devices with their hands but have sufficient control of their heads. Using vision-based consumer devices makes such a user interface readily available and allows its use to be non-intrusive. This user interface presents some significant challenges particularly with accuracy and design. This research aims to investigate such problems and discover solutions to creating a usable and robust face tracking user interface using currently available technology. Design requirements were set and different design options were implemented and evaluated.', 'doi': '10.1145/2384916.2385002', 'url': 'https://doi.org/10.1145/2384916.2385002', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Face tracking user interfaces using vision-based consumer devices', 'author': 'Villaroman, Norman H.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385002'}"
Kinempt: a Kinect-based prompting system to transition autonomously through vocational tasks for individuals with cognitive impairments,10.1145/2384916.2385003,"Kinect is used as assistive technology for individuals with cognitive impairments to achieve the goal of performing task steps independently. In a community-based rehabilitation program under the guidance of three job coaches, a task prompting system called Kinempt was designed to assist two participants involving pre-service food preparation training. Results indicate that for participants with cognitive disabilities, acquisition of job skills may be facilitated by use of Kinempt in conjunction with operant conditioning strategies.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'kinect', 'numpages': '2', 'pages': '299–300', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Kinect is used as assistive technology for individuals with cognitive impairments to achieve the goal of performing task steps independently. In a community-based rehabilitation program under the guidance of three job coaches, a task prompting system called Kinempt was designed to assist two participants involving pre-service food preparation training. Results indicate that for participants with cognitive disabilities, acquisition of job skills may be facilitated by use of Kinempt in conjunction with operant conditioning strategies.', 'doi': '10.1145/2384916.2385003', 'url': 'https://doi.org/10.1145/2384916.2385003', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Kinempt: a Kinect-based prompting system to transition autonomously through vocational tasks for individuals with cognitive impairments', 'author': 'Tsai, Yu-Chi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385003'}"
Reusable game interfaces for people with disabilities,10.1145/2384916.2385004,"Computer games are a very popular media today, spanning across multiple aspects of life, not only leisure but also health or education. But despite their importance their current level of accessibility is still low. One of the causes is that accessibility has an additional cost and effort for developers that is in many cases unaffordable. As a way to facilitate developers' job, this work proposes the creation of specialized tools to deal with accessibility. The hypothesis defined was that it was possible to produce tools that could reduce the input needed to adapt the games for people with special needs but achieving a good level of usability, resulting in a reduction of the cost and effort required. As game development tools and approaches are heterogeneous and diverse, two case studies were set up targeting two different platforms: a high level PC game authoring tool, and a low-level Android game programming framework. Several games were developed using the tools developed, and their usability was tested. Initial results depict that high usability levels can be achieved with a minimum additional input from the game author.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'eyes-free games, audio 3d, accessibility', 'numpages': '2', 'pages': '301–302', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Computer games are a very popular media today, spanning across multiple aspects of life, not only leisure but also health or education. But despite their importance their current level of accessibility is still low. One of the causes is that accessibility has an additional cost and effort for developers that is in many cases unaffordable. As a way to facilitate developers' job, this work proposes the creation of specialized tools to deal with accessibility. The hypothesis defined was that it was possible to produce tools that could reduce the input needed to adapt the games for people with special needs but achieving a good level of usability, resulting in a reduction of the cost and effort required. As game development tools and approaches are heterogeneous and diverse, two case studies were set up targeting two different platforms: a high level PC game authoring tool, and a low-level Android game programming framework. Several games were developed using the tools developed, and their usability was tested. Initial results depict that high usability levels can be achieved with a minimum additional input from the game author."", 'doi': '10.1145/2384916.2385004', 'url': 'https://doi.org/10.1145/2384916.2385004', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Reusable game interfaces for people with disabilities', 'author': 'Torrente, Javier', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385004'}"
Wii remote as a web navigation device for people with cerebral palsy,10.1145/2384916.2385005,"This study evaluates the use of the Nintendo Wii remote relative to the standard wireless mouse as a web navigational device. Nine participants with cerebral palsy performed three typical web tasks. Six of them showed improved task times using the Wii remote. With suitable customization available from freely available software, the Wii remote shows promise to be a flexible and inexpensive alternative.","{'series': ""ASSETS '12"", 'location': 'Boulder, Colorado, USA', 'keywords': 'wii remote, web accessibility, user study, customization', 'numpages': '2', 'pages': '303–304', 'booktitle': 'Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This study evaluates the use of the Nintendo Wii remote relative to the standard wireless mouse as a web navigational device. Nine participants with cerebral palsy performed three typical web tasks. Six of them showed improved task times using the Wii remote. With suitable customization available from freely available software, the Wii remote shows promise to be a flexible and inexpensive alternative.', 'doi': '10.1145/2384916.2385005', 'url': 'https://doi.org/10.1145/2384916.2385005', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450313216', 'year': '2012', 'title': 'Wii remote as a web navigation device for people with cerebral palsy', 'author': 'Santhanam, Nithin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/2384916.2385005'}"