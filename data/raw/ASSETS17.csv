Title,DOI,Abstract,BibTeX
Disability Access and the Federal Communications Commission,10.1145/3132525.3132526,"Biography: Karen Peltz Strauss is the Deputy Chief of the Consumer and Governmental Affairs Bureau at the Federal Communications Commission (FCC), where she helps to oversee implementation of the Commission's disability policies.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'policy, law, disability, accessibility', 'numpages': '1', 'pages': '1', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Biography: Karen Peltz Strauss is the Deputy Chief of the Consumer and Governmental Affairs Bureau at the Federal Communications Commission (FCC), where she helps to oversee implementation of the Commission's disability policies."", 'doi': '10.1145/3132525.3132526', 'url': 'https://doi.org/10.1145/3132525.3132526', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Disability Access and the Federal Communications Commission', 'author': 'Peltz Strauss, Karen', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132526'}"
Session details: Interaction Techniques and Frameworks,10.1145/3257979,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257979', 'url': 'https://doi.org/10.1145/3257979', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Interaction Techniques and Frameworks', 'author': 'Kane, Shaun', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257979'}"
Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment,10.1145/3132525.3132547,"Mobile accessibility is often a property considered at the level of a single mobile application (app), but rarely on a larger scale of the entire app ""ecosystem,"" such as all apps in an app store, their companies, developers, and user influences. We present a novel conceptual framework for the accessibility of mobile apps inspired by epidemiology. It considers apps within their ecosystems, over time, and at a population level. Under this metaphor, ""inaccessibility"" is a set of diseases that can be viewed through an epidemiological lens. Accordingly, our framework puts forth notions like risk and protective factors, prevalence, and health indicators found within a population of apps. This new framing offers terminology, motivation, and techniques to reframe how we approach and measure app accessibility. It establishes how app accessibility can benefit from multi-factor, longitudinal, and population-based analyses. Our epidemiology-inspired conceptual framework is the main contribution of this work, intended to provoke thought and inspire new work enhancing app accessibility at a systemic level. In a preliminary exercising of our framework, we perform an analysis of the prevalence of common determinants or accessibility barriers. We assess the health of a stratified sample of 100 popular Android apps using Google's Accessibility Scanner. We find that 100\% of apps have at least one of nine accessibility errors and examine which errors are most common. A preliminary analysis of the frequency of co-occurrences of multiple errors in a single app is also presented. We find 72\% of apps have five or six errors, suggesting an interaction among different errors or an underlying influence.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'mobile computing, mobile accessibility, epidemiology, conceptual framework, app accessibility, accessibility assessment', 'numpages': '10', 'pages': '2–11', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Mobile accessibility is often a property considered at the level of a single mobile application (app), but rarely on a larger scale of the entire app ""ecosystem,"" such as all apps in an app store, their companies, developers, and user influences. We present a novel conceptual framework for the accessibility of mobile apps inspired by epidemiology. It considers apps within their ecosystems, over time, and at a population level. Under this metaphor, ""inaccessibility"" is a set of diseases that can be viewed through an epidemiological lens. Accordingly, our framework puts forth notions like risk and protective factors, prevalence, and health indicators found within a population of apps. This new framing offers terminology, motivation, and techniques to reframe how we approach and measure app accessibility. It establishes how app accessibility can benefit from multi-factor, longitudinal, and population-based analyses. Our epidemiology-inspired conceptual framework is the main contribution of this work, intended to provoke thought and inspire new work enhancing app accessibility at a systemic level. In a preliminary exercising of our framework, we perform an analysis of the prevalence of common determinants or accessibility barriers. We assess the health of a stratified sample of 100 popular Android apps using Google\'s Accessibility Scanner. We find that 100\\% of apps have at least one of nine accessibility errors and examine which errors are most common. A preliminary analysis of the frequency of co-occurrences of multiple errors in a single app is also presented. We find 72\\% of apps have five or six errors, suggesting an interaction among different errors or an underlying influence.', 'doi': '10.1145/3132525.3132547', 'url': 'https://doi.org/10.1145/3132525.3132547', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Epidemiology as a Framework for Large-Scale Mobile Application Accessibility Assessment', 'author': 'Ross, Anne Spencer and Zhang, Xiaoyi and Fogarty, James and Wobbrock, Jacob O.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132547'}"
BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments,10.1145/3132525.3132528,"In this paper, we present BrailleSketch, a gesture-based text input method on touchscreen smartphones for people with visual impairments. To input a letter with BrailleSketch, a user simply sketches a gesture that passes through all dots in the corresponding Braille code for that letter. BrailleSketch allows users to place their fingers anywhere on the screen to begin a gesture and draw the Braille code in many ways. To encourage users to type faster, BrailleSketch does not provide immediate letter-level audio feedback but instead provides word-level audio feedback. It uses an auto-correction algorithm to correct typing errors. Our evaluation of the method with ten participants with visual impairments who each completed five typing sessions shows that BrailleSketch supports a text entry speed of 14.53 word per min (wpm) with 10.6\% error. Moreover, our data suggest that the speed had not begun to plateau yet by the last typing session and can continue to improve. Our evaluation also demonstrates the positive effect of the reduced audio feedback and the auto-correction algorithm.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'text input, sketch, people with visual impairments, mobile devices, gesture, braille, blind', 'numpages': '10', 'pages': '12–21', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this paper, we present BrailleSketch, a gesture-based text input method on touchscreen smartphones for people with visual impairments. To input a letter with BrailleSketch, a user simply sketches a gesture that passes through all dots in the corresponding Braille code for that letter. BrailleSketch allows users to place their fingers anywhere on the screen to begin a gesture and draw the Braille code in many ways. To encourage users to type faster, BrailleSketch does not provide immediate letter-level audio feedback but instead provides word-level audio feedback. It uses an auto-correction algorithm to correct typing errors. Our evaluation of the method with ten participants with visual impairments who each completed five typing sessions shows that BrailleSketch supports a text entry speed of 14.53 word per min (wpm) with 10.6\\% error. Moreover, our data suggest that the speed had not begun to plateau yet by the last typing session and can continue to improve. Our evaluation also demonstrates the positive effect of the reduced audio feedback and the auto-correction algorithm.', 'doi': '10.1145/3132525.3132528', 'url': 'https://doi.org/10.1145/3132525.3132528', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'BrailleSketch: A Gesture-based Text Input Method for People with Visual Impairments', 'author': 'Li, Mingzhe and Fan, Mingming and Truong, Khai N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132528'}"
Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction,10.1145/3132525.3132536,"For screenreader users who are blind or visually impaired (VI), today's mobile devices, while reasonably accessible, are not necessarily efficient. This inefficiency may be especially problematic for microinteractions, which are brief but high-frequency interactions that take only a few seconds for sighted users to complete (e.g., checking the weather or for new messages). One potential solution to support efficient non-visual microinteractions is on-body input, which appropriates the user's own body as the interaction medium. In this paper, we address two related research questions: How well are microinteractions currently supported for VI users' How should on-body interaction be designed to best support microinteractions for this user group? We conducted two studies: (1) an online survey to compare current microinteraction use between VI and sighted users (N=117); and (2) an in-person study where 12 VI screenreader users qualitatively evaluated a real-time on-body interaction system that provided three contrasting input designs. Our findings suggest that efficient microinteractions are not currently well-supported for VI users, at least using manual input, which highlights the need for new interaction approaches. On-body input offers this potential and the qualitative evaluation revealed tradeoffs with different on-body interaction techniques in terms of perceived efficiency, learnability, social acceptability, and ability to use on the go.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearable technology, visual impairments, on-body interaction, mobile, microinteraction', 'numpages': '10', 'pages': '22–31', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""For screenreader users who are blind or visually impaired (VI), today's mobile devices, while reasonably accessible, are not necessarily efficient. This inefficiency may be especially problematic for microinteractions, which are brief but high-frequency interactions that take only a few seconds for sighted users to complete (e.g., checking the weather or for new messages). One potential solution to support efficient non-visual microinteractions is on-body input, which appropriates the user's own body as the interaction medium. In this paper, we address two related research questions: How well are microinteractions currently supported for VI users' How should on-body interaction be designed to best support microinteractions for this user group? We conducted two studies: (1) an online survey to compare current microinteraction use between VI and sighted users (N=117); and (2) an in-person study where 12 VI screenreader users qualitatively evaluated a real-time on-body interaction system that provided three contrasting input designs. Our findings suggest that efficient microinteractions are not currently well-supported for VI users, at least using manual input, which highlights the need for new interaction approaches. On-body input offers this potential and the qualitative evaluation revealed tradeoffs with different on-body interaction techniques in terms of perceived efficiency, learnability, social acceptability, and ability to use on the go."", 'doi': '10.1145/3132525.3132536', 'url': 'https://doi.org/10.1145/3132525.3132536', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction', 'author': 'Oh, Uran and Stearns, Lee and Pradhan, Alisha and Froehlich, Jon E. and Findlater, Leah', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132536'}"
In-context Q&amp;A to Support Blind People Using Smartphones,10.1145/3132525.3132555,"Blind people face many barriers using smartphones. Still, previous research has been mostly restricted to non-visual gestural interaction, paying little attention to the deeper daily challenges of blind users. To bridge this gap, we conducted a series of workshops with 42 blind participants, uncovering application challenges across all levels of expertise, most of which could only be surpassed through a support network. We propose Hint Me!, a human-powered service that allows blind users to get in-app assistance by posing questions or browsing previously answered questions on a shared knowledge-base. We evaluated the perceived usefulness and acceptance of this approach with six blind people. Participants valued the ability to learn independently and anticipated a series of usages: labeling, layout and feature descriptions, bug workarounds, and learning to accomplish tasks. Creating or browsing questions depends on aspects like privacy, knowledge of respondents and response time, revealing the benefits of a hybrid approach.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'smartphone, human computation, blind, assistance', 'numpages': '5', 'pages': '32–36', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Blind people face many barriers using smartphones. Still, previous research has been mostly restricted to non-visual gestural interaction, paying little attention to the deeper daily challenges of blind users. To bridge this gap, we conducted a series of workshops with 42 blind participants, uncovering application challenges across all levels of expertise, most of which could only be surpassed through a support network. We propose Hint Me!, a human-powered service that allows blind users to get in-app assistance by posing questions or browsing previously answered questions on a shared knowledge-base. We evaluated the perceived usefulness and acceptance of this approach with six blind people. Participants valued the ability to learn independently and anticipated a series of usages: labeling, layout and feature descriptions, bug workarounds, and learning to accomplish tasks. Creating or browsing questions depends on aspects like privacy, knowledge of respondents and response time, revealing the benefits of a hybrid approach.', 'doi': '10.1145/3132525.3132555', 'url': 'https://doi.org/10.1145/3132525.3132555', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'In-context Q&amp;A to Support Blind People Using Smartphones', 'author': ""Rodrigues, Andr\\'{e} and Montague, Kyle and Nicolau, Hugo and Guerreiro, Jo\\~{a}o and Guerreiro, Tiago"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132555'}"
Improving Smartphone Accessibility with Personalizable Static Overlays,10.1145/3132525.3132558,"The physical keypads that used to dominate our mobile devices provided additional support for non-visual interaction - the keys could be recognized tactually, the interfaces were simpler and consistent. When combined with a screen reader, these devices could be easily operated by blind people. The advent of smartphones, with their rich, feature-filled applications and interfaces, have brought forward additional challenges for blind users. Apps and features are no longer developed by a single entity leading to an overwhelming variety of interfaces. We present an approach that superimposes a virtual overlay to all other interfaces ensuring interface consistency by re-structuring how content is accessed in every screen. To explore the approach, we split the screen, dedicating half to a configurable set of static options mimicking always available physical buttons regardless of context; while the other enables the standard content navigation gestures with the ability to re-order content and apply filters. In a qualitative study with nine visually impaired participants, the virtual overlays were reported as simpler to use, while still providing full-fledged usage of the system and the third party applications, and were seen as effective and useful, particularly for novice users.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'smartphone, personalizable interface, blind, accessibility', 'numpages': '5', 'pages': '37–41', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The physical keypads that used to dominate our mobile devices provided additional support for non-visual interaction - the keys could be recognized tactually, the interfaces were simpler and consistent. When combined with a screen reader, these devices could be easily operated by blind people. The advent of smartphones, with their rich, feature-filled applications and interfaces, have brought forward additional challenges for blind users. Apps and features are no longer developed by a single entity leading to an overwhelming variety of interfaces. We present an approach that superimposes a virtual overlay to all other interfaces ensuring interface consistency by re-structuring how content is accessed in every screen. To explore the approach, we split the screen, dedicating half to a configurable set of static options mimicking always available physical buttons regardless of context; while the other enables the standard content navigation gestures with the ability to re-order content and apply filters. In a qualitative study with nine visually impaired participants, the virtual overlays were reported as simpler to use, while still providing full-fledged usage of the system and the third party applications, and were seen as effective and useful, particularly for novice users.', 'doi': '10.1145/3132525.3132558', 'url': 'https://doi.org/10.1145/3132525.3132558', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Improving Smartphone Accessibility with Personalizable Static Overlays', 'author': ""Rodrigues, Andr\\'{e} and Santos, Andr\\'{e} and Montague, Kyle and Guerreiro, Tiago"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132558'}"
Session details: Technology for Neurodiverse Users,10.1145/3257980,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257980', 'url': 'https://doi.org/10.1145/3257980', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Technology for Neurodiverse Users', 'author': 'Carrington, Patrick', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257980'}"
Introducing People with ASD to Crowd Work,10.1145/3132525.3132544,"Adults with Autism Spectrum Disorders (ASD) are unemployed at a high rate, in part because the constraints and expectations of traditional employment can be difficult for them. In this paper, we report on our work in introducing people with ASD to remote work on a crowdsourcing platform and a prototype tool we developed by working with participants. We conducted a six-week long user-centered design study with three participants with ASD. The early stage of the study focused on assessing the abilities of our participants to search and work on micro-tasks available on the crowdsourcing market. Based on our preliminary findings, we designed, developed, and evaluated a prototype tool to facilitate image transcription tasks that are increasingly popular on crowd labor markets. Our findings suggest that people with ASD have varying levels of ability to work on micro-tasks, but are likely to be able to work on tasks like image transcription. The tool we introduce, Assistive Task Queue (ATQ), facilitated our participants' completion of image transcription tasks by removing ambiguity in finding the next task to work on and in simplifying tasks into discrete steps. ATQ may serve as a general platform for finding and delivering appropriate tasks to workers with autism.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'crowdsourcing, autism spectrum disorder', 'numpages': '10', 'pages': '42–51', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Adults with Autism Spectrum Disorders (ASD) are unemployed at a high rate, in part because the constraints and expectations of traditional employment can be difficult for them. In this paper, we report on our work in introducing people with ASD to remote work on a crowdsourcing platform and a prototype tool we developed by working with participants. We conducted a six-week long user-centered design study with three participants with ASD. The early stage of the study focused on assessing the abilities of our participants to search and work on micro-tasks available on the crowdsourcing market. Based on our preliminary findings, we designed, developed, and evaluated a prototype tool to facilitate image transcription tasks that are increasingly popular on crowd labor markets. Our findings suggest that people with ASD have varying levels of ability to work on micro-tasks, but are likely to be able to work on tasks like image transcription. The tool we introduce, Assistive Task Queue (ATQ), facilitated our participants' completion of image transcription tasks by removing ambiguity in finding the next task to work on and in simplifying tasks into discrete steps. ATQ may serve as a general platform for finding and delivering appropriate tasks to workers with autism."", 'doi': '10.1145/3132525.3132544', 'url': 'https://doi.org/10.1145/3132525.3132544', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Introducing People with ASD to Crowd Work', 'author': 'Hara, Kotaro and Bigham, Jeffrey P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132544'}"
Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism,10.1145/3132525.3132553,"Technologies to support children with autism tend to use predefined content to enhance specific skills, such as verbal communication or emotion recognition. Few mobilise the child's own (often very specific) interests, strengths and capabilities. Digital technologies offer opportunities for children to personalise learning with their own content, following their own interests and enabling their self-expression. This project sought to engage children to record and express their own interests within their contexts of support - the home and the classroom. The vehicle for self-expression was an audio-visual calendaring app called MeCalendar. Implementation was kept open-ended to allow teachers to use it in ways that best fit with their existing embedded practices. In this paper we report on how the prototype has been appropriated in two classrooms by teachers in an autism-specific school setting with children aged 6 to 7. Our contribution is an understanding of how technologies for self-expression led to enhanced verbal communication, positive reinforcement through video modelling, engagement in class tasks and enhanced social interaction. Children appropriated the design in unimagined ways, leading them to self-scaffold and to catalyse their confidence in social interaction and self-expression. Teachers played an integral role in appropriating the design in the classroom, specifically through their in-depth knowledge of each child and their individual needs, strengths and interests.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'classroom, child-computer interaction, autism, appropriation', 'numpages': '10', 'pages': '52–61', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Technologies to support children with autism tend to use predefined content to enhance specific skills, such as verbal communication or emotion recognition. Few mobilise the child's own (often very specific) interests, strengths and capabilities. Digital technologies offer opportunities for children to personalise learning with their own content, following their own interests and enabling their self-expression. This project sought to engage children to record and express their own interests within their contexts of support - the home and the classroom. The vehicle for self-expression was an audio-visual calendaring app called MeCalendar. Implementation was kept open-ended to allow teachers to use it in ways that best fit with their existing embedded practices. In this paper we report on how the prototype has been appropriated in two classrooms by teachers in an autism-specific school setting with children aged 6 to 7. Our contribution is an understanding of how technologies for self-expression led to enhanced verbal communication, positive reinforcement through video modelling, engagement in class tasks and enhanced social interaction. Children appropriated the design in unimagined ways, leading them to self-scaffold and to catalyse their confidence in social interaction and self-expression. Teachers played an integral role in appropriating the design in the classroom, specifically through their in-depth knowledge of each child and their individual needs, strengths and interests."", 'doi': '10.1145/3132525.3132553', 'url': 'https://doi.org/10.1145/3132525.3132553', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Digital Strategies for Supporting Strengths- and Interests-based Learning with Children with Autism', 'author': 'Wilson, Cara and Brereton, Margot and Ploderer, Bernd and Sitbon, Laurianne and Saggers, Beth', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132553'}"
CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum,10.1145/3132525.3132539,"Music Therapy has been shown to be an effective intervention for clients with Autism Spectrum Condition (ASC), a lifelong neurodevelopmental condition that can affect people in a number of ways. This paper presents a study evaluating the use of a multimodal 3D interactive tool, CymaSense, within a series of music therapy sessions. Eight adults with ASC participated in an 8-week period using a single case experimental design approach. The study used qualitative and quantitative methodological tools for analysis within and beyond the therapy sessions. The results indicate an increase in communicative behaviours for both verbal and non-verbal participants.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'music therapy, multi-sensory environments, interactive audio-visual, cymatics, autism spectrum condition (asc), assistive technologies', 'numpages': '10', 'pages': '62–71', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Music Therapy has been shown to be an effective intervention for clients with Autism Spectrum Condition (ASC), a lifelong neurodevelopmental condition that can affect people in a number of ways. This paper presents a study evaluating the use of a multimodal 3D interactive tool, CymaSense, within a series of music therapy sessions. Eight adults with ASC participated in an 8-week period using a single case experimental design approach. The study used qualitative and quantitative methodological tools for analysis within and beyond the therapy sessions. The results indicate an increase in communicative behaviours for both verbal and non-verbal participants.', 'doi': '10.1145/3132525.3132539', 'url': 'https://doi.org/10.1145/3132525.3132539', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'CymaSense: A Novel Audio-Visual Therapeutic Tool for People on the Autism Spectrum', 'author': ""McGowan, John and Lepl\\^{a}tre, Gr\\'{e}gory and McGregor, Iain"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132539'}"
Good Background Colors for Readers: A Study of People with and without Dyslexia,10.1145/3132525.3132546,"The use of colors to enhance the reading of people with dyslexia have been broadly discussed and is often recommended, but evidence of the effectiveness of this approach is lacking. This paper presents a user study with 341 participants (89 with dyslexia) that measures the effect of using background colors on screen readability. Readability was measured via reading time and distance travelled by the mouse. Comprehension was used as a control variable. The results show that using certain background colors have a significant impact on people with and without dyslexia. Warm background colors, Peach, Orange and Yellow, significantly improved reading performance over cool background colors, Blue, Blue Grey and Green. These results provide evidence to the practice of using colored backgrounds to improve readability; people with and without dyslexia benefit, but people with dyslexia may especially benefit from the practice given the difficulty they have in reading in general.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'reading speed, readability, dyslexia, background colors', 'numpages': '9', 'pages': '72–80', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The use of colors to enhance the reading of people with dyslexia have been broadly discussed and is often recommended, but evidence of the effectiveness of this approach is lacking. This paper presents a user study with 341 participants (89 with dyslexia) that measures the effect of using background colors on screen readability. Readability was measured via reading time and distance travelled by the mouse. Comprehension was used as a control variable. The results show that using certain background colors have a significant impact on people with and without dyslexia. Warm background colors, Peach, Orange and Yellow, significantly improved reading performance over cool background colors, Blue, Blue Grey and Green. These results provide evidence to the practice of using colored backgrounds to improve readability; people with and without dyslexia benefit, but people with dyslexia may especially benefit from the practice given the difficulty they have in reading in general.', 'doi': '10.1145/3132525.3132546', 'url': 'https://doi.org/10.1145/3132525.3132546', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Good Background Colors for Readers: A Study of People with and without Dyslexia', 'author': 'Rello, Luz and Bigham, Jeffrey P.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132546'}"
Session details: The Future of Work and the Web for People with VI,10.1145/3257981,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257981', 'url': 'https://doi.org/10.1145/3257981', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: The Future of Work and the Web for People with VI', 'author': 'Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257981'}"
Imagining Artificial Intelligence Applications with People with Visual Disabilities using Tactile Ideation,10.1145/3132525.3132530,"There has been a surge in artificial intelligence (AI) technologies co-opted by or designed for people with visual disabilities. Researchers and engineers have pushed technical boundaries in areas such as computer vision, natural language processing, location inference, and wearable computing. But what do people with visual disabilities imagine as their own technological future? To explore this question, we developed and carried out tactile ideation workshops with participants in the UK and India. Our participants generated a large and diverse set of ideas, most focusing on ways to meet needs related to social interaction. In some cases, this was a matter of recognizing people. In other cases, they wanted to be able to participate in social situations without foregrounding their disability. It was striking that this finding was consistent across UK and India despite substantial cultural and infrastructural differences. In this paper, we describe a new technique for working with people with visual disabilities to imagine new technologies that are tuned to their needs and aspirations. Based on our experience with these workshops, we provide a set of social dimensions to consider in the design of new AI technologies: social participation, social navigation, social maintenance, and social independence. We offer these social dimensions as a starting point to forefront users' social needs and desires as a more deliberate consideration for assistive technology design.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visually impaired, multicultural, ideation, design, blind, artificial intelligence, ai, accessibility', 'numpages': '10', 'pages': '81–90', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""There has been a surge in artificial intelligence (AI) technologies co-opted by or designed for people with visual disabilities. Researchers and engineers have pushed technical boundaries in areas such as computer vision, natural language processing, location inference, and wearable computing. But what do people with visual disabilities imagine as their own technological future? To explore this question, we developed and carried out tactile ideation workshops with participants in the UK and India. Our participants generated a large and diverse set of ideas, most focusing on ways to meet needs related to social interaction. In some cases, this was a matter of recognizing people. In other cases, they wanted to be able to participate in social situations without foregrounding their disability. It was striking that this finding was consistent across UK and India despite substantial cultural and infrastructural differences. In this paper, we describe a new technique for working with people with visual disabilities to imagine new technologies that are tuned to their needs and aspirations. Based on our experience with these workshops, we provide a set of social dimensions to consider in the design of new AI technologies: social participation, social navigation, social maintenance, and social independence. We offer these social dimensions as a starting point to forefront users' social needs and desires as a more deliberate consideration for assistive technology design."", 'doi': '10.1145/3132525.3132530', 'url': 'https://doi.org/10.1145/3132525.3132530', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Imagining Artificial Intelligence Applications with People with Visual Disabilities using Tactile Ideation', 'author': 'Morrison, Cecily and Cutrell, Edward and Dhareshwar, Anupama and Doherty, Kevin and Thieme, Anja and Taylor, Alex', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132530'}"
Interviews and Observation of Blind Software Developers at Work to Understand Code Navigation Challenges,10.1145/3132525.3132550,"Integrated Development Environments (IDEs) play an important role in the workflow of many software developers, e.g. providing syntactic highlighting or other navigation aids to support the creation of lengthy codebases. Unfortunately, such complex visual information is difficult to convey with current screen-reader technologies, thereby creating barriers for programmers who are blind, who are nevertheless using IDEs. To better understand their usage strategies and challenges, we conducted an exploratory study to investigate the issue of code navigation by developers who are blind. We observed 28 blind programmers using their preferred coding tool while they performed various programming activities, in particular while they navigated through complex codebases. Participants encountered many navigation difficulties when using their preferred coding software with assistive technologies (e.g., screen readers). During interviews, participants reported dissatisfaction with the accessibility of most IDEs due to the heavy use of visual abstractions. To compensate, participants used multiple input methods and workarounds to navigate through code comfortably and reduce complexity, but these approaches often reduced their speed and introduced mistakes, thereby reducing their efficiency as programmers. Our findings suggest an opportunity for researchers and the software industry to improve the accessibility and usability of code navigation for blind developers in IDEs.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'user studies, programming challenges, code navigation difficulties, blind programmers, accessibility', 'numpages': '10', 'pages': '91–100', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Integrated Development Environments (IDEs) play an important role in the workflow of many software developers, e.g. providing syntactic highlighting or other navigation aids to support the creation of lengthy codebases. Unfortunately, such complex visual information is difficult to convey with current screen-reader technologies, thereby creating barriers for programmers who are blind, who are nevertheless using IDEs. To better understand their usage strategies and challenges, we conducted an exploratory study to investigate the issue of code navigation by developers who are blind. We observed 28 blind programmers using their preferred coding tool while they performed various programming activities, in particular while they navigated through complex codebases. Participants encountered many navigation difficulties when using their preferred coding software with assistive technologies (e.g., screen readers). During interviews, participants reported dissatisfaction with the accessibility of most IDEs due to the heavy use of visual abstractions. To compensate, participants used multiple input methods and workarounds to navigate through code comfortably and reduce complexity, but these approaches often reduced their speed and introduced mistakes, thereby reducing their efficiency as programmers. Our findings suggest an opportunity for researchers and the software industry to improve the accessibility and usability of code navigation for blind developers in IDEs.', 'doi': '10.1145/3132525.3132550', 'url': 'https://doi.org/10.1145/3132525.3132550', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Interviews and Observation of Blind Software Developers at Work to Understand Code Navigation Challenges', 'author': 'Albusays, Khaled and Ludi, Stephanie and Huenerfauth, Matt', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132550'}"
"The Effects of ""Not Knowing What You Don't Know"" on Web Accessibility for Blind Web Users",10.1145/3132525.3132533,"Web accessibility and usability have been extensively studied for blind web users. The focus has generally been on making it technically possible for blind users to access content, or on helping to make the web more usable. This paper explores a challenge at the intersection of these two lenses, which is the effects of blind web users not knowing what they don't know. On the web, this often means that the user is having a problem completing a task, but does not know whether the problem is because the information is there and not accessible, whether the information is simply difficult to access, or whether the information is not present at all. We first discuss how this issue has manifested itself in other work in this space. We then present the results of a study with 30 sighted web users and 30 blind web users exploring the phenomenon, demonstrating that not knowing the source of a problem causes frustration and wastes time. We conclude with recommendations for future research to help understand and address this problem, as well as design implications for future technology that may assist non-visual web navigation.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'web, screen reader, blind, accessibility', 'numpages': '9', 'pages': '101–109', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Web accessibility and usability have been extensively studied for blind web users. The focus has generally been on making it technically possible for blind users to access content, or on helping to make the web more usable. This paper explores a challenge at the intersection of these two lenses, which is the effects of blind web users not knowing what they don't know. On the web, this often means that the user is having a problem completing a task, but does not know whether the problem is because the information is there and not accessible, whether the information is simply difficult to access, or whether the information is not present at all. We first discuss how this issue has manifested itself in other work in this space. We then present the results of a study with 30 sighted web users and 30 blind web users exploring the phenomenon, demonstrating that not knowing the source of a problem causes frustration and wastes time. We conclude with recommendations for future research to help understand and address this problem, as well as design implications for future technology that may assist non-visual web navigation."", 'doi': '10.1145/3132525.3132533', 'url': 'https://doi.org/10.1145/3132525.3132533', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'The Effects of ""Not Knowing What You Don\'t Know"" on Web Accessibility for Blind Web Users', 'author': 'Bigham, Jeffrey P. and Lin, Irene and Savage, Saiph', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132533'}"
Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing,10.1145/3132525.3132531,"Sighted people can browse the Web almost exclusively using a mouse. This is because web browsing mostly entails pointing and clicking on some element in the web page, and these two operations can be done almost instantaneously with a computer mouse. Unfortunately, people with vision impairments cannot use a mouse as it only provides visual feedback through a cursor. Instead, they are forced to go through a slow and tedious process of building a mental map of the web page, relying primarily on a screen reader's keyboard shortcuts and its serial audio readout of the textual content of the page, including metadata. This can often cause content and cognitive overload.This paper describes our Speed-Dial system which uses an off-the-shelf physical Dial as a surrogate for the mouse for non-visual web browsing. Speed-Dial interfaces the physical Dial with the semantic model of a web page, and provides an intuitive and rapid access to the entities and their content in the model, thereby bringing blind people's browsing experience closer to how sighted people perceive and interact with the Web. A user study with blind participants suggests that with Speed-Dial they can quickly move around the web page to select content of interest, akin to pointing and clicking with a mouse.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visual impairment, tactile interaction, tactile exploration, semantic web browsing, screen reader, microsoft surface dial', 'numpages': '10', 'pages': '110–119', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Sighted people can browse the Web almost exclusively using a mouse. This is because web browsing mostly entails pointing and clicking on some element in the web page, and these two operations can be done almost instantaneously with a computer mouse. Unfortunately, people with vision impairments cannot use a mouse as it only provides visual feedback through a cursor. Instead, they are forced to go through a slow and tedious process of building a mental map of the web page, relying primarily on a screen reader's keyboard shortcuts and its serial audio readout of the textual content of the page, including metadata. This can often cause content and cognitive overload.This paper describes our Speed-Dial system which uses an off-the-shelf physical Dial as a surrogate for the mouse for non-visual web browsing. Speed-Dial interfaces the physical Dial with the semantic model of a web page, and provides an intuitive and rapid access to the entities and their content in the model, thereby bringing blind people's browsing experience closer to how sighted people perceive and interact with the Web. A user study with blind participants suggests that with Speed-Dial they can quickly move around the web page to select content of interest, akin to pointing and clicking with a mouse."", 'doi': '10.1145/3132525.3132531', 'url': 'https://doi.org/10.1145/3132525.3132531', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Speed-Dial: A Surrogate Mouse for Non-Visual Web Browsing', 'author': 'Billah, Syed Masum and Ashok, Vikas and Porter, Donald E. and Ramakrishnan, IV', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132531'}"
Session details: Memory Impairments \&amp; Motor Impairments,10.1145/3257982,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257982', 'url': 'https://doi.org/10.1145/3257982', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Memory Impairments \\&amp; Motor Impairments', 'author': 'Wobbrock, Jacob O.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257982'}"
"""But, I Don't Want/Need a Power Wheelchair"": Toward Accessible Power Assistance for Manual Wheelchairs",10.1145/3132525.3132529,"Power assist devices help manual wheelchair users to propel their wheelchair thus increasing their independence and reducing the risk of upper limb injuries due to excessive use. These benefits can be invaluable for people that already have upper limb joint pain and reduced muscular strength. However, it is not clear if the way that assistance is provided by such devices is what manual wheelchair users need and expect. 12 manual wheelchair users were interviewed to understand: the situations in which they find it difficult to propel their wheelchairs; situations they considered paramount to have power assistance; their experience or knowledge of power assist devices; and likes and dislikes of commercially available power assist devices. Finally, they were asked to comment on their ideal form factor of a power assist device. Users have suggested improvements of the devices' accessibility and visualized new ways in which they could interact with the technology. These interactions involve ""chairable"" devices independent from, but not excluding, wearable devices and mobile applications. We have identified the need of monitoring emotions and the need for designing an open source do-it-yourself wheelchair propelling assistance device which we believe is required equally in developed and in developing countries.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'power assist device, participatory design, manual wheelchair, interviews, interaction design, human-centered, assistive technology, accessibility', 'numpages': '10', 'pages': '120–129', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Power assist devices help manual wheelchair users to propel their wheelchair thus increasing their independence and reducing the risk of upper limb injuries due to excessive use. These benefits can be invaluable for people that already have upper limb joint pain and reduced muscular strength. However, it is not clear if the way that assistance is provided by such devices is what manual wheelchair users need and expect. 12 manual wheelchair users were interviewed to understand: the situations in which they find it difficult to propel their wheelchairs; situations they considered paramount to have power assistance; their experience or knowledge of power assist devices; and likes and dislikes of commercially available power assist devices. Finally, they were asked to comment on their ideal form factor of a power assist device. Users have suggested improvements of the devices\' accessibility and visualized new ways in which they could interact with the technology. These interactions involve ""chairable"" devices independent from, but not excluding, wearable devices and mobile applications. We have identified the need of monitoring emotions and the need for designing an open source do-it-yourself wheelchair propelling assistance device which we believe is required equally in developed and in developing countries.', 'doi': '10.1145/3132525.3132529', 'url': 'https://doi.org/10.1145/3132525.3132529', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': '""But, I Don\'t Want/Need a Power Wheelchair"": Toward Accessible Power Assistance for Manual Wheelchairs', 'author': 'Morgado Ramirez, Dafne Zuleima and Holloway, Catherine', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132529'}"
Understanding Fatigue and Stamina Management Opportunities and Challenges in Wheelchair Basketball,10.1145/3132525.3132543,"Wearable fitness devices have demonstrated the capacity to improve overall fitness and athletic performance. Previous research has identified a need for these technologies to take into consideration a broader range of abilities to create more inclusive fitness communities. The adaptive sports community offers opportunities to explore technology use within a specialized community wherein physical activity is central to its identity. In this paper, we explore the current use and future potential of fitness technologies for stamina and fatigue management in wheelchair basketball, a team-based sport originally created for paraplegic athletes. We present findings from observations, seven interviews, and a survey (76 responses) with wheelchair basketball players and coaches. We present findings relating to their experience with and interest in automatic tracking of fatigue and stamina related metrics for themselves and their players.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wheelchair basketball, wearable, stamina and fatigue management, fitness, adaptive sports', 'numpages': '10', 'pages': '130–139', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Wearable fitness devices have demonstrated the capacity to improve overall fitness and athletic performance. Previous research has identified a need for these technologies to take into consideration a broader range of abilities to create more inclusive fitness communities. The adaptive sports community offers opportunities to explore technology use within a specialized community wherein physical activity is central to its identity. In this paper, we explore the current use and future potential of fitness technologies for stamina and fatigue management in wheelchair basketball, a team-based sport originally created for paraplegic athletes. We present findings from observations, seven interviews, and a survey (76 responses) with wheelchair basketball players and coaches. We present findings relating to their experience with and interest in automatic tracking of fatigue and stamina related metrics for themselves and their players.', 'doi': '10.1145/3132525.3132543', 'url': 'https://doi.org/10.1145/3132525.3132543', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Understanding Fatigue and Stamina Management Opportunities and Challenges in Wheelchair Basketball', 'author': 'Carrington, Patrick and Ketter, Denzel and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132543'}"
Narratives of Older Adults with Mild Cognitive Impairment and Their Caregivers,10.1145/3132525.3132554,"The design of assistive technology is dictated by the narratives surrounding a particular impairment and its impact on one's life. This in turn affects the perceptions of the users - both the role of technology as well as their own sense of identity. In the case of those with a mild cognitive impairment, a precursor to dementia, home-based technologies that are disability and change focused shape the identity of the very people they are to help - reifying their dependency and their loss of self. With this study, we set out to better understand the narratives of people living with a mild cognitive impairment as well as their partners that live with them and provide care. Within this investigation, we uncovered the predominance of a disease-focused narrative - one that laments loss of identity and the struggles of daily care. However, we also uncovered a different narrative centered on the role of technology to provide support within the dyads' life. These technology narratives were evidence of a need to support the biopsychosocial aspects of autonomy for both parties and improved relationships. We use our findings to further discuss the driving force behind design goals for home-based technologies for those with a mild cognitive impairment.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'home-based technology, dementia, cognitive impairment', 'numpages': '10', 'pages': '140–149', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""The design of assistive technology is dictated by the narratives surrounding a particular impairment and its impact on one's life. This in turn affects the perceptions of the users - both the role of technology as well as their own sense of identity. In the case of those with a mild cognitive impairment, a precursor to dementia, home-based technologies that are disability and change focused shape the identity of the very people they are to help - reifying their dependency and their loss of self. With this study, we set out to better understand the narratives of people living with a mild cognitive impairment as well as their partners that live with them and provide care. Within this investigation, we uncovered the predominance of a disease-focused narrative - one that laments loss of identity and the struggles of daily care. However, we also uncovered a different narrative centered on the role of technology to provide support within the dyads' life. These technology narratives were evidence of a need to support the biopsychosocial aspects of autonomy for both parties and improved relationships. We use our findings to further discuss the driving force behind design goals for home-based technologies for those with a mild cognitive impairment."", 'doi': '10.1145/3132525.3132554', 'url': 'https://doi.org/10.1145/3132525.3132554', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Narratives of Older Adults with Mild Cognitive Impairment and Their Caregivers', 'author': 'Madjaroff, Galina and Mentis, Helena', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132554'}"
AMI: An Adaptable Music Interface to Support the Varying Needs of People with Dementia,10.1145/3132525.3132557,"Dementia is a progressive, degenerative syndrome that erodes cognition, long term memory, and the ability to maintain social relationships. Anxiety is common among those with dementia, and ranges from momentary and mild, to chronic and severe. Listening to familiar music from childhood or early adulthood has been shown to provide therapeutic and positive quality of life effects for individuals with dementia, but most modern interfaces are unfamiliar and difficult to use which may add frustration and stress that music is intended to relieve. To enable individuals with dementia to control playback of music, we present AMI, a tangible music player that can be reconfigured and adapted to meet the changing needs and preferences of individuals. AMI provides a set of input components (e.g., buttons, switches, knobs) with varying physical properties which can be easily interchanged by a non-technical user (such as a caregiver). This work contributes the system design, results of user tests with the target population, as well as a set of design principles that can be used in the development of future interfaces.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tangible input, personalization, music and audio, dementia, accessibility', 'numpages': '5', 'pages': '150–154', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Dementia is a progressive, degenerative syndrome that erodes cognition, long term memory, and the ability to maintain social relationships. Anxiety is common among those with dementia, and ranges from momentary and mild, to chronic and severe. Listening to familiar music from childhood or early adulthood has been shown to provide therapeutic and positive quality of life effects for individuals with dementia, but most modern interfaces are unfamiliar and difficult to use which may add frustration and stress that music is intended to relieve. To enable individuals with dementia to control playback of music, we present AMI, a tangible music player that can be reconfigured and adapted to meet the changing needs and preferences of individuals. AMI provides a set of input components (e.g., buttons, switches, knobs) with varying physical properties which can be easily interchanged by a non-technical user (such as a caregiver). This work contributes the system design, results of user tests with the target population, as well as a set of design principles that can be used in the development of future interfaces.', 'doi': '10.1145/3132525.3132557', 'url': 'https://doi.org/10.1145/3132525.3132557', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'AMI: An Adaptable Music Interface to Support the Varying Needs of People with Dementia', 'author': 'Seymour, P. Frazer and Matejka, Justin and Foulds, Geoff and Petelycky, Ihor and Anderson, Fraser', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132557'}"
Session details: Sign Language \&amp; Captioning,10.1145/3257983,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257983', 'url': 'https://doi.org/10.1145/3257983', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Sign Language \\&amp; Captioning', 'author': 'Kushalnagar, Raja', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257983'}"
Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings,10.1145/3132525.3132541,"Recent advances in Automatic Speech Recognition (ASR) have made this technology a potential solution for transcribing audio input in real-time for people who are Deaf or Hard of Hearing (DHH). However, ASR is imperfect; users must cope with errors in the output. While some prior research has studied ASR-generated transcriptions to provide captions for DHH people, there has not been a systematic study of how to best present captions that may include errors from ASR software nor how to make use of the ASR system's word-level confidence. We conducted two studies, with 21 and 107 DHH participants, to compare various methods of visually presenting the ASR output with certainty values. Participants answered subjective preference questions and provided feedback on how ASR captioning could be used with confidence display markup. Users preferred captioning styles with which they were already most familiar (that did not display confidence information), and they were concerned about the accuracy of ASR systems. While they expressed interest in systems that display word confidence during captions, they were concerned that text appearance changes may be distracting. The findings of this study should be useful for researchers and companies developing automated captioning systems for DHH users.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'user study, real-time captions, feedback, deaf and hard of hearing, communication, automatic speech recognition', 'numpages': '10', 'pages': '155–164', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Recent advances in Automatic Speech Recognition (ASR) have made this technology a potential solution for transcribing audio input in real-time for people who are Deaf or Hard of Hearing (DHH). However, ASR is imperfect; users must cope with errors in the output. While some prior research has studied ASR-generated transcriptions to provide captions for DHH people, there has not been a systematic study of how to best present captions that may include errors from ASR software nor how to make use of the ASR system's word-level confidence. We conducted two studies, with 21 and 107 DHH participants, to compare various methods of visually presenting the ASR output with certainty values. Participants answered subjective preference questions and provided feedback on how ASR captioning could be used with confidence display markup. Users preferred captioning styles with which they were already most familiar (that did not display confidence information), and they were concerned about the accuracy of ASR systems. While they expressed interest in systems that display word confidence during captions, they were concerned that text appearance changes may be distracting. The findings of this study should be useful for researchers and companies developing automated captioning systems for DHH users."", 'doi': '10.1145/3132525.3132541', 'url': 'https://doi.org/10.1145/3132525.3132541', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings', 'author': 'Berke, Larwan and Caulfield, Christopher and Huenerfauth, Matt', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132541'}"
Evaluating the Usability of Automatically Generated Captions for People who are Deaf or Hard of Hearing,10.1145/3132525.3132542,"The accuracy of Automated Speech Recognition (ASR) technology has improved, but it is still imperfect in many settings. Researchers who evaluate ASR performance often focus on improving the Word Error Rate (WER) metric, but WER has been found to have little correlation with human-subject performance on many applications. We propose a new captioning-focused evaluation metric that better predicts the impact of ASR recognition errors on the usability of automatically generated captions for people who are Deaf or Hard of Hearing (DHH). Through a user study with 30 DHH users, we compared our new metric with the traditional WER metric on a caption usability evaluation task. In a side-by-side comparison of pairs of ASR text output (with identical WER), the texts preferred by our new metric were preferred by DHH participants. Further, our metric had significantly higher correlation with DHH participants' subjective scores on the usability of a caption, as compared to the correlation between WER metric and participant subjective scores. This new metric could be used to select ASR systems for captioning applications, and it may be a better metric for ASR researchers to consider when optimizing ASR systems.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'real-time captioning system, caption usability evaluation, automatic speech recognition, accessibility for people who are deaf or hard-of-hearing', 'numpages': '10', 'pages': '165–174', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""The accuracy of Automated Speech Recognition (ASR) technology has improved, but it is still imperfect in many settings. Researchers who evaluate ASR performance often focus on improving the Word Error Rate (WER) metric, but WER has been found to have little correlation with human-subject performance on many applications. We propose a new captioning-focused evaluation metric that better predicts the impact of ASR recognition errors on the usability of automatically generated captions for people who are Deaf or Hard of Hearing (DHH). Through a user study with 30 DHH users, we compared our new metric with the traditional WER metric on a caption usability evaluation task. In a side-by-side comparison of pairs of ASR text output (with identical WER), the texts preferred by our new metric were preferred by DHH participants. Further, our metric had significantly higher correlation with DHH participants' subjective scores on the usability of a caption, as compared to the correlation between WER metric and participant subjective scores. This new metric could be used to select ASR systems for captioning applications, and it may be a better metric for ASR researchers to consider when optimizing ASR systems."", 'doi': '10.1145/3132525.3132542', 'url': 'https://doi.org/10.1145/3132525.3132542', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Evaluating the Usability of Automatically Generated Captions for People who are Deaf or Hard of Hearing', 'author': 'Kafle, Sushant and Huenerfauth, Matt', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132542'}"
Design and Psychometric Evaluation of an American Sign Language Translation of the System Usability Scale,10.1145/3132525.3132540,"In usability studies, designers and researchers frequently use subjective questions to evaluate participants' impression of the usability of some product. The System Usability Scale (SUS) is a popular standardized questionnaire consisting of ten English statements about the usability of a product, to which participants indicate their agreement on a five-point scale. Many deaf adults in the U.S. have lower levels of English reading literacy, but there are currently no standardized questionnaires similar to SUS for Deaf and Hard-of-Hearing (DHH) users who are fluent in American Sign Language (ASL). To facilitate the inclusion of such users in studies, we created an ASL translation of SUS following accepted methods of survey translation: using a bilingual team including native ASL signers who are members of the Deaf community, along with back-translation evaluation to determine whether the meaning of the original was preserved. To validate whether key psychometric properties were preserved during translation, we deployed the ASL instrument in a study with 30 DHH participants. By comparing the results to users? responses to another measurement instrument, along with scores from 10 additional DHH participants responding to the original English SUS, we verified the criterion validity and internal reliability of the new ""ASL-SUS."" We are disseminating the translated instrument to promote the inclusion of DHH users in HCI research studies or in usability testing of consumer products.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'translation, system usability scale, sus, internal reliability, criterion validity, asl-sus, american sign language', 'numpages': '10', 'pages': '175–184', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In usability studies, designers and researchers frequently use subjective questions to evaluate participants\' impression of the usability of some product. The System Usability Scale (SUS) is a popular standardized questionnaire consisting of ten English statements about the usability of a product, to which participants indicate their agreement on a five-point scale. Many deaf adults in the U.S. have lower levels of English reading literacy, but there are currently no standardized questionnaires similar to SUS for Deaf and Hard-of-Hearing (DHH) users who are fluent in American Sign Language (ASL). To facilitate the inclusion of such users in studies, we created an ASL translation of SUS following accepted methods of survey translation: using a bilingual team including native ASL signers who are members of the Deaf community, along with back-translation evaluation to determine whether the meaning of the original was preserved. To validate whether key psychometric properties were preserved during translation, we deployed the ASL instrument in a study with 30 DHH participants. By comparing the results to users? responses to another measurement instrument, along with scores from 10 additional DHH participants responding to the original English SUS, we verified the criterion validity and internal reliability of the new ""ASL-SUS."" We are disseminating the translated instrument to promote the inclusion of DHH users in HCI research studies or in usability testing of consumer products.', 'doi': '10.1145/3132525.3132540', 'url': 'https://doi.org/10.1145/3132525.3132540', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Design and Psychometric Evaluation of an American Sign Language Translation of the System Usability Scale', 'author': 'Huenerfauth, Matt and Patel, Kasmira and Berke, Larwan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132540'}"
Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites,10.1145/3132525.3132559,"Sign language is the primary medium of communication for many people who are deaf or hard of hearing. Members of this community access online sign language (SL) content posted on video sharing sites to stay informed. Unfortunately, locating SL videos can be difficult since the text-based search on video sharing sites is based on metadata rather than on the video content. Low cost or real-time video classification techniques would be invaluable for improving access to this content. Our prior work developed a technique to identify SL content based on video features alone but is computationally expensive. Here we describe and evaluate three optimization strategies that have the potential to reduce the computation time without overly impacting precision and recall. Two optimizations reduce the cost of face-detection, whereas the third focuses on analyzing shorter segments of the video. Our results identify a combination of these techniques that yields a 96\% reduction in computation time while losing only 1\% in F1 score. To further reduce computation, we additionally explore a keyframe-based approach that achieves comparable recall but lower precision than the above techniques, making it appropriate as an early filter in a staged classifier.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'signal processing, sign language detection, computer vision', 'numpages': '5', 'pages': '185–189', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Sign language is the primary medium of communication for many people who are deaf or hard of hearing. Members of this community access online sign language (SL) content posted on video sharing sites to stay informed. Unfortunately, locating SL videos can be difficult since the text-based search on video sharing sites is based on metadata rather than on the video content. Low cost or real-time video classification techniques would be invaluable for improving access to this content. Our prior work developed a technique to identify SL content based on video features alone but is computationally expensive. Here we describe and evaluate three optimization strategies that have the potential to reduce the computation time without overly impacting precision and recall. Two optimizations reduce the cost of face-detection, whereas the third focuses on analyzing shorter segments of the video. Our results identify a combination of these techniques that yields a 96\\% reduction in computation time while losing only 1\\% in F1 score. To further reduce computation, we additionally explore a keyframe-based approach that achieves comparable recall but lower precision than the above techniques, making it appropriate as an early filter in a staged classifier.', 'doi': '10.1145/3132525.3132559', 'url': 'https://doi.org/10.1145/3132525.3132559', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing Sites', 'author': 'Shipman, Frank M. and Duggina, Satyakiran and Monteiro, Caio D.D. and Gutierrez-Osuna, Ricardo', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132559'}"
"Session details: Tactile, Haptic, and Augmented Reality Interfaces for Vision Loss",10.1145/3257984,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257984', 'url': 'https://doi.org/10.1145/3257984', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Tactile, Haptic, and Augmented Reality Interfaces for Vision Loss', 'author': 'Guerreiro, Tiago', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257984'}"
FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers,10.1145/3132525.3132548,"For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visual impairment, tangible interfaces, interactive tactile graphics, dynamic tactile markers', 'numpages': '10', 'pages': '190–199', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.', 'doi': '10.1145/3132525.3132548', 'url': 'https://doi.org/10.1145/3132525.3132548', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers', 'author': 'Suzuki, Ryo and Stangl, Abigale and Gross, Mark D. and Yeh, Tom', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132548'}"
Designing Interactions for 3D Printed Models with Blind People,10.1145/3132525.3132549,"Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visually impairments, interactive 3d printed models, exploration behaviors, elicitation', 'numpages': '10', 'pages': '200–209', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms."", 'doi': '10.1145/3132525.3132549', 'url': 'https://doi.org/10.1145/3132525.3132549', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Designing Interactions for 3D Printed Models with Blind People', 'author': 'Shi, Lei and Zhao, Yuhang and Azenkot, Shiri', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132549'}"
Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface,10.1145/3132525.3132538,"Precisely guiding a blind person's hand can be useful for a range of applications from tracing printed text to learning and understanding shapes and gestures. In this paper, we evaluate wrist-worn haptics as a directional hand guide. We implemented and evaluated the following haptic wristband variations: (1) four versus eight vibromotor designs; (2) vibration from only a single motor at a time versus from two adjacent motors using interpolation. To evaluate our designs, we conducted two studies: Study 1 (N=13, 2 blind) showed that participants could non-visually find targets and trace paths more quickly and accurately with single-motor feedback than with interpolated feedback, particularly when only four motors were used. Study 2 (N=14 blind or visually impaired participants) found that single-motor feedback with four motors was faster, more accurate, and most preferred compared to similar feedback with eight motors. We derive implications for the design of wrist-worn directional haptic feedback and discuss future work.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearable computing, haptic wristband, haptic feedback, blind user, accessibility', 'numpages': '10', 'pages': '210–219', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Precisely guiding a blind person's hand can be useful for a range of applications from tracing printed text to learning and understanding shapes and gestures. In this paper, we evaluate wrist-worn haptics as a directional hand guide. We implemented and evaluated the following haptic wristband variations: (1) four versus eight vibromotor designs; (2) vibration from only a single motor at a time versus from two adjacent motors using interpolation. To evaluate our designs, we conducted two studies: Study 1 (N=13, 2 blind) showed that participants could non-visually find targets and trace paths more quickly and accurately with single-motor feedback than with interpolated feedback, particularly when only four motors were used. Study 2 (N=14 blind or visually impaired participants) found that single-motor feedback with four motors was faster, more accurate, and most preferred compared to similar feedback with eight motors. We derive implications for the design of wrist-worn directional haptic feedback and discuss future work."", 'doi': '10.1145/3132525.3132538', 'url': 'https://doi.org/10.1145/3132525.3132538', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Evaluating Wrist-Based Haptic Feedback for Non-Visual Target Finding and Path Tracing on a 2D Surface', 'author': 'Hong, Jonggi and Pradhan, Alisha and Froehlich, Jon E. and Findlater, Leah', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132538'}"
Technology-Mediated Sight: A Case Study of Early Adopters of a Low Vision Assistive Technology,10.1145/3132525.3132552,"A case study of early adopters of a head-mounted assistive device for low vision provides the basis for a sociotechnical analysis of technology-mediated sight. Our research complements recent work in HCI focused on designing, building, and evaluating the performance of assistive devices for low vision by highlighting psychosocial and adaptive aspects of digitally enhanced vision. Through a series of semi-structured interviews with users of the eSight 2.0 device and customer-facing employees of the eSight company, we sought to better understand the social and emotional impacts associated with adoption of this type of low-vision assistive technology. Four analytic themes emerged from our interviews: 1) assessing the value of assistive technology in real life, 2) negotiating social engagement, 3) boundaries of sight, and 4) attitudes toward and expectations of technology. We introduce the concept of multiplicities of vision to describe technology-mediated sight as being a form of skilled vision and neither fully-human nor fully-digital, but rather, assembled through a combination of social and technical affordances. We propose that instead of seeing low-vision users through a deficit model of sight, HCI designers have more to gain by viewing people with low vision as individuals with a distinct type of skilled vision that is both socially and technologically mediated.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'qualitative research, low vision, head-mounted systems, assistive technology', 'numpages': '10', 'pages': '220–229', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'A case study of early adopters of a head-mounted assistive device for low vision provides the basis for a sociotechnical analysis of technology-mediated sight. Our research complements recent work in HCI focused on designing, building, and evaluating the performance of assistive devices for low vision by highlighting psychosocial and adaptive aspects of digitally enhanced vision. Through a series of semi-structured interviews with users of the eSight 2.0 device and customer-facing employees of the eSight company, we sought to better understand the social and emotional impacts associated with adoption of this type of low-vision assistive technology. Four analytic themes emerged from our interviews: 1) assessing the value of assistive technology in real life, 2) negotiating social engagement, 3) boundaries of sight, and 4) attitudes toward and expectations of technology. We introduce the concept of multiplicities of vision to describe technology-mediated sight as being a form of skilled vision and neither fully-human nor fully-digital, but rather, assembled through a combination of social and technical affordances. We propose that instead of seeing low-vision users through a deficit model of sight, HCI designers have more to gain by viewing people with low vision as individuals with a distinct type of skilled vision that is both socially and technologically mediated.', 'doi': '10.1145/3132525.3132552', 'url': 'https://doi.org/10.1145/3132525.3132552', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Technology-Mediated Sight: A Case Study of Early Adopters of a Low Vision Assistive Technology', 'author': 'Zolyomi, Annuska and Shukla, Anushree and Snyder, Jaime', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132552'}"
Session details: Supporting Communication,10.1145/3257985,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257985', 'url': 'https://doi.org/10.1145/3257985', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Supporting Communication', 'author': 'Brewer, Robin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257985'}"
Development and Theoretical Evaluation of Optimized Phonemic Interfaces,10.1145/3132525.3132537,"In this paper, optimized communication interfaces in which users select phonemes (sounds) instead of letters or whole words are presented and evaluated. Optimization is based on phoneme transition likelihoods (i.e., the probability of transitioning from one phoneme to another in a particular communication corpus), similar to letter-to-letter transition likelihoods used to optimize orthographic interfaces. However, it is unknown to what extent phoneme transition likelihoods vary by corpus, nor how optimizing based on different corpora affects the final interface efficiency. Here we used computational evaluations to compare phoneme transition likelihoods between various phonemic corpora and optimize phonemic interfaces with each corpus. Each interface's efficiency was evaluated against all the corpora. Phoneme-to-phoneme transitions were highly correlated across corpora (r = 0.7-0.86). Optimization based on phoneme-to-phoneme transition likelihoods improved efficiency by around 20-30\% compared to random phonemic layouts, regardless of the corpus used to optimize the interface. Optimizations using different corpora were similar, varying only by 3-5\%. We conclude that, if possible, future phonemic interfaces should be optimized via a corpus from the intended user's communication. If this is not possible, however, optimization still improved efficiency using all testing corpora, suggesting that optimizing via any relevant corpus is indicated over other layouts.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': ""phonemic interface, minimal movement capabilities, fitts' law, corpus-based optimizations, augmentative and alternative communication (aac)"", 'numpages': '10', 'pages': '230–239', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""In this paper, optimized communication interfaces in which users select phonemes (sounds) instead of letters or whole words are presented and evaluated. Optimization is based on phoneme transition likelihoods (i.e., the probability of transitioning from one phoneme to another in a particular communication corpus), similar to letter-to-letter transition likelihoods used to optimize orthographic interfaces. However, it is unknown to what extent phoneme transition likelihoods vary by corpus, nor how optimizing based on different corpora affects the final interface efficiency. Here we used computational evaluations to compare phoneme transition likelihoods between various phonemic corpora and optimize phonemic interfaces with each corpus. Each interface's efficiency was evaluated against all the corpora. Phoneme-to-phoneme transitions were highly correlated across corpora (r = 0.7-0.86). Optimization based on phoneme-to-phoneme transition likelihoods improved efficiency by around 20-30\\% compared to random phonemic layouts, regardless of the corpus used to optimize the interface. Optimizations using different corpora were similar, varying only by 3-5\\%. We conclude that, if possible, future phonemic interfaces should be optimized via a corpus from the intended user's communication. If this is not possible, however, optimization still improved efficiency using all testing corpora, suggesting that optimizing via any relevant corpus is indicated over other layouts."", 'doi': '10.1145/3132525.3132537', 'url': 'https://doi.org/10.1145/3132525.3132537', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Development and Theoretical Evaluation of Optimized Phonemic Interfaces', 'author': 'Cler, Gabriel J. and Stepp, Cara E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132537'}"
Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior,10.1145/3132525.3132551,"Overselectivity is a learning challenge that is largely unaddressed in the assistive technology community. Screening and intervention, done by specialists, is time-intensive and requires substantial training. Little to no treatments are available to the broader population of preliterate, minimally verbal individuals. In this work, we examine the impact of an iPad game based on the tenets of behavioral therapy to mitigate overselectivity. We developed software-based techniques and evaluated the system using established methods from the field of Special Education. We present the results of a deployment in a special education school that demonstrates that an assistive tablet game is a feasible means of addressing overselectivity, and we present generalizable technological features drawn from evidenced-based therapies to consider in future assistive technologies. We suggest that designers of assistive technology systems, particularly those who address physical, cognitive, and behavioral difficulties for preliterate AAC users, should consider overselectivity as a potential co-occurring condition.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tablet games, overselectivity, multiple cue responding, language development, children, autism, assistive technology, aac', 'numpages': '10', 'pages': '240–249', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Overselectivity is a learning challenge that is largely unaddressed in the assistive technology community. Screening and intervention, done by specialists, is time-intensive and requires substantial training. Little to no treatments are available to the broader population of preliterate, minimally verbal individuals. In this work, we examine the impact of an iPad game based on the tenets of behavioral therapy to mitigate overselectivity. We developed software-based techniques and evaluated the system using established methods from the field of Special Education. We present the results of a deployment in a special education school that demonstrates that an assistive tablet game is a feasible means of addressing overselectivity, and we present generalizable technological features drawn from evidenced-based therapies to consider in future assistive technologies. We suggest that designers of assistive technology systems, particularly those who address physical, cognitive, and behavioral difficulties for preliterate AAC users, should consider overselectivity as a potential co-occurring condition.', 'doi': '10.1145/3132525.3132551', 'url': 'https://doi.org/10.1145/3132525.3132551', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Evaluating an iPad Game to Address Overselectivity in Preliterate AAC Users with Minimal Verbal Behavior', 'author': 'Boyd, LouAnne E. and Ringland, Kathryn E. and Faucett, Heather and Hiniker, Alexis and Klein, Kimberley and Patel, Kanika and Hayes, Gillian R.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132551'}"
Using Participatory Design with Proxies with Children with Limited Communication,10.1145/3132525.3132527,"Including children with communication disorders in the participatory design and evaluation of digital technologies is challenging, as communication between designers and users - an important part of this approach - can be impacted. Using Participatory Design with Proxies (PDwP) supports the inclusion of input from different stakeholders, such as parents, teachers and Speech-Language Pathologists (SLPs), who can provide valuable insight and feedback to augment direct input from users, which may be severely limited. We describe how we used PDwP to design a digital living media system that motivates children with disabilities to use digital therapeutic and learning applications and supports communication and collaboration between users. We employed an iterative design process to fabricate three functional prototypes and used them as design probes in participants' home and school settings. We present lessons learned in the form of the strengths and shortcomings of using PDwP for designing systems for children with limited communication abilities.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'participatory design with proxies, digital living media systems, children with disabilities', 'numpages': '10', 'pages': '250–259', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Including children with communication disorders in the participatory design and evaluation of digital technologies is challenging, as communication between designers and users - an important part of this approach - can be impacted. Using Participatory Design with Proxies (PDwP) supports the inclusion of input from different stakeholders, such as parents, teachers and Speech-Language Pathologists (SLPs), who can provide valuable insight and feedback to augment direct input from users, which may be severely limited. We describe how we used PDwP to design a digital living media system that motivates children with disabilities to use digital therapeutic and learning applications and supports communication and collaboration between users. We employed an iterative design process to fabricate three functional prototypes and used them as design probes in participants' home and school settings. We present lessons learned in the form of the strengths and shortcomings of using PDwP for designing systems for children with limited communication abilities."", 'doi': '10.1145/3132525.3132527', 'url': 'https://doi.org/10.1145/3132525.3132527', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Using Participatory Design with Proxies with Children with Limited Communication', 'author': ""Hamidi, Foad and Baljko, Melanie and G\\'{o}mez, Isabel"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132527'}"
Session details: Navigation \&amp; Safety,10.1145/3257986,Abstract not available,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'doi': '10.1145/3257986', 'url': 'https://doi.org/10.1145/3257986', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Session details: Navigation \\&amp; Safety', 'author': 'Flatla, David', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3257986'}"
"""Is Someone There? Do They Have a Gun"": How Visual Information about Others Can Improve Personal Safety Management for Blind Individuals",10.1145/3132525.3132534,"For decades, researchers have investigated and developed technologies that support independent navigation for people who are blind. This has led to systems that primarily aid in detecting routes, landmarks, and building features. However, there has been relatively little inquiry regarding how technologies might support navigation around and in the presence of other people. What visual information, if any, do blind navigators wish they had about people on their path? To address this question, we surveyed 58 blind and low vision individuals and interviewed 10 blind individuals. We discovered our participants were interested in using visual information about others to increase their physical safety. For example, they wanted to know if a passerby was holding a weapon, if a presumed official had a proper uniform or badge, and how to describe visual aspects of a criminal to law enforcement. This paper presents one of the only reports documenting accessibility challenges related to physical safety posed by others, including how future assistive tools can empower individuals with disabilities to more actively increase their sense of safety. We call this emerging area Personal Safety Management and contribute a set of four broad subareas that deserve further exploration by researchers and designers working within the blind and broader disabilities communities.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visual impairment, violence, police, personal safety management, facial recognition, disability, crime, blind, assistive technology', 'numpages': '10', 'pages': '260–269', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'For decades, researchers have investigated and developed technologies that support independent navigation for people who are blind. This has led to systems that primarily aid in detecting routes, landmarks, and building features. However, there has been relatively little inquiry regarding how technologies might support navigation around and in the presence of other people. What visual information, if any, do blind navigators wish they had about people on their path? To address this question, we surveyed 58 blind and low vision individuals and interviewed 10 blind individuals. We discovered our participants were interested in using visual information about others to increase their physical safety. For example, they wanted to know if a passerby was holding a weapon, if a presumed official had a proper uniform or badge, and how to describe visual aspects of a criminal to law enforcement. This paper presents one of the only reports documenting accessibility challenges related to physical safety posed by others, including how future assistive tools can empower individuals with disabilities to more actively increase their sense of safety. We call this emerging area Personal Safety Management and contribute a set of four broad subareas that deserve further exploration by researchers and designers working within the blind and broader disabilities communities.', 'doi': '10.1145/3132525.3132534', 'url': 'https://doi.org/10.1145/3132525.3132534', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': '""Is Someone There? Do They Have a Gun"": How Visual Information about Others Can Improve Personal Safety Management for Blind Individuals', 'author': 'Branham, Stacy M. and Abdolrahmani, Ali and Easley, William and Scheuerman, Morgan and Ronquillo, Erick and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132534'}"
NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment,10.1145/3132525.3132535,"Navigating in unfamiliar environments is challenging for most people, especially for individuals with visual impairments. While many personal navigation tools have been proposed to enable in- dependent indoor navigation, they have insufficient accuracy (e.g., 5-10 m), do not provide semantic features about surroundings (e.g., doorways, shops, etc.), and may require specialized devices to function. Moreover, the deployment of many systems is often only evaluated in constrained scenarios, which may not precisely reflect the performance in the real world. Therefore, we have de- signed and implemented NavCog3, a smartphone-based indoor navigation assistant that has been evaluated in a 21,000 m2 shop- ping mall. In addition to turn-by-turn instructions, it provides in- formation on landmarks (e.g., tactile paving) and points of interests nearby. We first conducted a controlled study with 10 visually im- paired users to assess localization accuracy and the perceived use- fulness of semantic features. To understand the usability of the app in a real-world setting, we then conducted another study with 43 participants with visual impairments where they could freely nav- igate in the shopping mall using NavCog3. Our findings suggest that NavCog3 can open a new opportunity for users with visual im- pairments to independently find and visit large and complex places with confidence.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'voice- based interaction, visual impairments, user evaluation, points of interest, indoor navigation', 'numpages': '10', 'pages': '270–279', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Navigating in unfamiliar environments is challenging for most people, especially for individuals with visual impairments. While many personal navigation tools have been proposed to enable in- dependent indoor navigation, they have insufficient accuracy (e.g., 5-10 m), do not provide semantic features about surroundings (e.g., doorways, shops, etc.), and may require specialized devices to function. Moreover, the deployment of many systems is often only evaluated in constrained scenarios, which may not precisely reflect the performance in the real world. Therefore, we have de- signed and implemented NavCog3, a smartphone-based indoor navigation assistant that has been evaluated in a 21,000 m2 shop- ping mall. In addition to turn-by-turn instructions, it provides in- formation on landmarks (e.g., tactile paving) and points of interests nearby. We first conducted a controlled study with 10 visually im- paired users to assess localization accuracy and the perceived use- fulness of semantic features. To understand the usability of the app in a real-world setting, we then conducted another study with 43 participants with visual impairments where they could freely nav- igate in the shopping mall using NavCog3. Our findings suggest that NavCog3 can open a new opportunity for users with visual im- pairments to independently find and visit large and complex places with confidence.', 'doi': '10.1145/3132525.3132535', 'url': 'https://doi.org/10.1145/3132525.3132535', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment', 'author': 'Sato, Daisuke and Oh, Uran and Naito, Kakuya and Takagi, Hironobu and Kitani, Kris and Asakawa, Chieko', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132535'}"
Virtual Navigation for Blind People: Building Sequential Representations of the Real-World,10.1145/3132525.3132545,"When preparing to visit new locations, sighted people often look at maps to build an a priori mental representation of the environment as a sequence of step-by-step actions and points of interest (POIs), e.g., turn right after the coffee shop. Based on this observation, we would like to understand if building the same type of sequential representation, prior to navigating in a new location, is helpful for people with visual impairments (VI). In particular, our goal is to understand how the simultaneous interplay between turn-by-turn navigation instructions and the relevant POIs in the route can aid the creation of a memorable sequential representation of the world. To this end, we present two smartphone-based virtual navigation interfaces: VirtualLeap, which allows the user to jump through a sequence of street intersection labels, turn-by-turn instructions and POIs along the route; and VirtualWalk, which simulates variable speed step-by-step walking using audio effects, whilst conveying similar route information. In a user study with 14 VI participants, most were able to create and maintain an accurate mental representation of both the sequential structure of the route and the approximate locations of the POIs. While both virtual navigation modalities resulted in similar spatial understanding, results suggests that each method is useful in different interaction contexts.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'virtual navigation, orientation and mobility, cognitive mapping, blind navigation, assistive technology', 'numpages': '10', 'pages': '280–289', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'When preparing to visit new locations, sighted people often look at maps to build an a priori mental representation of the environment as a sequence of step-by-step actions and points of interest (POIs), e.g., turn right after the coffee shop. Based on this observation, we would like to understand if building the same type of sequential representation, prior to navigating in a new location, is helpful for people with visual impairments (VI). In particular, our goal is to understand how the simultaneous interplay between turn-by-turn navigation instructions and the relevant POIs in the route can aid the creation of a memorable sequential representation of the world. To this end, we present two smartphone-based virtual navigation interfaces: VirtualLeap, which allows the user to jump through a sequence of street intersection labels, turn-by-turn instructions and POIs along the route; and VirtualWalk, which simulates variable speed step-by-step walking using audio effects, whilst conveying similar route information. In a user study with 14 VI participants, most were able to create and maintain an accurate mental representation of both the sequential structure of the route and the approximate locations of the POIs. While both virtual navigation modalities resulted in similar spatial understanding, results suggests that each method is useful in different interaction contexts.', 'doi': '10.1145/3132525.3132545', 'url': 'https://doi.org/10.1145/3132525.3132545', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Virtual Navigation for Blind People: Building Sequential Representations of the Real-World', 'author': 'Guerreiro, Jo\\~{a}o and Ahmetovic, Dragan and Kitani, Kris M. and Asakawa, Chieko', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132545'}"
Opinions and Preferences of Blind and Low Vision Consumers Regarding Self-Driving Vehicles: Results of Focus Group Discussions,10.1145/3132525.3132532,"Fully autonomous vehicles, commonly referred to as self-driving vehicles, are an emerging technology that may hold tremendous mobility potential for individuals who are blind or visually impaired who have been previously disadvantaged by an inability to operate conventional motor vehicles. This study explores the opinions of 38 participants who are blind and low vision, through the use of focus group methodology, regarding this emerging self-driving vehicle technology. Participants were overwhelmingly optimistic about the potential for independence and mobility that self-driving vehicles may provide but were concerned that the needs of individuals with visual impairments were not being adequately considered in the development of the technology. Participants also raised questions about how the technology would satisfy their need for situational awareness, how the technology would enable blind or visually impaired operators to verify their arrival at their desired location and a host of issues related to parking, vehicle location and roadside assistance. Participants also expressed a preference for smartphone and speech input capabilities as a primary means of system interaction. These findings suggest that at a minimum more needs to be done to engage individuals with visual impairments in the development of self-driving vehicle technology and to increase awareness of manufacturer efforts.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'self-driving vehicles, low vision, blindness, advanced driver assistance systems, accessibility', 'numpages': '10', 'pages': '290–299', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Fully autonomous vehicles, commonly referred to as self-driving vehicles, are an emerging technology that may hold tremendous mobility potential for individuals who are blind or visually impaired who have been previously disadvantaged by an inability to operate conventional motor vehicles. This study explores the opinions of 38 participants who are blind and low vision, through the use of focus group methodology, regarding this emerging self-driving vehicle technology. Participants were overwhelmingly optimistic about the potential for independence and mobility that self-driving vehicles may provide but were concerned that the needs of individuals with visual impairments were not being adequately considered in the development of the technology. Participants also raised questions about how the technology would satisfy their need for situational awareness, how the technology would enable blind or visually impaired operators to verify their arrival at their desired location and a host of issues related to parking, vehicle location and roadside assistance. Participants also expressed a preference for smartphone and speech input capabilities as a primary means of system interaction. These findings suggest that at a minimum more needs to be done to engage individuals with visual impairments in the development of self-driving vehicle technology and to increase awareness of manufacturer efforts.', 'doi': '10.1145/3132525.3132532', 'url': 'https://doi.org/10.1145/3132525.3132532', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Opinions and Preferences of Blind and Low Vision Consumers Regarding Self-Driving Vehicles: Results of Focus Group Discussions', 'author': 'Brinkley, Julian and Posadas, Brianna and Woodward, Julia and Gilbert, Juan E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132532'}"
DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers,10.1145/3132525.3132556,"Although a large number of navigation support systems for visually impaired people have been proposed in the past, navigating through unknown environments is still a major challenge for visually impaired travelers. Existing systems provide navigation information through headphones, speakers or tactile actuators. In this paper, we propose to use small lightweight quadcopters instead to provide navigation information for people with visual impairments. Using a leashed or free-floating quadcopter, the user is navigated by the distinct sound that the quadcopter emits and a haptic stimulus provided by the leash. In a user with 14 visually impaired participants, we compared leashed quadcopter navigation, free-floating quadcopter navigation, and traditional audio navigation. The results show that compared to audio navigation, participants navigate significantly faster with a free-floating quadcopter and make fewer navigation errors using the quadcopter navigation methods.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visual impairments, quadcopter, navigation aid, drones', 'numpages': '5', 'pages': '300–304', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Although a large number of navigation support systems for visually impaired people have been proposed in the past, navigating through unknown environments is still a major challenge for visually impaired travelers. Existing systems provide navigation information through headphones, speakers or tactile actuators. In this paper, we propose to use small lightweight quadcopters instead to provide navigation information for people with visual impairments. Using a leashed or free-floating quadcopter, the user is navigated by the distinct sound that the quadcopter emits and a haptic stimulus provided by the leash. In a user with 14 visually impaired participants, we compared leashed quadcopter navigation, free-floating quadcopter navigation, and traditional audio navigation. The results show that compared to audio navigation, participants navigate significantly faster with a free-floating quadcopter and make fewer navigation errors using the quadcopter navigation methods.', 'doi': '10.1145/3132525.3132556', 'url': 'https://doi.org/10.1145/3132525.3132556', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'DroneNavigator: Using Leashed and Free-Floating Quadcopters to Navigate Visually Impaired Travelers', 'author': 'Avila Soto, Mauro and Funk, Markus and Hoppe, Matthias and Boldt, Robin and Wolf, Katrin and Henze, Niels', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3132556'}"
A Pilot Deployment of an Online Tool for Large-Scale Virtual Auditing of Urban Accessibility,10.1145/3132525.3134775,"We present Project Sidewalk, a new online tool that allows anyone-from motivated citizens to government workers-to remotely label accessibility problems by virtually walking through city streets. Basic game design principles such as interactive onboarding, mission-based tasks, and stats dashboards are used to train, engage, and sustain users. We describe the current Project Sidewalk system, present results of a pilot public deployment with 581 users, and discuss open questions and future work.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'urban accessibility, mobility impaired users, gis, crowdsourcing', 'numpages': '2', 'pages': '305–306', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present Project Sidewalk, a new online tool that allows anyone-from motivated citizens to government workers-to remotely label accessibility problems by virtually walking through city streets. Basic game design principles such as interactive onboarding, mission-based tasks, and stats dashboards are used to train, engage, and sustain users. We describe the current Project Sidewalk system, present results of a pilot public deployment with 581 users, and discuss open questions and future work.', 'doi': '10.1145/3132525.3134775', 'url': 'https://doi.org/10.1145/3132525.3134775', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'A Pilot Deployment of an Online Tool for Large-Scale Virtual Auditing of Urban Accessibility', 'author': 'Saha, Manaswi and Hara, Kotaro and Behnezhad, Soheil and Li, Anthony and Saugstad, Michael and Maddali, Hanuma and Chen, Sage and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134775'}"
ActVirtual: Making Public Activism Accessible,10.1145/3132525.3134815,"Technology-mediated public activism has grown popular in recent years with the high uptake of social media. Facebook and Twitter have become venues for activists to participate in online activism, or organize offline activism events. However, due to accessibility barriers in physical environments and accessibility issues in social media, people with disabilities continue to face challenges when they engage with such social movements. We interviewed 22 disabled activists about how they used technology to mediate civic engagement and barriers they faced. We present preliminary findings from these interviews and describe a potential solution named ActVirtual, a mobile platform for accessible activism. Our future work will include implementing and testing ActVirtual with users to make online and offline activism more accessible.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'public protests, accessible technology, accessible activism', 'numpages': '2', 'pages': '307–308', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Technology-mediated public activism has grown popular in recent years with the high uptake of social media. Facebook and Twitter have become venues for activists to participate in online activism, or organize offline activism events. However, due to accessibility barriers in physical environments and accessibility issues in social media, people with disabilities continue to face challenges when they engage with such social movements. We interviewed 22 disabled activists about how they used technology to mediate civic engagement and barriers they faced. We present preliminary findings from these interviews and describe a potential solution named ActVirtual, a mobile platform for accessible activism. Our future work will include implementing and testing ActVirtual with users to make online and offline activism more accessible.', 'doi': '10.1145/3132525.3134815', 'url': 'https://doi.org/10.1145/3132525.3134815', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'ActVirtual: Making Public Activism Accessible', 'author': 'Bora, Disha and Li, Hanlin and Salvi, Sagar and Brady, Erin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134815'}"
An Exploratory Case Study to Support Young Children with Spinal Muscular Atrophy (SMA),10.1145/3132525.3134772,"In this paper, we describe a preliminary case study that examines the challenges faced by very young children with Type I Spinal Muscular Atrophy (SMA) and how technology may help these children live a more independent life. Several input solutions were examined to support interaction between a young patient and computer-based systems. We started working with the patient when he was two and a half years old. The challenges observed and lessons learned regarding both working with very young children with severe disabilities and the use of specific technical solutions are discussed.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'young children, spinal muscular atrophy (sma), sensor', 'numpages': '2', 'pages': '309–310', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this paper, we describe a preliminary case study that examines the challenges faced by very young children with Type I Spinal Muscular Atrophy (SMA) and how technology may help these children live a more independent life. Several input solutions were examined to support interaction between a young patient and computer-based systems. We started working with the patient when he was two and a half years old. The challenges observed and lessons learned regarding both working with very young children with severe disabilities and the use of specific technical solutions are discussed.', 'doi': '10.1145/3132525.3134772', 'url': 'https://doi.org/10.1145/3132525.3134772', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'An Exploratory Case Study to Support Young Children with Spinal Muscular Atrophy (SMA)', 'author': 'Miao, Sheng and Tang, Ziying and Feng, Jinjuan Heidi and Jozkowski, Amanda and Lichtenwalner, Molly', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134772'}"
Assessing Collaboration between Autistic Players: An Engagement Metric,10.1145/3132525.3134795,We developed an engagement metric that is embedded in a two-player Kinect game. The game is designed to help autistic people interact and collaborate with each other and others. Each level has two phases - initially the players work on a task independently. In order to complete the task they must collaborate and agree. We added a component to assess the level of in-game cooperation. Using face tracking we developed a metric to automatically quantify collaboration based on the amount of time each player individually and together engage with one another. This can replace the time-consuming hand-coded evaluations. We also designed collaborative reward games including one that encourages players to interact with each other.,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'engagement, cooperation, collaborative games, autism', 'numpages': '2', 'pages': '311–312', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We developed an engagement metric that is embedded in a two-player Kinect game. The game is designed to help autistic people interact and collaborate with each other and others. Each level has two phases - initially the players work on a task independently. In order to complete the task they must collaborate and agree. We added a component to assess the level of in-game cooperation. Using face tracking we developed a metric to automatically quantify collaboration based on the amount of time each player individually and together engage with one another. This can replace the time-consuming hand-coded evaluations. We also designed collaborative reward games including one that encourages players to interact with each other.', 'doi': '10.1145/3132525.3134795', 'url': 'https://doi.org/10.1145/3132525.3134795', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Assessing Collaboration between Autistic Players: An Engagement Metric', 'author': 'Sturm, Deborah and Gillespie-Lynch, Kristen and Kholodovsky, Michael', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134795'}"
Blocks4All Demonstration: a Blocks-Based Programming Environment for Blind Children,10.1145/3132525.3134774,"Blocks-based programming environments, such as Scratch and Blockly, are designed to make learning programming easier for young children. They are increasingly being used for both formal and informal curriculum, such as many of Code.org's hour of code projects. However, these block-based environments rely heavily on visual metaphors and interactions, making them inaccessible for blind children. We describe our initial design of a touchscreen-based blocks environment that is accessible for blind children.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'touchscreen interactions., educational technology, children, blind, accessibility', 'numpages': '2', 'pages': '313–314', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Blocks-based programming environments, such as Scratch and Blockly, are designed to make learning programming easier for young children. They are increasingly being used for both formal and informal curriculum, such as many of Code.org's hour of code projects. However, these block-based environments rely heavily on visual metaphors and interactions, making them inaccessible for blind children. We describe our initial design of a touchscreen-based blocks environment that is accessible for blind children."", 'doi': '10.1145/3132525.3134774', 'url': 'https://doi.org/10.1145/3132525.3134774', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Blocks4All Demonstration: a Blocks-Based Programming Environment for Blind Children', 'author': 'Milne, Lauren R. and Baker, Catherine M. and Ladner, Richard E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134774'}"
CollabAll: Inclusive Discussion Support System For Deaf and Hearing Students,10.1145/3132525.3134800,"Even with advances in technology, group meetings between hearing and deaf and hard-of-hearing (D/HH) students can be challenging for all participants. This paper introduces CollabAll, a system that aims to better facilitate productive meetings between D/HH and hearing students. CollabAll provides D/HH individuals with a mechanism to actively participate in making decisions and getting their point across in team meetings. CollabAll enables every team member to create discussion topics for their meeting, track the person currently communicating and the current topic being discussed along with providing a mechanism for ""polite"" interruptions. Early feedback from participatory design and focus group studies indicated a positive user experience.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'turn taking, mixed groups, meetings, discussions, deaf', 'numpages': '2', 'pages': '315–316', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Even with advances in technology, group meetings between hearing and deaf and hard-of-hearing (D/HH) students can be challenging for all participants. This paper introduces CollabAll, a system that aims to better facilitate productive meetings between D/HH and hearing students. CollabAll provides D/HH individuals with a mechanism to actively participate in making decisions and getting their point across in team meetings. CollabAll enables every team member to create discussion topics for their meeting, track the person currently communicating and the current topic being discussed along with providing a mechanism for ""polite"" interruptions. Early feedback from participatory design and focus group studies indicated a positive user experience.', 'doi': '10.1145/3132525.3134800', 'url': 'https://doi.org/10.1145/3132525.3134800', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'CollabAll: Inclusive Discussion Support System For Deaf and Hearing Students', 'author': 'Peruma, Anthony and El-Glaly, Yasmine N.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134800'}"
Developing Interfaces for Rehabilitation: A Graduate Level Course for Students from Multiple Disciplines,10.1145/3132525.3134784,"This paper describes the design of a graduate level course focusing on developing interfaces for purposes of rehabilitation, catering to students from multiple disciplines. Students work on projects relating to the needs of individuals with disabilities, under the supervision of internal and external mentors. The course has been designed to help students develop both research and mobile interface technical design skills. Course structure and deliverables are described, along with the challenges faced and lessons learned from multiple offerings of the course.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'rehabilitation., inclusive design, graduate course, accessibility', 'numpages': '2', 'pages': '317–318', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper describes the design of a graduate level course focusing on developing interfaces for purposes of rehabilitation, catering to students from multiple disciplines. Students work on projects relating to the needs of individuals with disabilities, under the supervision of internal and external mentors. The course has been designed to help students develop both research and mobile interface technical design skills. Course structure and deliverables are described, along with the challenges faced and lessons learned from multiple offerings of the course.', 'doi': '10.1145/3132525.3134784', 'url': 'https://doi.org/10.1145/3132525.3134784', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Developing Interfaces for Rehabilitation: A Graduate Level Course for Students from Multiple Disciplines', 'author': 'Kuber, Ravi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134784'}"
DytectiveU: A Game to Train the Difficulties and the Strengths of Children with Dyslexia,10.1145/3132525.3134773,"In this demo we present DytectiveU, a game with 35,000 exercises to train the cognitive abilities related to dyslexia. To personalize the exercises, the game takes into consideration 25 indicators grouped in performance measures, language skills, working memory, executive functions and perceptual processes. The main contribution of this approach is to train dyslexia from a holistic point of view addressing not only the difficulties in reading and writing but also other cognitive abilities that are related to dyslexia and/or contribute to create coping skills to overcome dyslexia. The game is available for Android, iOS and Web (PC/Mac).","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'training exercises, serious games, dyslexia, cognitive skills', 'numpages': '2', 'pages': '319–320', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this demo we present DytectiveU, a game with 35,000 exercises to train the cognitive abilities related to dyslexia. To personalize the exercises, the game takes into consideration 25 indicators grouped in performance measures, language skills, working memory, executive functions and perceptual processes. The main contribution of this approach is to train dyslexia from a holistic point of view addressing not only the difficulties in reading and writing but also other cognitive abilities that are related to dyslexia and/or contribute to create coping skills to overcome dyslexia. The game is available for Android, iOS and Web (PC/Mac).', 'doi': '10.1145/3132525.3134773', 'url': 'https://doi.org/10.1145/3132525.3134773', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'DytectiveU: A Game to Train the Difficulties and the Strengths of Children with Dyslexia', 'author': ""Rello, Luz and Macias, Arturo and Herrera, Mari\\'{\\i}a and de Ros, Camila and Romero, Enrique and Bigham, Jeffrey P."", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134773'}"
ICT to Aid Dental Care of Children with Autism,10.1145/3132525.3134799,"Dental health in children with autism presents many challenges, due to their different perception of sensory experience and difficulty accepting unknown social contexts. The dental care setting presents strong sound-visual stimulations that can upset a patient with autism, often forcing dentists to administer chemical sedation in order to deliver dental care. In recent years, several technology-enhanced systems and apps have been proposed to help people with autism adapt to new contexts and cope with distressing social situations. Our study explores the potential of personalized digital tools for familiarizing these children with dental procedures and environments, and teaching them how to perform proper oral hygiene at home. A 3-month study to test ICT tools created to control children's anxiety and avoid sedation was carried out involving researchers, developers, dentists, psychologists, parents and ten children with autism observed under natural conditions during their first dental care cycle. The results appear to confirm the potential of personalized technology to reduce anxiety in professional settings, increasing children's wellbeing and safety and encouraging oral hygiene as part of their daily routine.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'web application, dental health, cognitive games, autism', 'numpages': '2', 'pages': '321–322', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Dental health in children with autism presents many challenges, due to their different perception of sensory experience and difficulty accepting unknown social contexts. The dental care setting presents strong sound-visual stimulations that can upset a patient with autism, often forcing dentists to administer chemical sedation in order to deliver dental care. In recent years, several technology-enhanced systems and apps have been proposed to help people with autism adapt to new contexts and cope with distressing social situations. Our study explores the potential of personalized digital tools for familiarizing these children with dental procedures and environments, and teaching them how to perform proper oral hygiene at home. A 3-month study to test ICT tools created to control children's anxiety and avoid sedation was carried out involving researchers, developers, dentists, psychologists, parents and ten children with autism observed under natural conditions during their first dental care cycle. The results appear to confirm the potential of personalized technology to reduce anxiety in professional settings, increasing children's wellbeing and safety and encouraging oral hygiene as part of their daily routine."", 'doi': '10.1145/3132525.3134799', 'url': 'https://doi.org/10.1145/3132525.3134799', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'ICT to Aid Dental Care of Children with Autism', 'author': 'Bondioli, Mariasole and Pelagatti, Susanna and Buzzi, Maria Claudia and Buzzi, Marina and Senette, Caterina', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134799'}"
Identifying and Mitigating Technology-Related Anxiety,10.1145/3132525.3134820,"Our experience with digital literacy tutoring for older adults confirms other research that identifies technology-related anxiety as a significant barrier for this constituency. Based on established theories of instructional design, we hypothesize that anxiety impairs the development of digital literacy skills by consuming critical cognitive resources. Our research with experienced tutors reveals numerous strategies for mitigating anxiety and motivates the development of anxiety-aware instructional and design frameworks, and new anxiety scales that better reflect current devices, services, and threats.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'senior citizens, digital literacy, anxiety, accessibility', 'numpages': '2', 'pages': '323–324', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Our experience with digital literacy tutoring for older adults confirms other research that identifies technology-related anxiety as a significant barrier for this constituency. Based on established theories of instructional design, we hypothesize that anxiety impairs the development of digital literacy skills by consuming critical cognitive resources. Our research with experienced tutors reveals numerous strategies for mitigating anxiety and motivates the development of anxiety-aware instructional and design frameworks, and new anxiety scales that better reflect current devices, services, and threats.', 'doi': '10.1145/3132525.3134820', 'url': 'https://doi.org/10.1145/3132525.3134820', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Identifying and Mitigating Technology-Related Anxiety', 'author': 'Steelman, Kelly and Wallace, Charles', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134820'}"
Independent Word Discovery for People with Aphasia,10.1145/3132525.3134790,"Augmentative and Alternative Communication (AAC) devices help people with speech-language impairments due to aphasia express themselves more independently and fluently. However, current AACs are still limited in that they must be pre-programmed with common words or phrases prior to use. When people with aphasia encounter unexpected and unplanned situations or contexts, they may have difficulty using their AAC device to generate the desired speech. We present the design of an AAC application that uses image recognition technology to give people with aphasia just in time access to words. Our design automatically carries out an extensive retrieval of words associated with a captured image. The relevance, accuracy and usefulness of each word can then be verified independently by the user through an intensive retrieval of images associated with each word.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'augmentative and alternative communication devices, aphasia', 'numpages': '2', 'pages': '325–326', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Augmentative and Alternative Communication (AAC) devices help people with speech-language impairments due to aphasia express themselves more independently and fluently. However, current AACs are still limited in that they must be pre-programmed with common words or phrases prior to use. When people with aphasia encounter unexpected and unplanned situations or contexts, they may have difficulty using their AAC device to generate the desired speech. We present the design of an AAC application that uses image recognition technology to give people with aphasia just in time access to words. Our design automatically carries out an extensive retrieval of words associated with a captured image. The relevance, accuracy and usefulness of each word can then be verified independently by the user through an intensive retrieval of images associated with each word.', 'doi': '10.1145/3132525.3134790', 'url': 'https://doi.org/10.1145/3132525.3134790', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Independent Word Discovery for People with Aphasia', 'author': 'Obiorah, Mmachi G. and Piper, Anne Marie and Horn, Michael', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134790'}"
Interactive Urdu Braille Learning System for Parents of Visually Impaired Students,10.1145/3132525.3134809,"Braille literacy is one of the core pillars of education for visually impaired children. Previous studies have highlighted the importance of Braille for Visually impaired children by suggesting that students who read Braille outside of the classroom have higher reading speed and fluency. In resource constrained countries, such as Pakistan, the visually impaired students may acquire the knowledge of Urdu Braille at special education schools. However, there is a dearth of resources for the parents of such children, when it comes to learning Urdu Braille. Therefore, we designed a web-based Urdu Braille Translator and interactive Braille learning tool to enhance the Urdu Braille learning experience for parents of visually impaired. The usability study of this tool was conducted with 15 parents of the visually impaired students.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'special education, selflearning, ict for education, braille learning', 'numpages': '2', 'pages': '327–328', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Braille literacy is one of the core pillars of education for visually impaired children. Previous studies have highlighted the importance of Braille for Visually impaired children by suggesting that students who read Braille outside of the classroom have higher reading speed and fluency. In resource constrained countries, such as Pakistan, the visually impaired students may acquire the knowledge of Urdu Braille at special education schools. However, there is a dearth of resources for the parents of such children, when it comes to learning Urdu Braille. Therefore, we designed a web-based Urdu Braille Translator and interactive Braille learning tool to enhance the Urdu Braille learning experience for parents of visually impaired. The usability study of this tool was conducted with 15 parents of the visually impaired students.', 'doi': '10.1145/3132525.3134809', 'url': 'https://doi.org/10.1145/3132525.3134809', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Interactive Urdu Braille Learning System for Parents of Visually Impaired Students', 'author': 'Iqbal, Muhammad Zahid and Shahid, Suleman and Naseem, Mustafa', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134809'}"
JustPoint: Identifying Colors with a Natural User Interface,10.1145/3132525.3134802,"People with severe visual impairments usually have no way of identifying the colors of objects in their environment. While existing smartphone apps can recognize colors and speak them aloud, they require the user to center the object of interest in the camera's field of view, which is challenging for many users. We developed a smartphone app to address this problem that reads aloud the color of the object pointed to by the user's fingertip, without confusion from background colors. We evaluated the app with nine people who are blind, demonstrating the app's effectiveness and suggesting directions for improvements in the future.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'hand recognition, color detection, assistive technology', 'numpages': '2', 'pages': '329–330', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""People with severe visual impairments usually have no way of identifying the colors of objects in their environment. While existing smartphone apps can recognize colors and speak them aloud, they require the user to center the object of interest in the camera's field of view, which is challenging for many users. We developed a smartphone app to address this problem that reads aloud the color of the object pointed to by the user's fingertip, without confusion from background colors. We evaluated the app with nine people who are blind, demonstrating the app's effectiveness and suggesting directions for improvements in the future."", 'doi': '10.1145/3132525.3134802', 'url': 'https://doi.org/10.1145/3132525.3134802', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'JustPoint: Identifying Colors with a Natural User Interface', 'author': ""Mascetti, Sergio and Gerino, Andrea and Bernareggi, Cristian and D'Acquisto, Silvia and Ducci, Mattia and Coughlan, James M."", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134802'}"
Making Facial Expressions of Emotions Accessible for Visually Impaired Persons,10.1145/3132525.3134823,"One of the big problems visually impaired persons experience in their daily lives, is the inability to see non-verbal cues of conversation partners. In this study, a wearable assistive technology is presented and evaluated which supports visually impaired persons with the recognition of facial expressions of emotions. The wearable assistive technology consists of a camera clipped on spectacles, emotion recognition software, and a vibrotactile belt with six tactors. An earlier controlled experimental study showed that users of the system improved significantly in their ability to recognize emotions from validated stimuli. In this paper, the next iteration in testing the system is presented, in which a more realistic usage situation was simulated. Eight visually impaired persons were invited to participate in conversations with an actor, who was instructed not to exaggerate his facial expressions. Participants engaged in two 15-minute mock job interview conversations, during one of which they were wearing the system. In the other conversation, no assistive technologies were used. The preliminary results showed that the concept of such wearable assistive technologies remains feasible. Participants within the study found it easy to learn and interpret the vibrotactile cues, which was also shown in their training performance. Furthermore, most participants could use the vibrotactile cues, while being able to stay engaged in the conversation. Nevertheless, some improvements are needed before the system can be used as assistive technology. The accuracy of the system was negatively affected by the lighting and movement conditions present in realistic conversations, compared to the controlled experiment condition. Furthermore, participants requested developments to improve the wearability of the system.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearables, vision-impaired persons, vibrotactile output, facial expressions, emotion recognition', 'numpages': '2', 'pages': '331–332', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'One of the big problems visually impaired persons experience in their daily lives, is the inability to see non-verbal cues of conversation partners. In this study, a wearable assistive technology is presented and evaluated which supports visually impaired persons with the recognition of facial expressions of emotions. The wearable assistive technology consists of a camera clipped on spectacles, emotion recognition software, and a vibrotactile belt with six tactors. An earlier controlled experimental study showed that users of the system improved significantly in their ability to recognize emotions from validated stimuli. In this paper, the next iteration in testing the system is presented, in which a more realistic usage situation was simulated. Eight visually impaired persons were invited to participate in conversations with an actor, who was instructed not to exaggerate his facial expressions. Participants engaged in two 15-minute mock job interview conversations, during one of which they were wearing the system. In the other conversation, no assistive technologies were used. The preliminary results showed that the concept of such wearable assistive technologies remains feasible. Participants within the study found it easy to learn and interpret the vibrotactile cues, which was also shown in their training performance. Furthermore, most participants could use the vibrotactile cues, while being able to stay engaged in the conversation. Nevertheless, some improvements are needed before the system can be used as assistive technology. The accuracy of the system was negatively affected by the lighting and movement conditions present in realistic conversations, compared to the controlled experiment condition. Furthermore, participants requested developments to improve the wearability of the system.', 'doi': '10.1145/3132525.3134823', 'url': 'https://doi.org/10.1145/3132525.3134823', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Making Facial Expressions of Emotions Accessible for Visually Impaired Persons', 'author': 'Buimer, Hendrik and Van der Geest, Thea and Nemri, Abdellatif and Schellens, Renske and Van Wezel, Richard and Zhao, Yan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134823'}"
Predictive Link Following Plug-In For Web Browsers,10.1145/3132525.3134776,"We demonstrate a target-aware pointing assistance software plug-in for web browsers. Web browsing can be difficult for people with motor impairments because of small links and cluttered web pages. The lack of high-precision movements presents a challenge for such interfaces because clicks may miss a link, or links may be followed unintentionally. Our approach is based on Predictive Link Following which alleviates the difficulties with link selection when using mouse replacement interfaces by predicting which link should be clicked based on the proximity of the cursor to the link. We previously presented and evaluated this method as lab-based experiments that were restricted to our custom web pages. We here demonstrate a plug-in for web browsers that allows our method to work on any web page. This publication accompanies the public release of the software as a freely-available download.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'web browsing, target-aware pointing, mouse replacement interfaces, accessibilty', 'numpages': '2', 'pages': '333–334', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We demonstrate a target-aware pointing assistance software plug-in for web browsers. Web browsing can be difficult for people with motor impairments because of small links and cluttered web pages. The lack of high-precision movements presents a challenge for such interfaces because clicks may miss a link, or links may be followed unintentionally. Our approach is based on Predictive Link Following which alleviates the difficulties with link selection when using mouse replacement interfaces by predicting which link should be clicked based on the proximity of the cursor to the link. We previously presented and evaluated this method as lab-based experiments that were restricted to our custom web pages. We here demonstrate a plug-in for web browsers that allows our method to work on any web page. This publication accompanies the public release of the software as a freely-available download.', 'doi': '10.1145/3132525.3134776', 'url': 'https://doi.org/10.1145/3132525.3134776', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Predictive Link Following Plug-In For Web Browsers', 'author': 'Pierson Stachecki, Lyle and Magee, John', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134776'}"
Prime III: Voting for a More Accessible Future,10.1145/3132525.3134771,"In 2012, about one-third of voters with disabilities reported having issues when voting in a polling place. Although the Help America Vote Act (HAVA) was passed in 2002, it is clear that there is room for improvement within the domain of accessible voting. Prime III is a voting technology that addresses many issues that plague other accessible voting systems. By addressing the needs of different communities, Prime III has become a ballot marking system that allows all voters to vote on one machine. This demonstration will showcase the accessibility features of Prime III and how it can be used in elections.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'universal design, accessible voting', 'numpages': '2', 'pages': '335–336', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In 2012, about one-third of voters with disabilities reported having issues when voting in a polling place. Although the Help America Vote Act (HAVA) was passed in 2002, it is clear that there is room for improvement within the domain of accessible voting. Prime III is a voting technology that addresses many issues that plague other accessible voting systems. By addressing the needs of different communities, Prime III has become a ballot marking system that allows all voters to vote on one machine. This demonstration will showcase the accessibility features of Prime III and how it can be used in elections.', 'doi': '10.1145/3132525.3134771', 'url': 'https://doi.org/10.1145/3132525.3134771', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Prime III: Voting for a More Accessible Future', 'author': 'Smarr, Simone A. and Sherman, Imani N. and Posadas, Brianna and Gilbert, Juan E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134771'}"
Real-Time Depth-Camera Based Hand Tracking for ASL Recognition,10.1145/3132525.3134777,"Accurate real-time depth camera-based tracking of limbs, fingers and faces would be of great use to the field of Sign Language Recognition (SLR). While aspects of depth-based tracking have been applied to SLR, technological limitations have previously forced trade-offs between the resolution necessary to track finger positions and the field of view necessary to track the signer's body. Only recently, with improvements in cameras and computing power, have algorithms been developed which boast the capability of maintaining accurate finger tracking over an appropriately sized volume of space. In this paper, we employ the publicly available Sphere-Mesh [1] hand tracking algorithm to collect and recognize ASL handshapes. In doing so, we demonstrate recognition rates comparable to other state of the art handshape classifiers using simple na\'{\i}ve Bayesian classifiers that can run in real-time.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'sign recognition, hand tracking, asl', 'numpages': '2', 'pages': '337–338', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Accurate real-time depth camera-based tracking of limbs, fingers and faces would be of great use to the field of Sign Language Recognition (SLR). While aspects of depth-based tracking have been applied to SLR, technological limitations have previously forced trade-offs between the resolution necessary to track finger positions and the field of view necessary to track the signer's body. Only recently, with improvements in cameras and computing power, have algorithms been developed which boast the capability of maintaining accurate finger tracking over an appropriately sized volume of space. In this paper, we employ the publicly available Sphere-Mesh [1] hand tracking algorithm to collect and recognize ASL handshapes. In doing so, we demonstrate recognition rates comparable to other state of the art handshape classifiers using simple na\\'{\\i}ve Bayesian classifiers that can run in real-time."", 'doi': '10.1145/3132525.3134777', 'url': 'https://doi.org/10.1145/3132525.3134777', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Real-Time Depth-Camera Based Hand Tracking for ASL Recognition', 'author': 'Taylor, Brandon and Dey, Anind and Siewiorek, Daniel and Smailagic, Asim', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134777'}"
Sign Language Support System for Viewing Sports Programs,10.1145/3132525.3134768,"To expand the services that are based on Japanese Sign Language (JSL) for deaf and hard of hearing people, we developed a support system for viewing sports program. The system provides sign language computer graphics (CG) animations and other auxiliary information such as text, image, and notifications automatically generated from game metadata. Results obtained from gathered opinions showed that the system is effective for understanding the situation when a game is interrupted.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'jsl, japanese sign language, avatar, accessibility technology', 'numpages': '2', 'pages': '339–340', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'To expand the services that are based on Japanese Sign Language (JSL) for deaf and hard of hearing people, we developed a support system for viewing sports program. The system provides sign language computer graphics (CG) animations and other auxiliary information such as text, image, and notifications automatically generated from game metadata. Results obtained from gathered opinions showed that the system is effective for understanding the situation when a game is interrupted.', 'doi': '10.1145/3132525.3134768', 'url': 'https://doi.org/10.1145/3132525.3134768', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Sign Language Support System for Viewing Sports Programs', 'author': 'Uchida, Tsubasa and Miyazaki, Taro and Azuma, Makiko and Umeda, Shuichi and Kato, Naoto and Sumiyoshi, Hideki and Yamanouchi, Yuko and Hiruma, Nobuyuki', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134768'}"
Tangibles + Programming + Audio Stories = Fun,10.1145/3132525.3134769,"Block-based programming languages enable novice programmers, including children, to learn the basics of programming. However, most block-based programming languages are not accessible to blind and visually impaired users because they rely upon visual drag-and-drop interaction, and because they typically create visual output. To improve access to block-based programming languages, we introduce Story Blocks, a programming toolkit that uses tangible blocks to represent story components, and which produces output in the form of accessible audio stories and games. Story Blocks provides an introductory programming environment that can be enjoyed by people of all abilities.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tangible computing, programming, accessibility', 'numpages': '2', 'pages': '341–342', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Block-based programming languages enable novice programmers, including children, to learn the basics of programming. However, most block-based programming languages are not accessible to blind and visually impaired users because they rely upon visual drag-and-drop interaction, and because they typically create visual output. To improve access to block-based programming languages, we introduce Story Blocks, a programming toolkit that uses tangible blocks to represent story components, and which produces output in the form of accessible audio stories and games. Story Blocks provides an introductory programming environment that can be enjoyed by people of all abilities.', 'doi': '10.1145/3132525.3134769', 'url': 'https://doi.org/10.1145/3132525.3134769', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Tangibles + Programming + Audio Stories = Fun', 'author': 'Koushik, Varsha and Kane, Shaun K.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134769'}"
The Participatory Design of an Adaptive Interface to Support Users with Changing Pointing Ability,10.1145/3132525.3134810,"Individuals who experience temporary, intermittent, or gradual changes in pointing ability may encounter frustrating experiences when using computer input devices. Personalized pointing systems that automatically assess changes in performance and provide individualized information and assistance may benefit these users. However, there has been little inquiry into this populations' expectations for interacting with these types of systems. We describe a participatory design process in which we used a technology probe to assess the information needs and expectations of 27 individuals who experience occasional changes in pointing ability, through interactions with and discussion regarding a high-fidelity personalized pointing prototype. Participants preferred notification and adaptation interactions that provided them with control and explanation of system actions, instead of abstract notifications and automatic adaptations. We describe how we applied these finding in the design of the PINATA system.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'web navigation, pointing problems, adaptive systems', 'numpages': '2', 'pages': '343–344', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Individuals who experience temporary, intermittent, or gradual changes in pointing ability may encounter frustrating experiences when using computer input devices. Personalized pointing systems that automatically assess changes in performance and provide individualized information and assistance may benefit these users. However, there has been little inquiry into this populations' expectations for interacting with these types of systems. We describe a participatory design process in which we used a technology probe to assess the information needs and expectations of 27 individuals who experience occasional changes in pointing ability, through interactions with and discussion regarding a high-fidelity personalized pointing prototype. Participants preferred notification and adaptation interactions that provided them with control and explanation of system actions, instead of abstract notifications and automatic adaptations. We describe how we applied these finding in the design of the PINATA system."", 'doi': '10.1145/3132525.3134810', 'url': 'https://doi.org/10.1145/3132525.3134810', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'The Participatory Design of an Adaptive Interface to Support Users with Changing Pointing Ability', 'author': 'Martin-Hammond, Aqueasha and Hamidi, Foad and Bhalerao, Tejas and Ali, Abdullah and Hornback, Catherine and Means, Casey and Hurst, Amy', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134810'}"
Towards a Sensor-based System for Assessing and Monitoring Powered Mobility Skills in Children,10.1145/3132525.3134813,"Children with motor or cognitive impairments who require powered mobility at a very young age will face social and environmental barriers that make learning how to use the mobility device a challenging task. We present a first approach of a framework to help therapists and service providers to assess and monitor how children use their mobility device, which results from the combination of a plug and play inertial sensor, and the support of the Assessment Learning tool (ALP) from Nilsson and Durkin. We performed a formative study on four able-bodied children using an electric wheelchair. Results suggest it is possible to measure children's driving skills with this approach, and that results can be mapped to the validated ALP tool. We present the limitations of our study and the direction of future work.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wheelchair skills, powered mobility, imu., children, assessment and monitoring', 'numpages': '2', 'pages': '345–346', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Children with motor or cognitive impairments who require powered mobility at a very young age will face social and environmental barriers that make learning how to use the mobility device a challenging task. We present a first approach of a framework to help therapists and service providers to assess and monitor how children use their mobility device, which results from the combination of a plug and play inertial sensor, and the support of the Assessment Learning tool (ALP) from Nilsson and Durkin. We performed a formative study on four able-bodied children using an electric wheelchair. Results suggest it is possible to measure children's driving skills with this approach, and that results can be mapped to the validated ALP tool. We present the limitations of our study and the direction of future work."", 'doi': '10.1145/3132525.3134813', 'url': 'https://doi.org/10.1145/3132525.3134813', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Towards a Sensor-based System for Assessing and Monitoring Powered Mobility Skills in Children', 'author': 'Ramirez Herrera, Roxana and Holloway, Catherine and Heravi, Behzad Momahed and Carlson, Tom', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134813'}"
Towards Improving Predictive AAC using Crowdsourced Dialogues and Partner Context,10.1145/3132525.3134814,"Augmentative and Alternative Communication (AAC) devices typically rely on a language model to help make predictions or disambiguate user input. We investigate how to improve predictions in two-sided conversational dialogues. We collect and share a new corpus of crowdsourced everyday dialogues. We show how language models based on recurrent neural networks outperform N-gram models on these dialogues. We demonstrate further gains are possible using text obtained from an AAC user's communication partner, even when that text is partial or contains errors.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'text input, text entry, rnnlm, language modeling, crowdsourcing, augmentative and alternative communication, aac', 'numpages': '2', 'pages': '347–348', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Augmentative and Alternative Communication (AAC) devices typically rely on a language model to help make predictions or disambiguate user input. We investigate how to improve predictions in two-sided conversational dialogues. We collect and share a new corpus of crowdsourced everyday dialogues. We show how language models based on recurrent neural networks outperform N-gram models on these dialogues. We demonstrate further gains are possible using text obtained from an AAC user's communication partner, even when that text is partial or contains errors."", 'doi': '10.1145/3132525.3134814', 'url': 'https://doi.org/10.1145/3132525.3134814', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Towards Improving Predictive AAC using Crowdsourced Dialogues and Partner Context', 'author': 'Vertanen, Keith', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134814'}"
Towards Supporting Individuals with Situational Impairments in Inhospitable Environments,10.1145/3132525.3134783,"In this paper, we describe an approach to develop alerts for individuals experiencing situationally-induced impairments and disabilities (SIIDs) in inhospitable environments, with a view to support situational awareness and decision making. Our research focuses on the needs of firefighters whose visual and auditory channels may be restricted while performing tasks, resulting in cues sometimes being missed. Through a series of contextual interviews, scenarios have been developed focusing on areas where alerts are required. A participatory-approach has been adopted with the aim of developing perceivable, meaningful and respected alerts, designed to better resist the impacts of SIIDs.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'situationally-induced impairments and disabilities', 'numpages': '2', 'pages': '349–350', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'In this paper, we describe an approach to develop alerts for individuals experiencing situationally-induced impairments and disabilities (SIIDs) in inhospitable environments, with a view to support situational awareness and decision making. Our research focuses on the needs of firefighters whose visual and auditory channels may be restricted while performing tasks, resulting in cues sometimes being missed. Through a series of contextual interviews, scenarios have been developed focusing on areas where alerts are required. A participatory-approach has been adopted with the aim of developing perceivable, meaningful and respected alerts, designed to better resist the impacts of SIIDs.', 'doi': '10.1145/3132525.3134783', 'url': 'https://doi.org/10.1145/3132525.3134783', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Towards Supporting Individuals with Situational Impairments in Inhospitable Environments', 'author': 'Wolf, Flynn and Kuber, Ravi and Pawluk, Dianne and Turnage, Brian', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134783'}"
Unobtrusive Monitoring of Parkinson's Disease Based on Digital Biomarkers of Human Behaviour,10.1145/3132525.3134782,"Parkinson's Disease impairs the motor, cognitive and emotional functioning of people. Clinicians do not get an accurate image of the disease because patients visit them every six months and their symptoms can change within hours. Technology has been used to tackle this problem, but most approaches disrupt people's routines or are uncomfortable to use. We aim to monitor Parkinson's in an unobtrusive, longitudinal and naturalistic way based on digital biomarkers inferred from smartphone-collected heterogeneous data. We use Parkinson's clinical scales and self-reporting of symptoms as ground truth to evaluate our methodology. Here, we present three insights we gained after tracking four people 24/7 for four months: a) the monitoring smartphone needs to be people's primary device, b) participants prefer a paper diary for symptom self-reporting, and c) social and phone interaction will be explored as digital biomarkers. We expect this approach to improve patient's quality of life and the efficiency of health services.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': ""ubiquitous sensing, smartphone, passive sensing, parkinson's disease, health monitoring"", 'numpages': '2', 'pages': '351–352', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Parkinson's Disease impairs the motor, cognitive and emotional functioning of people. Clinicians do not get an accurate image of the disease because patients visit them every six months and their symptoms can change within hours. Technology has been used to tackle this problem, but most approaches disrupt people's routines or are uncomfortable to use. We aim to monitor Parkinson's in an unobtrusive, longitudinal and naturalistic way based on digital biomarkers inferred from smartphone-collected heterogeneous data. We use Parkinson's clinical scales and self-reporting of symptoms as ground truth to evaluate our methodology. Here, we present three insights we gained after tracking four people 24/7 for four months: a) the monitoring smartphone needs to be people's primary device, b) participants prefer a paper diary for symptom self-reporting, and c) social and phone interaction will be explored as digital biomarkers. We expect this approach to improve patient's quality of life and the efficiency of health services."", 'doi': '10.1145/3132525.3134782', 'url': 'https://doi.org/10.1145/3132525.3134782', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': ""Unobtrusive Monitoring of Parkinson's Disease Based on Digital Biomarkers of Human Behaviour"", 'author': 'Vega, Julio and Jay, Caroline and Vigo, Markel and Harper, Simon', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134782'}"
Value Sensitive Design for Neurodiverse Teams in Higher Education,10.1145/3132525.3134787,"In a neurodiverse team, such as one comprised of students with autism and neurotypical students, social interaction and cognitive styles differ. We used a Value-Sensitive Design (VSD) approach to explore the role of technology in supporting the diverse values of neurodiverse teams. We conducted interviews with higher education disability services staff, followed by interviews with students with autism. We analyzed the students' values using the Q-Methodology. We found that key values of students with autism are: freedom from stigma, individual comfort, social comfort, social connection, and team cohesion. Through a VSD technical investigation, we found that current collaboration and affective technologies focus on supporting social connection and team cohesion. However, these technologies tend to not enable tailoring the user experience for the individual comfort that can benefit autistic users.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'value-sensitive design, neurodiversity, collaboration, autism', 'numpages': '2', 'pages': '353–354', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""In a neurodiverse team, such as one comprised of students with autism and neurotypical students, social interaction and cognitive styles differ. We used a Value-Sensitive Design (VSD) approach to explore the role of technology in supporting the diverse values of neurodiverse teams. We conducted interviews with higher education disability services staff, followed by interviews with students with autism. We analyzed the students' values using the Q-Methodology. We found that key values of students with autism are: freedom from stigma, individual comfort, social comfort, social connection, and team cohesion. Through a VSD technical investigation, we found that current collaboration and affective technologies focus on supporting social connection and team cohesion. However, these technologies tend to not enable tailoring the user experience for the individual comfort that can benefit autistic users."", 'doi': '10.1145/3132525.3134787', 'url': 'https://doi.org/10.1145/3132525.3134787', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Value Sensitive Design for Neurodiverse Teams in Higher Education', 'author': 'Zolyomi, Annuska and Ross, Anne Spencer and Bhattacharya, Arpita and Milne, Lauren and Munson, Sean', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134787'}"
WeLi: A Smartwatch Application to Assist Students with Intellectual and Developmental Disabilities,10.1145/3132525.3134770,"To explore the potential of smartwatches in special education, we designed, developed and evaluated WeLi - a wearable application that assists students with intellectual and developmental disabilities (IDDs) in educational environments. By integrating a mobile and a wearable solution, the WeLi application assists students with IDDs and their assistants, facilitating collaboration, communication and planning. WeLi provides features for intervention, self-regulation for mood and reminders. The findings from the user studies show that the students and their assistants are enthusiastic about adopting the novel technology in classroom settings.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearables, special education, smartwatches, intellectual and developmental disabilities (idds), assistive technologies', 'numpages': '2', 'pages': '355–356', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'To explore the potential of smartwatches in special education, we designed, developed and evaluated WeLi - a wearable application that assists students with intellectual and developmental disabilities (IDDs) in educational environments. By integrating a mobile and a wearable solution, the WeLi application assists students with IDDs and their assistants, facilitating collaboration, communication and planning. WeLi provides features for intervention, self-regulation for mood and reminders. The findings from the user studies show that the students and their assistants are enthusiastic about adopting the novel technology in classroom settings.', 'doi': '10.1145/3132525.3134770', 'url': 'https://doi.org/10.1145/3132525.3134770', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'WeLi: A Smartwatch Application to Assist Students with Intellectual and Developmental Disabilities', 'author': 'Zheng, Hui and Motti, Vivian Genaro', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134770'}"
"""I Always Have to Think About It First"": Authentication Experiences of People with Cognitive Impairments",10.1145/3132525.3134788,"Authentication is a mundane yet often integral part of people's experiences with computing devices and Internet services. Since most authentication mechanisms were designed without explicitly considering people with disabilities, these mechanisms may pose significant challenges for these users. In this paper, we report results from a contextual inquiry study on the authentication experiences and challenges of people with cognitive impairments. We identified a number of difficulties our participants experienced, such as remembering usernames and passwords, typing on the keyboard, logging out from their existing online accounts and being aware of errors. We hope the insights we found will indeed make future authentication systems more accessible and easier to use for users with cognitive impairments.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'security, hci, cognitive impairments, authentication', 'numpages': '2', 'pages': '357–358', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Authentication is a mundane yet often integral part of people's experiences with computing devices and Internet services. Since most authentication mechanisms were designed without explicitly considering people with disabilities, these mechanisms may pose significant challenges for these users. In this paper, we report results from a contextual inquiry study on the authentication experiences and challenges of people with cognitive impairments. We identified a number of difficulties our participants experienced, such as remembering usernames and passwords, typing on the keyboard, logging out from their existing online accounts and being aware of errors. We hope the insights we found will indeed make future authentication systems more accessible and easier to use for users with cognitive impairments."", 'doi': '10.1145/3132525.3134788', 'url': 'https://doi.org/10.1145/3132525.3134788', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': '""I Always Have to Think About It First"": Authentication Experiences of People with Cognitive Impairments', 'author': 'Hayes, Jordan and Li, Xiao and Wang, Yang', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134788'}"
Augmented Reality for the Literacy Development of Deaf Children: A Preliminary Investigation,10.1145/3132525.3134789,"This paper reports the on-going research on the development of an Augmented Reality (AR) application for the literacy development of hard of hearing children, particularly deaf children that rely on Arabic Sign Language (ArSL). Preliminary studies were conducted to determine the visual needs of deaf Arabic learners when reading using three different instruments and targets: interviews with teachers, observation of deaf children, and questionnaire from their parents. Results from teachers and parents indicate a preference for multiple resources, primarily ArSL, including photos and videos. Students, on the other hand, performed better with finger-spelling and poorly in ArSL. This disconnect between both achievements highlights the importance of considering various perspectives in the development of AR that targets literacy in younger children.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'hard of hearing, dhh, deaf, augmented reality, arsl, arabic sign language', 'numpages': '2', 'pages': '359–360', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper reports the on-going research on the development of an Augmented Reality (AR) application for the literacy development of hard of hearing children, particularly deaf children that rely on Arabic Sign Language (ArSL). Preliminary studies were conducted to determine the visual needs of deaf Arabic learners when reading using three different instruments and targets: interviews with teachers, observation of deaf children, and questionnaire from their parents. Results from teachers and parents indicate a preference for multiple resources, primarily ArSL, including photos and videos. Students, on the other hand, performed better with finger-spelling and poorly in ArSL. This disconnect between both achievements highlights the importance of considering various perspectives in the development of AR that targets literacy in younger children.', 'doi': '10.1145/3132525.3134789', 'url': 'https://doi.org/10.1145/3132525.3134789', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Augmented Reality for the Literacy Development of Deaf Children: A Preliminary Investigation', 'author': 'Almutairi, Aziza and Al-Megren, Shiroq', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134789'}"
Augmented Reality Magnification for Low Vision Users with the Microsoft Hololens and a Finger-Worn Camera,10.1145/3132525.3134812,"Recent technical advances have enabled new wearable augmented reality (AR) solutions that can aid people with visual impairments (VI) in their everyday lives. Here, we investigate an AR-based magnification solution that combines a small finger-worn camera with a transparent augmented reality display (the Microsoft Hololens). The image from the camera is processed and projected on the Hololens to magnify visible content below the user's finger such as text and images. Our approach offers: (i) a close-up camera view (similar to a CCTV system) with the portability and processing power of a smartphone magnifier app, (ii) access to content through direct touch, and (iii) flexible placement of the magnified image within the wearer's field of view. We present three proof-of-concept interfaces and plans for a user evaluation.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearable computing, visually impaired, reading printed text, magnification, augmented reality, assistive technology', 'numpages': '2', 'pages': '361–362', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Recent technical advances have enabled new wearable augmented reality (AR) solutions that can aid people with visual impairments (VI) in their everyday lives. Here, we investigate an AR-based magnification solution that combines a small finger-worn camera with a transparent augmented reality display (the Microsoft Hololens). The image from the camera is processed and projected on the Hololens to magnify visible content below the user's finger such as text and images. Our approach offers: (i) a close-up camera view (similar to a CCTV system) with the portability and processing power of a smartphone magnifier app, (ii) access to content through direct touch, and (iii) flexible placement of the magnified image within the wearer's field of view. We present three proof-of-concept interfaces and plans for a user evaluation."", 'doi': '10.1145/3132525.3134812', 'url': 'https://doi.org/10.1145/3132525.3134812', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Augmented Reality Magnification for Low Vision Users with the Microsoft Hololens and a Finger-Worn Camera', 'author': 'Stearns, Lee and DeSouza, Victor and Yin, Jessica and Findlater, Leah and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134812'}"
Automatic Environment Adjustment for Emotional Disabilities,10.1145/3132525.3134816,"One often-overlooked area for assistive technology is help for those with emotional needs. Since these individuals may not emote in a typical way, most techniques for affective computing will not work for this population. Further, the applications that detect emotion are generally concerned with helping the user with some task, not simply helping them with their emotional difficulties. In this work, we present React 2 Me, a system that uses ambient technology to detect multimodal behavioral cues that may indicate emotional distress and adjust the environment to help the individual regulate their emotions.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'interactive spaces, affective computing, accessible computing', 'numpages': '2', 'pages': '363–364', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'One often-overlooked area for assistive technology is help for those with emotional needs. Since these individuals may not emote in a typical way, most techniques for affective computing will not work for this population. Further, the applications that detect emotion are generally concerned with helping the user with some task, not simply helping them with their emotional difficulties. In this work, we present React 2 Me, a system that uses ambient technology to detect multimodal behavioral cues that may indicate emotional distress and adjust the environment to help the individual regulate their emotions.', 'doi': '10.1145/3132525.3134816', 'url': 'https://doi.org/10.1145/3132525.3134816', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Automatic Environment Adjustment for Emotional Disabilities', 'author': 'Duvall, Shannon and Spurlock, Scott and Duvall, Robert', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134816'}"
Caption Placement on an Augmented Reality Head Worn Device,10.1145/3132525.3134786,"Head worn augmented reality devices present new opportunities for creating assistive devices. This work investigates possible locations for captions for one-on-one conversations from a visibility and usability standpoint. Even in a one-on-one conversation there are interesting design challenges. We investigate 3 main factors: 1) Reference frame for captions (viewport or world coordinates); 2) Relation to the viewer (track the viewer's view, or track the speaker); and 3) Relation to speaker's face (most desirable vertical offset for captions). In this preliminary work, we argue that world coordinates are preferable (for reference frame), and then present our work in progress on the other factors.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'hwd, hard-of-hearing, deaf, captions, augmented reality, ar', 'numpages': '2', 'pages': '365–366', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Head worn augmented reality devices present new opportunities for creating assistive devices. This work investigates possible locations for captions for one-on-one conversations from a visibility and usability standpoint. Even in a one-on-one conversation there are interesting design challenges. We investigate 3 main factors: 1) Reference frame for captions (viewport or world coordinates); 2) Relation to the viewer (track the viewer's view, or track the speaker); and 3) Relation to speaker's face (most desirable vertical offset for captions). In this preliminary work, we argue that world coordinates are preferable (for reference frame), and then present our work in progress on the other factors."", 'doi': '10.1145/3132525.3134786', 'url': 'https://doi.org/10.1145/3132525.3134786', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Caption Placement on an Augmented Reality Head Worn Device', 'author': 'Schipper, Chris and Brinkman, Bo', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134786'}"
Checklist for Accessible Media Player Evaluation,10.1145/3132525.3134791,"Nowadays, consumption of video content is extremely high and the corresponding technology must be accessible in order for people with disabilities to be able to access it. For this reason, the user agents or media players used to access video content must be accessible. This poster presents a checklist that includes indicators which can assist in the design and evaluation of accessible media players.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'media player, evaluation, accessibility', 'numpages': '2', 'pages': '367–368', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Nowadays, consumption of video content is extremely high and the corresponding technology must be accessible in order for people with disabilities to be able to access it. For this reason, the user agents or media players used to access video content must be accessible. This poster presents a checklist that includes indicators which can assist in the design and evaluation of accessible media players.', 'doi': '10.1145/3132525.3134791', 'url': 'https://doi.org/10.1145/3132525.3134791', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Checklist for Accessible Media Player Evaluation', 'author': ""Moreno, Lourdes and Gonz\\'{a}lez-Garc\\'{\\i}a, Mar\\'{\\i}a and Mart\\'{\\i}nez, Paloma and Gonz\\'{a}lez, Yolanda"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134791'}"
Evaluating Author and User Experience for an Audio-Haptic System for Annotation of Physical Models,10.1145/3132525.3134811,"We describe three usability studies involving a prototype system for creation and haptic exploration of labeled locations on 3D objects. The system uses a computer, webcam, and fiducial markers to associate a physical 3D object in the camera's view with a pre-defined digital map of labeled locations (""hotspots""), and to do real-time finger tracking, allowing a blind or visually impaired user to explore the object and hear individual labels spoken as each hotspot is touched. This paper describes: (a) a formative study with blind users exploring pre-annotated objects to assess system usability and accuracy; (b) a focus group of blind participants who used the system and, through structured and unstructured discussion, provided feedback on its practicality, possible applications, and real-world potential; and (c) a formative study in which a sighted adult used the system to add labels to on-screen images of objects, demonstrating the practicality of remote annotation of 3D models. These studies and related literature suggest potential for future iterations of the system to benefit blind and visually impaired users in educational, professional, and recreational contexts.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tactile graphics, low vision, blindness, 3d models', 'numpages': '2', 'pages': '369–370', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We describe three usability studies involving a prototype system for creation and haptic exploration of labeled locations on 3D objects. The system uses a computer, webcam, and fiducial markers to associate a physical 3D object in the camera\'s view with a pre-defined digital map of labeled locations (""hotspots""), and to do real-time finger tracking, allowing a blind or visually impaired user to explore the object and hear individual labels spoken as each hotspot is touched. This paper describes: (a) a formative study with blind users exploring pre-annotated objects to assess system usability and accuracy; (b) a focus group of blind participants who used the system and, through structured and unstructured discussion, provided feedback on its practicality, possible applications, and real-world potential; and (c) a formative study in which a sighted adult used the system to add labels to on-screen images of objects, demonstrating the practicality of remote annotation of 3D models. These studies and related literature suggest potential for future iterations of the system to benefit blind and visually impaired users in educational, professional, and recreational contexts.', 'doi': '10.1145/3132525.3134811', 'url': 'https://doi.org/10.1145/3132525.3134811', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Evaluating Author and User Experience for an Audio-Haptic System for Annotation of Physical Models', 'author': 'Coughlan, James M. and Miele, Joshua', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134811'}"
Exploring the Community of Blind or Visually Impaired People on YouTube,10.1145/3132525.3134801,"We explore a community of blind or visually impaired (BVI) people through video blogs (vlogs) on YouTube. Many researchers have used vlogs as a means of identifying communities of video bloggers (vloggers). Nevertheless, little is still known about how BVI vloggers interact through vlogs in video-based social media. To fill the gap, we identified BVI vloggers and types of the vlogs they produced by analyzing videos on YouTube. Also, we found how BVI vloggers were connected with each other through vlogs. To our knowledge, this is the first attempt utilizing vlogs to understand the community of BVI people on YouTube.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'youtube, vlogs, visually impaired, online community, blind, accessibility', 'numpages': '2', 'pages': '371–372', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We explore a community of blind or visually impaired (BVI) people through video blogs (vlogs) on YouTube. Many researchers have used vlogs as a means of identifying communities of video bloggers (vloggers). Nevertheless, little is still known about how BVI vloggers interact through vlogs in video-based social media. To fill the gap, we identified BVI vloggers and types of the vlogs they produced by analyzing videos on YouTube. Also, we found how BVI vloggers were connected with each other through vlogs. To our knowledge, this is the first attempt utilizing vlogs to understand the community of BVI people on YouTube.', 'doi': '10.1145/3132525.3134801', 'url': 'https://doi.org/10.1145/3132525.3134801', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Exploring the Community of Blind or Visually Impaired People on YouTube', 'author': 'Seo, Woosuk and Jung, Hyunggu', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134801'}"
Feasibility of Using Automatic Speech Recognition with Voices of Deaf and Hard-of-Hearing Individuals,10.1145/3132525.3134819,"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78\% word error rate (WER) compared to a hearing speech 18\% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'word error rate, hearing speech, hearing, deaf speech, deaf, automatic speech recognition', 'numpages': '2', 'pages': '373–374', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78\\% word error rate (WER) compared to a hearing speech 18\\% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people."", 'doi': '10.1145/3132525.3134819', 'url': 'https://doi.org/10.1145/3132525.3134819', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Feasibility of Using Automatic Speech Recognition with Voices of Deaf and Hard-of-Hearing Individuals', 'author': 'Glasser, Abraham T. and Kushalnagar, Kesavan R. and Kushalnagar, Raja S.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134819'}"
FingerVoice: A Syllable Based Input System Via Fingers Touching,10.1145/3132525.3134806,"This paper proposes a novel input system via fingers touching, called FingerVoice. Instead of traditional letter based input system, it is based on syllables. Fingers on the left hand stand for consonants while fingers on the right hand for vowels. Fingers from both hands touch each other to form a syllable. We use a pair of gloves with conductive finger caps to detect the touch. As an example, the implementation of Chinese is presented in details. By connecting it to a smart phone via Bluetooth, a utility speaking system is implemented to help the speech-impaired to ""speak"" via smart phone. Our experimental result shows that this syllable based input device is faster than letter based input devices.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'syllable, speech impaired, input device, gloves, fingervoice', 'numpages': '2', 'pages': '375–376', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper proposes a novel input system via fingers touching, called FingerVoice. Instead of traditional letter based input system, it is based on syllables. Fingers on the left hand stand for consonants while fingers on the right hand for vowels. Fingers from both hands touch each other to form a syllable. We use a pair of gloves with conductive finger caps to detect the touch. As an example, the implementation of Chinese is presented in details. By connecting it to a smart phone via Bluetooth, a utility speaking system is implemented to help the speech-impaired to ""speak"" via smart phone. Our experimental result shows that this syllable based input device is faster than letter based input devices.', 'doi': '10.1145/3132525.3134806', 'url': 'https://doi.org/10.1145/3132525.3134806', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'FingerVoice: A Syllable Based Input System Via Fingers Touching', 'author': 'Chen, Wenjie and Ma, Yangyang and Chai, Zhilei and Chen, Mingsong', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134806'}"
Goby: A Wearable Swimming Aid for Blind Athletes,10.1145/3132525.3134822,"We introduce Goby, a swimming aid that provides audio feedback for blind and visually impaired athletes. Goby's activity tracker is worn on the thigh and uses a downward-facing camera to track the swimmer's position in the pool. Goby detects when the user is swimming outside the lane or approaching a wall, and warns the user via an audio notification. In this paper, we introduce the Goby prototype and share formative feedback about the prototype from blind and sighted swimmers.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearable computing, fitness, blindness, accessibility', 'numpages': '2', 'pages': '377–378', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""We introduce Goby, a swimming aid that provides audio feedback for blind and visually impaired athletes. Goby's activity tracker is worn on the thigh and uses a downward-facing camera to track the swimmer's position in the pool. Goby detects when the user is swimming outside the lane or approaching a wall, and warns the user via an audio notification. In this paper, we introduce the Goby prototype and share formative feedback about the prototype from blind and sighted swimmers."", 'doi': '10.1145/3132525.3134822', 'url': 'https://doi.org/10.1145/3132525.3134822', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Goby: A Wearable Swimming Aid for Blind Athletes', 'author': 'Muehlbradt, Annika and Koushik, Varsha and Kane, Shaun K.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134822'}"
HOMER: An Interactive System for Home Based Stroke Rehabilitation,10.1145/3132525.3134807,"Delivering long term, unsupervised stroke rehabilitation in the home is a complex challenge that requires robust, low cost, scalable, and engaging solutions. We present HOMER, an interactive system that uses novel therapy artifacts, a computer vision approach, and a tablet interface to provide users with a flexible solution suitable for home based rehabilitation. HOMER builds on our prior work developing systems for lightly supervised rehabilitation use in the clinic, by identifying key features for functional movement analysis, adopting a simplified classification assessment approach, and supporting transferability of therapy outcomes to daily living experiences through the design of novel rehabilitation artifacts. A small pilot study with unimpaired subjects indicates the potential of the system in effectively assessing movement and establishing a creative environment for training.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'stroke rehabilitation, interactive neuro rehabilitation, home based care, health, assistive technology, aging', 'numpages': '2', 'pages': '379–380', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Delivering long term, unsupervised stroke rehabilitation in the home is a complex challenge that requires robust, low cost, scalable, and engaging solutions. We present HOMER, an interactive system that uses novel therapy artifacts, a computer vision approach, and a tablet interface to provide users with a flexible solution suitable for home based rehabilitation. HOMER builds on our prior work developing systems for lightly supervised rehabilitation use in the clinic, by identifying key features for functional movement analysis, adopting a simplified classification assessment approach, and supporting transferability of therapy outcomes to daily living experiences through the design of novel rehabilitation artifacts. A small pilot study with unimpaired subjects indicates the potential of the system in effectively assessing movement and establishing a creative environment for training.', 'doi': '10.1145/3132525.3134807', 'url': 'https://doi.org/10.1145/3132525.3134807', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'HOMER: An Interactive System for Home Based Stroke Rehabilitation', 'author': 'Kelliher, Aisling and Choi, Jinwoo and Huang, Jia-Bin and Rikakis, Thanassis and Kitani, Kris', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134807'}"
Needs and Challenges of Post-Acute Brain Injury Patients in Understanding Personal Recovery,10.1145/3132525.3134794,"Recovery from brain injury is a complex and longitudinal process consisting of clinical recovery, illness management, and personal recovery. Lack of initiatives to create awareness about expected recovery and tools for helping patients understand their progress make the personal recovery journey challenging and frustrating. In this paper, we discuss results from interviews with five post-acute brain injury patients about their needs and challenges in understanding their personal recovery.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'rehabilitation., personal recovery, patient awareness, brain injury', 'numpages': '2', 'pages': '381–382', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Recovery from brain injury is a complex and longitudinal process consisting of clinical recovery, illness management, and personal recovery. Lack of initiatives to create awareness about expected recovery and tools for helping patients understand their progress make the personal recovery journey challenging and frustrating. In this paper, we discuss results from interviews with five post-acute brain injury patients about their needs and challenges in understanding their personal recovery.', 'doi': '10.1145/3132525.3134794', 'url': 'https://doi.org/10.1145/3132525.3134794', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Needs and Challenges of Post-Acute Brain Injury Patients in Understanding Personal Recovery', 'author': 'Karanam, Yamini and Miller, Andrew and Brady, Erin', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134794'}"
On How Deaf People Might Use Speech to Control Devices,10.1145/3132525.3134821,"Smart devices connected to the Internet are proliferating.To reduce costs of devices that havetraditionally been inexpensive(toasters, microwaves, printers, etc), manyof these devices have chosen to use a speech interface rather than a visual one. This transition has been hastened by the increasing capabilities of speech interfaces,exemplifiedbyproducts likeAmazon Echo and Apple'sSiri.A consequence of these products moving to voice control is that people who are deaf and hard of hearing (DHH) may be unable to use them. In this paper, we briefly introduce two technical approaches we are pursuingfor enabling DHH people to provide input to these devices: (i) human computationworkflows for understanding ""deaf speech,"" and (ii) mobile interfaces that can be instructed to speak on the user's behalf.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'accessibility', 'numpages': '2', 'pages': '383–384', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Smart devices connected to the Internet are proliferating.To reduce costs of devices that havetraditionally been inexpensive(toasters, microwaves, printers, etc), manyof these devices have chosen to use a speech interface rather than a visual one. This transition has been hastened by the increasing capabilities of speech interfaces,exemplifiedbyproducts likeAmazon Echo and Apple\'sSiri.A consequence of these products moving to voice control is that people who are deaf and hard of hearing (DHH) may be unable to use them. In this paper, we briefly introduce two technical approaches we are pursuingfor enabling DHH people to provide input to these devices: (i) human computationworkflows for understanding ""deaf speech,"" and (ii) mobile interfaces that can be instructed to speak on the user\'s behalf.', 'doi': '10.1145/3132525.3134821', 'url': 'https://doi.org/10.1145/3132525.3134821', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'On How Deaf People Might Use Speech to Control Devices', 'author': 'Bigham, Jeffrey P. and Kushalnagar, Raja and Huang, Ting-Hao Kenneth and Flores, Juan Pablo and Savage, Saiph', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134821'}"
Perceptions of Mobile Device Authentication Mechanisms by Individuals who are Blind,10.1145/3132525.3134793,"This paper describes an exploratory study focusing on the methods of mobile authentication currently utilized by individuals who are blind. Perceptions of security are discussed, along with the trade-offs with usability and accessibility. A tactile aid for a mobile authentication interface was introduced to participants to obtain preliminary feedback on its design. The aid was found to offer promise for supporting orientation, which could be used support novice users, and provide assistance when the mobile device must be used privately in public spaces.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tactile, security, mobile devices, blind, authentication', 'numpages': '2', 'pages': '385–386', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper describes an exploratory study focusing on the methods of mobile authentication currently utilized by individuals who are blind. Perceptions of security are discussed, along with the trade-offs with usability and accessibility. A tactile aid for a mobile authentication interface was introduced to participants to obtain preliminary feedback on its design. The aid was found to offer promise for supporting orientation, which could be used support novice users, and provide assistance when the mobile device must be used privately in public spaces.', 'doi': '10.1145/3132525.3134793', 'url': 'https://doi.org/10.1145/3132525.3134793', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Perceptions of Mobile Device Authentication Mechanisms by Individuals who are Blind', 'author': 'Wolf, Flynn and Kuber, Ravi and Aviv, Adam J.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134793'}"
Rapid Prototyping of Accessible Interfaces With Gaze-Contingent Tunnel Vision Simulation,10.1145/3132525.3134803,"Active involvement of users with disabilities is difficult to employ during the iterative stages of the design process due to high costs and effort associated with user studies. This research proposes a user centered design (UCD) strategy to incorporate the use of gaze-contingent tunnel vision simulation with sighted individuals to facilitate rapid prototyping of accessible interfaces. Through three types of validation studies, we examined how our simulation techniques can provide the opportunity for continued evaluation and refinement of the design. Our simulation approach was effective in emulating scanning behaviors caused by tunnel vision along with grasping user feedback to recognize user interface and usability criteria early in the design cycle.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'simulation, rapid prototyping, gaze tracking, accessibility design', 'numpages': '2', 'pages': '387–388', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Active involvement of users with disabilities is difficult to employ during the iterative stages of the design process due to high costs and effort associated with user studies. This research proposes a user centered design (UCD) strategy to incorporate the use of gaze-contingent tunnel vision simulation with sighted individuals to facilitate rapid prototyping of accessible interfaces. Through three types of validation studies, we examined how our simulation techniques can provide the opportunity for continued evaluation and refinement of the design. Our simulation approach was effective in emulating scanning behaviors caused by tunnel vision along with grasping user feedback to recognize user interface and usability criteria early in the design cycle.', 'doi': '10.1145/3132525.3134803', 'url': 'https://doi.org/10.1145/3132525.3134803', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Rapid Prototyping of Accessible Interfaces With Gaze-Contingent Tunnel Vision Simulation', 'author': 'Kamikubo, Rie and Higuchi, Keita and Yonetani, Ryo and Koike, Hideki and Sato, Yoichi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134803'}"
Reaching Out: Investigating Different Modalities to Help People with Visual Impairments Acquire Items,10.1145/3132525.3134817,"We present a lab study of multiple feedback designs for guiding small-scale arm-and-hand movement for people with visual impairments (PVI), so that they can reach out to and grasp an item on a shelf. Little attention has been paid to the guidance of small-scale arm-and-hand movements by PVI, yet this is an essential element of product acquisition in a grocery shopping task and other similar daily activities. We developed a feedback interface that allowed us to explore two types of auditory feedback (speech and tones), haptic vibration feedback, and a combination of both. The result of the study demonstrated that the multi-modal navigational feedback, specifically speech and haptic, was the most effective and preferred mode for small-scale navigation.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'people with visual impairment, multimodal feedback, haptic feedback, auditory feedback', 'numpages': '2', 'pages': '389–390', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We present a lab study of multiple feedback designs for guiding small-scale arm-and-hand movement for people with visual impairments (PVI), so that they can reach out to and grasp an item on a shelf. Little attention has been paid to the guidance of small-scale arm-and-hand movements by PVI, yet this is an essential element of product acquisition in a grocery shopping task and other similar daily activities. We developed a feedback interface that allowed us to explore two types of auditory feedback (speech and tones), haptic vibration feedback, and a combination of both. The result of the study demonstrated that the multi-modal navigational feedback, specifically speech and haptic, was the most effective and preferred mode for small-scale navigation.', 'doi': '10.1145/3132525.3134817', 'url': 'https://doi.org/10.1145/3132525.3134817', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Reaching Out: Investigating Different Modalities to Help People with Visual Impairments Acquire Items', 'author': 'Lee, Sooyeon and Yuan, Chien Wen and Hanrahan, Benjamin V. and Rosson, Mary Beth and Carroll, John M.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134817'}"
Real-time Activity-sensitive Wearable Ankle Edema Monitoring System For Elderly and Visually Impaired Heart Failure Patients,10.1145/3132525.3134792,"Heart failure (HF) is a leading cause of hospital admissions and readmissions, and carries significant morbidity and mortality in the elderly patient population. Worsening ankle edema is an early sign of an acute HF exacerbation and needs to be carefully monitored. We present the AnkleBracelet, a prototype of a real-time, activity-sensitive, wearable assistive device for monitoring ankle edema specifically designed for elderly, blind, and visually impaired patients. This device provides unobtrusive data regarding changes in ankle edema to encourage behavioral change through multimodal feedback and interaction. Data can be transmitted remotely to maintain a constant connection between physicians and their patients. This device has a patient-centered design that is evidence-based and was derived from a series of collaborations with cardiologists at Rush University Medical Center.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearables, multimodal, heart failure, haptic feedback, elderly, blind and visually impaired, assistive technology, accessibility', 'numpages': '2', 'pages': '391–392', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Heart failure (HF) is a leading cause of hospital admissions and readmissions, and carries significant morbidity and mortality in the elderly patient population. Worsening ankle edema is an early sign of an acute HF exacerbation and needs to be carefully monitored. We present the AnkleBracelet, a prototype of a real-time, activity-sensitive, wearable assistive device for monitoring ankle edema specifically designed for elderly, blind, and visually impaired patients. This device provides unobtrusive data regarding changes in ankle edema to encourage behavioral change through multimodal feedback and interaction. Data can be transmitted remotely to maintain a constant connection between physicians and their patients. This device has a patient-centered design that is evidence-based and was derived from a series of collaborations with cardiologists at Rush University Medical Center.', 'doi': '10.1145/3132525.3134792', 'url': 'https://doi.org/10.1145/3132525.3134792', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Real-time Activity-sensitive Wearable Ankle Edema Monitoring System For Elderly and Visually Impaired Heart Failure Patients', 'author': 'Manshad, Ahmad S. and Manshad, Muhanad S. and Manshad, Sara S.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134792'}"
Recognizing Clothing Colors and Visual Textures Using a Finger-Mounted Camera: An Initial Investigation,10.1145/3132525.3134805,"We investigate clothing color and visual texture recognition using images from a finger-mounted camera to support people with visual impairments. Our approach mitigates issues with distance and lighting that can impact the accuracy of existing color and texture recognizers and allows for easy touch-based interrogation to better understand clothing appearance. We classify image textures by combining two off-the-shelf techniques commonly used for object recognition achieving 99.4\% accuracy on a dataset of 520 clothing images across 9 texture categories. We close with a discussion of potential applications, user evaluation plans, and open questions.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearables, visually impaired, texture recognition, blind', 'numpages': '2', 'pages': '393–394', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We investigate clothing color and visual texture recognition using images from a finger-mounted camera to support people with visual impairments. Our approach mitigates issues with distance and lighting that can impact the accuracy of existing color and texture recognizers and allows for easy touch-based interrogation to better understand clothing appearance. We classify image textures by combining two off-the-shelf techniques commonly used for object recognition achieving 99.4\\% accuracy on a dataset of 520 clothing images across 9 texture categories. We close with a discussion of potential applications, user evaluation plans, and open questions.', 'doi': '10.1145/3132525.3134805', 'url': 'https://doi.org/10.1145/3132525.3134805', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Recognizing Clothing Colors and Visual Textures Using a Finger-Mounted Camera: An Initial Investigation', 'author': 'Medeiros, Alexander J. and Stearns, Lee and Findlater, Leah and Chen, Chuan and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134805'}"
Sign'Maths: An Interactive Dictionary in French Sign Language,10.1145/3132525.3134804,"Sign'Maths is an online dictionary that gives deaf students access to mathematical definitions in Sign Language. In this study, we investigate a new approach that uses Sign Language as a means to navigate and access online content.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'sign language, ict in education, deaf education', 'numpages': '2', 'pages': '395–396', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Sign'Maths is an online dictionary that gives deaf students access to mathematical definitions in Sign Language. In this study, we investigate a new approach that uses Sign Language as a means to navigate and access online content."", 'doi': '10.1145/3132525.3134804', 'url': 'https://doi.org/10.1145/3132525.3134804', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': ""Sign'Maths: An Interactive Dictionary in French Sign Language"", 'author': 'Nadal, Camille and Collet, Christophe', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134804'}"
TacTILE: A Preliminary Toolchain for Creating Accessible Graphics with 3D-Printed Overlays and Auditory Annotations,10.1145/3132525.3134818,"Tactile overlays with audio annotations can increase the accessibility of touchscreens for blind users; however, preparing these overlays is complex and labor intensive. We introduce TacTILE, a novel toolchain to more easily create tactile overlays with audio annotations for arbitrary touchscreen graphics (e.g., graphs, pictures, maps). The workflow includes: (i) an annotation tool to add audio to graphical elements, (ii) a fabrication process that generates 3D-printed tactile overlays, and (iii) a custom app for the user to explore graphics with these overlays. We close with a pilot study with one blind participant who explores three examples (floor plan, photo, and chart), and a discussion of future work.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'visual impairments, touchscreens., tactile overlays, speech, blind users, accessible graphics, 3d printing', 'numpages': '2', 'pages': '397–398', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Tactile overlays with audio annotations can increase the accessibility of touchscreens for blind users; however, preparing these overlays is complex and labor intensive. We introduce TacTILE, a novel toolchain to more easily create tactile overlays with audio annotations for arbitrary touchscreen graphics (e.g., graphs, pictures, maps). The workflow includes: (i) an annotation tool to add audio to graphical elements, (ii) a fabrication process that generates 3D-printed tactile overlays, and (iii) a custom app for the user to explore graphics with these overlays. We close with a pilot study with one blind participant who explores three examples (floor plan, photo, and chart), and a discussion of future work.', 'doi': '10.1145/3132525.3134818', 'url': 'https://doi.org/10.1145/3132525.3134818', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'TacTILE: A Preliminary Toolchain for Creating Accessible Graphics with 3D-Printed Overlays and Auditory Annotations', 'author': 'He, Liang and Wan, Zijian and Findlater, Leah and Froehlich, Jon E.', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134818'}"
Teaching Inclusive Thinking in Undergraduate Computing,10.1145/3132525.3134808,"With the increasing importance of accessibility awareness and knowledge as both a moral imperative and an employment differentiator, it is incumbent on educational programs to have demonstrated ability to teach these skills. We report on our year-long evaluation of university students' accessibility awareness and knowledge following a week of accessibility lectures as part of courses on Human-Computer Interaction (HCI). We report gains in awareness and knowledge when accessibility lectures were part of the course. We describe the test battery developed to measure these skills, and describe our ongoing longitudinal research to measure the effectiveness of several interventions for teaching inclusive thinking in undergraduate computing courses.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'ethics, computer science education, accessibility', 'numpages': '2', 'pages': '399–400', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""With the increasing importance of accessibility awareness and knowledge as both a moral imperative and an employment differentiator, it is incumbent on educational programs to have demonstrated ability to teach these skills. We report on our year-long evaluation of university students' accessibility awareness and knowledge following a week of accessibility lectures as part of courses on Human-Computer Interaction (HCI). We report gains in awareness and knowledge when accessibility lectures were part of the course. We describe the test battery developed to measure these skills, and describe our ongoing longitudinal research to measure the effectiveness of several interventions for teaching inclusive thinking in undergraduate computing courses."", 'doi': '10.1145/3132525.3134808', 'url': 'https://doi.org/10.1145/3132525.3134808', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Teaching Inclusive Thinking in Undergraduate Computing', 'author': 'Palan, Nidhi Rajendra and Hanson, Vicki L. and Huenerfauth, Matt and Ludi, Stephanie', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134808'}"
The Accessibility of MOOCs for Blind Learners,10.1145/3132525.3134796,"The development of Massive Open Online Courses (MOOCs) has provided unique learning opportunities for many people. In the meantime, the lack of accessibility in some MOOCs has also created barriers for diverse learners. In this paper, we present the preliminary results from a study on the accessibility a selected set of MOOCs on the Coursera platform for blind learners who use screen readers and Braille to interact with computers and mobile devices.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'screen reader, massive open online course (mooc), braille., blind, accessibility', 'numpages': '2', 'pages': '401–402', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The development of Massive Open Online Courses (MOOCs) has provided unique learning opportunities for many people. In the meantime, the lack of accessibility in some MOOCs has also created barriers for diverse learners. In this paper, we present the preliminary results from a study on the accessibility a selected set of MOOCs on the Coursera platform for blind learners who use screen readers and Braille to interact with computers and mobile devices.', 'doi': '10.1145/3132525.3134796', 'url': 'https://doi.org/10.1145/3132525.3134796', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'The Accessibility of MOOCs for Blind Learners', 'author': ""Kr\\'{o}lak, Aleksandra and Chen, Weiqin and Sanderson, Norun C. and Kessel, Siri"", 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134796'}"
Towards Brain-Computer Interface (BCI) and Gestural-Based Authentication for Individuals who are Blind,10.1145/3132525.3134785,"This paper describes an exploratory study examining the feasibility of using Brain-Computer Interface (BCI) and gestural technologies to support individuals who are blind during the authentication process. Four legally-blind participants were asked to don the Emotiv Epoc headset, and authenticate entry using gestural cues, emotional cues and mental commands. Findings highlighted that while BCI and gestural technologies may be slower and less accurate to use compared to four digit PINs, levels of perceived security were higher, as some of these cues were thought to be more difficult for third parties to replicate. A trade-off between perceived security and usability was evident.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'gestural technologies, brain-computer interface technologies, blind, authentication', 'numpages': '2', 'pages': '403–404', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This paper describes an exploratory study examining the feasibility of using Brain-Computer Interface (BCI) and gestural technologies to support individuals who are blind during the authentication process. Four legally-blind participants were asked to don the Emotiv Epoc headset, and authenticate entry using gestural cues, emotional cues and mental commands. Findings highlighted that while BCI and gestural technologies may be slower and less accurate to use compared to four digit PINs, levels of perceived security were higher, as some of these cues were thought to be more difficult for third parties to replicate. A trade-off between perceived security and usability was evident.', 'doi': '10.1145/3132525.3134785', 'url': 'https://doi.org/10.1145/3132525.3134785', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Towards Brain-Computer Interface (BCI) and Gestural-Based Authentication for Individuals who are Blind', 'author': 'Saulynas, Sidas and Kuber, Ravi', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134785'}"
User Experiences When Testing a Messaging App for Communication Between Individuals who are Hearing and Deaf or Hard of Hearing,10.1145/3132525.3134798,"This study investigated user experiences of participants testing a prototype messaging app with automatic speech recognition (ASR). Twelve pairs of participants, where one individual was deaf or hard-of-hearing (DHH), and the other one was hearing used the app, with the hearing individual using speech and ASR and the DHH one using typing. Participants completed a standardized decision making task to test the app. Regardless of hearing status of the participants or the type of device used, participants were generally satisfied with the app. These findings indicate that ASR has potential to facilitate communication between DHH and hearing individuals in small groups and that the technology merits further investigation.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'small groups, mobile devices, hard-of-hearing, deaf, communication, automatic speech recognition', 'numpages': '2', 'pages': '405–406', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This study investigated user experiences of participants testing a prototype messaging app with automatic speech recognition (ASR). Twelve pairs of participants, where one individual was deaf or hard-of-hearing (DHH), and the other one was hearing used the app, with the hearing individual using speech and ASR and the DHH one using typing. Participants completed a standardized decision making task to test the app. Regardless of hearing status of the participants or the type of device used, participants were generally satisfied with the app. These findings indicate that ASR has potential to facilitate communication between DHH and hearing individuals in small groups and that the technology merits further investigation.', 'doi': '10.1145/3132525.3134798', 'url': 'https://doi.org/10.1145/3132525.3134798', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'User Experiences When Testing a Messaging App for Communication Between Individuals who are Hearing and Deaf or Hard of Hearing', 'author': 'Elliot, Lisa B. and Stinson, Michael and Ahmed, Syed and Easton, Donna', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134798'}"
Using Automatic Speech Recognition to Facilitate Communication Between an Individual who is Hearing and One who is Deaf or Hard of Hearing,10.1145/3132525.3134797,"This study investigated use of automatic speech recognition (ASR) in 12 pairs where one individual was deaf or hard-of-hearing (DHH), and the other one was hearing, with the hearing individual using speech and ASR and the DHH one using typing. Each of the pairs used prototype software for messaging to communicate while completing a standardized decision making task. Results suggested that ASR produced text at a faster rate than a keyboard. When both participants used keyboards, they exchanged more messages than when one or both of them used a smartphone with a miniature keyboard.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'small groups, hard-of-hearing, deaf, communication, automatic speech recognition', 'numpages': '2', 'pages': '407–408', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This study investigated use of automatic speech recognition (ASR) in 12 pairs where one individual was deaf or hard-of-hearing (DHH), and the other one was hearing, with the hearing individual using speech and ASR and the DHH one using typing. Each of the pairs used prototype software for messaging to communicate while completing a standardized decision making task. Results suggested that ASR produced text at a faster rate than a keyboard. When both participants used keyboards, they exchanged more messages than when one or both of them used a smartphone with a miniature keyboard.', 'doi': '10.1145/3132525.3134797', 'url': 'https://doi.org/10.1145/3132525.3134797', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Using Automatic Speech Recognition to Facilitate Communication Between an Individual who is Hearing and One who is Deaf or Hard of Hearing', 'author': 'Stinson, Michael and Ahmed, Syed and Elliot, Lisa and Easton, Donna', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134797'}"
COSMA: Cooperative Self-Management Tool for Adolescents with Autism,10.1145/3132525.3134825,"Adolescence is a challenging period for individuals with autism because they undergo radical physical, emotional, and social transitions. We describe our setup to support the self-management of adolescents with autism for assisting adaptive transitions. We propose COSMA, an interactive mobile application that allows both individuals with autism and their caregivers to cooperatively manage, plan, reflect on, and improve behavior. COSMA is a self- management tool for adolescents with autism, including behavioral goal setting by means of the co-contract process, self-reporting while performing everyday tasks, and cooperative reflection to support their smooth transition to adulthood.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'self-management, puberty, behavior change, autism, adolescents', 'numpages': '2', 'pages': '409–410', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Adolescence is a challenging period for individuals with autism because they undergo radical physical, emotional, and social transitions. We describe our setup to support the self-management of adolescents with autism for assisting adaptive transitions. We propose COSMA, an interactive mobile application that allows both individuals with autism and their caregivers to cooperatively manage, plan, reflect on, and improve behavior. COSMA is a self- management tool for adolescents with autism, including behavioral goal setting by means of the co-contract process, self-reporting while performing everyday tasks, and cooperative reflection to support their smooth transition to adulthood.', 'doi': '10.1145/3132525.3134825', 'url': 'https://doi.org/10.1145/3132525.3134825', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'COSMA: Cooperative Self-Management Tool for Adolescents with Autism', 'author': 'Ryu, Myeonghan and Jo, Eunkyung and Kim, Sung-In', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134825'}"
Crowdsourcing the Installation and Maintenance of Indoor Navigation Infrastructure,10.1145/3132525.3134827,"Indoor navigation systems, especially for people with visual impairments, often require intensive data collection and instrumentation to install and maintain over time. The most accurate navigation systems require experts to install many Bluetooth beacons or similar technologies throughout the environment. After hardware installation, data samples must be collected to construct a signal model of the environment to be navigated. The demands of this intense upfront workload and ongoing updates pose barriers for widespread adoption of navigation systems. This document describes LuzDeploy, a system to break these installation and maintenance workflows into small, simple tasks that be completed by volunteers with little to no training. A Facebook Messenger bot coordinates the overall deployment by assigning volunteers to beacon placement, data collection, or quality assurance tasks that take only a few minutes to complete. These tasks may be batched to do more or less work, depending on the amount of time the volunteer is able to contribute. LuzDeploy allows building owners and managers to distribute the difficult task of installation and maintenance. By making these workloads easier for non-experts, LuzDeploy aims to increase adoption of indoor navigation systems and make unfamiliar spaces more accessible to people who wish to navigate independently.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'physical crowdsourcing, indoor navigation assistance, individuals with visual impairments', 'numpages': '2', 'pages': '411–412', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'Indoor navigation systems, especially for people with visual impairments, often require intensive data collection and instrumentation to install and maintain over time. The most accurate navigation systems require experts to install many Bluetooth beacons or similar technologies throughout the environment. After hardware installation, data samples must be collected to construct a signal model of the environment to be navigated. The demands of this intense upfront workload and ongoing updates pose barriers for widespread adoption of navigation systems. This document describes LuzDeploy, a system to break these installation and maintenance workflows into small, simple tasks that be completed by volunteers with little to no training. A Facebook Messenger bot coordinates the overall deployment by assigning volunteers to beacon placement, data collection, or quality assurance tasks that take only a few minutes to complete. These tasks may be batched to do more or less work, depending on the amount of time the volunteer is able to contribute. LuzDeploy allows building owners and managers to distribute the difficult task of installation and maintenance. By making these workloads easier for non-experts, LuzDeploy aims to increase adoption of indoor navigation systems and make unfamiliar spaces more accessible to people who wish to navigate independently.', 'doi': '10.1145/3132525.3134827', 'url': 'https://doi.org/10.1145/3132525.3134827', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Crowdsourcing the Installation and Maintenance of Indoor Navigation Infrastructure', 'author': 'Gleason, Cole', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134827'}"
Identifying Visual Cues to Improve Independent IndoorNavigation for Blind Individuals,10.1145/3132525.3134828,"The idea of using technology to help those with visual impairments navigate has been studied extensively. However, most of these systems focus on getting the user from place to place, rather than helping the person get a better sense and intuition of their environment. Providing blind people with the same intuitive clues that sighted persons have may allow them to better navigate physical spaces, and also feel more empowered to freely explore the physical location. For this purpose, we have begun to study the process that sighted individuals use for familiarizing and getting a sense of their environment. We believe our results will show it is possible to enhance the navigational capabilities of blind people by providing access to the same clues used by sighted people to get a sense of their environment.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'wearable technology, navigation, localization, indoor positioning system, computer vision', 'numpages': '2', 'pages': '413–414', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'The idea of using technology to help those with visual impairments navigate has been studied extensively. However, most of these systems focus on getting the user from place to place, rather than helping the person get a better sense and intuition of their environment. Providing blind people with the same intuitive clues that sighted persons have may allow them to better navigate physical spaces, and also feel more empowered to freely explore the physical location. For this purpose, we have begun to study the process that sighted individuals use for familiarizing and getting a sense of their environment. We believe our results will show it is possible to enhance the navigational capabilities of blind people by providing access to the same clues used by sighted people to get a sense of their environment.', 'doi': '10.1145/3132525.3134828', 'url': 'https://doi.org/10.1145/3132525.3134828', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Identifying Visual Cues to Improve Independent IndoorNavigation for Blind Individuals', 'author': 'Cassidy, Cameron Tyler', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134828'}"
Participatory Design Using Sensory Substitution Devices with Tactile and Audio Feedback,10.1145/3132525.3134826,"Blind or visually impaired persons may experience challenges forming a mental map of the world around them. Our goal, in this participatory study, was to bridge the gap found in current technologies for a blind woman by creating a model utilizing tactile feedback. To achieve this goal, we designed three prototype sensory substitution systems. These prototypes attempted to aid our participant in crafting a world around her by providing auditory or tactile feedback in accordance to her proximity to an object. Each used a different form of output for the user: audio, vibration, or pressure. All of the stimuli produced by the models was managed by a researcher in place of a computer-vision algorithm. From our participant's feedback, we hope to develop a device that will provide the user a physical sensory substitution experience of sight.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'tactile feedback, sensory substitution, participatory design, blind individuals, accessibilty', 'numpages': '2', 'pages': '415–416', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Blind or visually impaired persons may experience challenges forming a mental map of the world around them. Our goal, in this participatory study, was to bridge the gap found in current technologies for a blind woman by creating a model utilizing tactile feedback. To achieve this goal, we designed three prototype sensory substitution systems. These prototypes attempted to aid our participant in crafting a world around her by providing auditory or tactile feedback in accordance to her proximity to an object. Each used a different form of output for the user: audio, vibration, or pressure. All of the stimuli produced by the models was managed by a researcher in place of a computer-vision algorithm. From our participant's feedback, we hope to develop a device that will provide the user a physical sensory substitution experience of sight."", 'doi': '10.1145/3132525.3134826', 'url': 'https://doi.org/10.1145/3132525.3134826', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Participatory Design Using Sensory Substitution Devices with Tactile and Audio Feedback', 'author': 'Tuson, Ella and Hughson, Samantha and Zymaris, Christina and King, Ryan', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134826'}"
VocalIDE: An IDE for Programming via Speech Recognition,10.1145/3132525.3134824,"We believe that developing tools using a publicly available speech recognition API can provide a basis for keyboard free programming. We set out to develop a specialized voice-based IDE for this purpose. Early research revealed the strengths and limitations of current vocal text editors and environments. We report on a Wizard of Oz (WOz) experiment we conducted asking participants to edit code using vocal instructions given to a ""human computer."" Following the study, we developed a prototype, VocalIDE, which implements specialized editing mechanisms to better facilitate vocal programming.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'vocal programming, upper-limb impairment, speech recognition, ide', 'numpages': '2', 'pages': '417–418', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'We believe that developing tools using a publicly available speech recognition API can provide a basis for keyboard free programming. We set out to develop a specialized voice-based IDE for this purpose. Early research revealed the strengths and limitations of current vocal text editors and environments. We report on a Wizard of Oz (WOz) experiment we conducted asking participants to edit code using vocal instructions given to a ""human computer."" Following the study, we developed a prototype, VocalIDE, which implements specialized editing mechanisms to better facilitate vocal programming.', 'doi': '10.1145/3132525.3134824', 'url': 'https://doi.org/10.1145/3132525.3134824', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'VocalIDE: An IDE for Programming via Speech Recognition', 'author': 'Rosenblatt, Lucas', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134824'}"
Personal Perspectives on Using Automatic Speech Recognition to Facilitate Communication between Deaf Students and Hearing Customers,10.1145/3132525.3134779,Automatic Speech Recognition (ASR) and the wide use of smart phones and their apps have allowed huge inroads when preparing deaf and hard-of-hearing (D/HH) students to be effective and productive in the hearing workplace. This paper presents both a hearing instructor's experiences and a deaf researcher's observations when preparing deaf and hard of hearing students as computer technicians for the hearing work place.,"{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'whatsapp, technology, smart phones, small group interaction, research, phone apps, deaf in workplace. automatic speech recognition, deaf education, deaf and hard of hearing', 'numpages': '3', 'pages': '419–421', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""Automatic Speech Recognition (ASR) and the wide use of smart phones and their apps have allowed huge inroads when preparing deaf and hard-of-hearing (D/HH) students to be effective and productive in the hearing workplace. This paper presents both a hearing instructor's experiences and a deaf researcher's observations when preparing deaf and hard of hearing students as computer technicians for the hearing work place."", 'doi': '10.1145/3132525.3134779', 'url': 'https://doi.org/10.1145/3132525.3134779', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Personal Perspectives on Using Automatic Speech Recognition to Facilitate Communication between Deaf Students and Hearing Customers', 'author': 'Mallory, James R. and Stinson, Michael and Elliot, Lisa and Easton, Donna', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134779'}"
Cyborg Pride: Self-Design in e-NABLE,10.1145/3132525.3134780,"The e-NABLE community is a global, distributed, loosely coordinated effort to design, produce, and deliver upper-limb assistive technology to those in need. e-NABLE's volunteers are often 3D-printing enthusiasts who are driven to use their skills to improve the lives of others. Volunteers provide the devices they produce to recipients around the world; these recipients are mainly children, but some are adults as well. While e-NABLE and its participants have received much attention from the media, little has been written about the experiences of the e-NABLE device users themselves. In this paper, we document the experiences of one user of an e-NABLE hand, detailing in a first-person account the journey from a recipient to an active participant and advocate for self-efficacy.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'prostheses, e-nable, diy-at, cyborg pride, 3d printing', 'numpages': '5', 'pages': '422–426', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': ""The e-NABLE community is a global, distributed, loosely coordinated effort to design, produce, and deliver upper-limb assistive technology to those in need. e-NABLE's volunteers are often 3D-printing enthusiasts who are driven to use their skills to improve the lives of others. Volunteers provide the devices they produce to recipients around the world; these recipients are mainly children, but some are adults as well. While e-NABLE and its participants have received much attention from the media, little has been written about the experiences of the e-NABLE device users themselves. In this paper, we document the experiences of one user of an e-NABLE hand, detailing in a first-person account the journey from a recipient to an active participant and advocate for self-efficacy."", 'doi': '10.1145/3132525.3134780', 'url': 'https://doi.org/10.1145/3132525.3134780', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Cyborg Pride: Self-Design in e-NABLE', 'author': 'Hawthorn, Peregrine and Ashbrook, Daniel', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134780'}"
"Deaf, Hard of Hearing, and Hearing Perspectives on Using Automatic Speech Recognition in Conversation",10.1145/3132525.3134781,"This experience report describes the accessibility challenges in using the top seven most popular Automatic Speech Recognition (ASR) applications on personal devices for commands and group conversation, by five deaf, hard of hearing and hearing participants, including the authors. The report discusses the most common use cases, their challenges, and best practices plus pitfalls to avoid in using personal devices with ASR for commands or conversation.","{'series': ""ASSETS '17"", 'location': 'Baltimore, Maryland, USA', 'keywords': 'hearing, hard of hearing, deaf, automatic speech recognition', 'numpages': '6', 'pages': '427–432', 'booktitle': 'Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility', 'abstract': 'This experience report describes the accessibility challenges in using the top seven most popular Automatic Speech Recognition (ASR) applications on personal devices for commands and group conversation, by five deaf, hard of hearing and hearing participants, including the authors. The report discusses the most common use cases, their challenges, and best practices plus pitfalls to avoid in using personal devices with ASR for commands or conversation.', 'doi': '10.1145/3132525.3134781', 'url': 'https://doi.org/10.1145/3132525.3134781', 'address': 'New York, NY, USA', 'publisher': 'Association for Computing Machinery', 'isbn': '9781450349260', 'year': '2017', 'title': 'Deaf, Hard of Hearing, and Hearing Perspectives on Using Automatic Speech Recognition in Conversation', 'author': 'Glasser, Abraham and Kushalnagar, Kesavan and Kushalnagar, Raja', 'ENTRYTYPE': 'inproceedings', 'ID': '10.1145/3132525.3134781'}"