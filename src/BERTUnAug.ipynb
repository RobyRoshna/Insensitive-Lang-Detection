{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1iC5maqAFPPi6XSRlNPofXaD2KISBKP6P",
      "authorship_tag": "ABX9TyPu9YiLTSarBaFhsG/jGo8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobyRoshna/Insensitive-Lang-Detection/blob/streamlitapp/src/BERTUnAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and custom\n",
        "\n"
      ],
      "metadata": {
        "id": "AUBs-3GaSEFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_vZxv5xB2Vx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, set_seed\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "JWKIgtDkX68Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: random_state for splits need to set separately for this study its 42 throughout\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "dfH8RRFnB9zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for Tokenization\n",
        "class SentenceDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels): #labels: List of labels corresponding to the sentences (e.g., 0 for NotInsensitive, 1 for Insensitive).\n",
        "\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #For pytorch tensors\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Function to compute metrics\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    preds = predictions.argmax(axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "Kyudyx_nCAyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Prep"
      ],
      "metadata": {
        "id": "Hzka2GzDTIS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "file1 = \"/content/drive/MyDrive/Honours MiscData(Roshna)/25Augmented_annotationsV2.csv\"\n",
        "file2 = \"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\"\n",
        "\n",
        "# Load CSV files\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "\n",
        "# Ensure the 'Sentence' column exists in both files\n",
        "if 'Sentence' not in df1.columns or 'Sentence' not in df2.columns:\n",
        "    raise ValueError(\"One of the files does not contain a 'Sentence' column.\")\n",
        "\n",
        "# Convert 'Sentence' columns to strings for consistency\n",
        "df1['Sentence'] = df1['Sentence'].astype(str)\n",
        "df2['Sentence'] = df2['Sentence'].astype(str)\n",
        "\n",
        "# Create a dictionary mapping sentences to their actual index in File2 (df2)\n",
        "sentence_to_index_file2 = df2.set_index('Sentence').index.to_series().to_dict()\n",
        "\n",
        "# Find matching sentences and store the actual 1-based indices\n",
        "matching_records = []\n",
        "for index1, sentence in df1['Sentence'].items():  # index1 is zero-based\n",
        "    if sentence in sentence_to_index_file2:\n",
        "        index2 = df2[df2['Sentence'] == sentence].index[0]  # Get the correct index from df2\n",
        "        matching_records.append((index2 + 2, index1 + 2, sentence))  # Convert to 1-based row numbers\n",
        "\n",
        "# Convert results to DataFrame\n",
        "matching_df = pd.DataFrame(matching_records, columns=['File2 Row', 'File1 Row', 'Sentence'])\n",
        "\n",
        "# Print total count of matches\n",
        "print(f\"Total Matching Sentences: {len(matching_df)}\\n\")\n",
        "\n",
        "# Print the first few matches\n",
        "print(\"Matching Records (File2 Row -> File1 Row -> Sentence):\")\n",
        "print(matching_df.head(20).to_string(index=False))  # Show first 20 matches neatly\n"
      ],
      "metadata": {
        "id": "fo7-XFl9QZIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The annotated dataset\n",
        "file_path = '/content/drive/MyDrive/Honours MiscData(Roshna)/deduplicated_annotations.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# cleaning data\n",
        "data = data[['Sentence', 'Manual_Annotation', 'Matched_Terms']]\n",
        "data = data.dropna()\n",
        "\n",
        "# 1 for insensitive and 0 for notInsensitive\n",
        "data['Manual_Annotation'] = data['Manual_Annotation'].apply(lambda x: 1 if x.lower() == 'insensitive' else 0)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, stratify=data['Manual_Annotation'], random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['Manual_Annotation'], random_state=42)\n",
        "\n",
        "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n",
        "# Save train, val, and test splits\n",
        "train_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/train_dataOriginal.csv\", index=False)\n",
        "val_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/val_dataOriginal.csv\", index=False)\n",
        "test_data.to_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\", index=False)\n",
        "print(\"Dataset splits saved!\")\n"
      ],
      "metadata": {
        "id": "9OvFkNQ_CFMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "def tokenize_data(data, tokenizer, max_length=109):\n",
        "    return tokenizer(\n",
        "        list(data['Sentence']),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length= max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_labels = list(train_data['Manual_Annotation'])\n",
        "val_labels = list(val_data['Manual_Annotation'])\n",
        "test_labels = list(test_data['Manual_Annotation'])\n",
        "\n",
        "train_encodings = tokenize_data(train_data, tokenizer)\n",
        "val_encodings = tokenize_data(val_data, tokenizer)\n",
        "test_encodings = tokenize_data(test_data, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_encodings_input_ids.npy\", train_encodings['input_ids'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_encodings_attention_mask.npy\", train_encodings['attention_mask'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_labels.npy\", np.array(train_labels))\n"
      ],
      "metadata": {
        "id": "53KyaBceFCZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#datasets for train, validation, and test sets\n",
        "train_datasetOriginalV2 = SentenceDataset(train_encodings, train_labels)\n",
        "val_datasetOriginalV2 = SentenceDataset(val_encodings, val_labels)\n",
        "test_datasetOriginalV2 = SentenceDataset(test_encodings, test_labels)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/train_datasetOriginalV2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_datasetOriginalV2, f)\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/val_datasetOriginalV2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(val_datasetOriginalV2, f)\n",
        "with open(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_datasetOriginalV2.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_datasetOriginalV2, f)\n"
      ],
      "metadata": {
        "id": "BTY-WL4dFg5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original BERT Model"
      ],
      "metadata": {
        "id": "kPCK7NXkTRA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "modelBbaseV2Original = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n"
      ],
      "metadata": {
        "id": "aehZqjqZE5-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wandb.finish()\n",
        "\n",
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_baseOrgininalV2\")\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=[\"wandb\"],  # WandB is used for logging\n",
        "    run_name=\"Bert_base\"  # the run name for this Trainer\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=modelBbaseV2Original,\n",
        "    args=training_args,\n",
        "    train_dataset=train_datasetOriginalV2,\n",
        "    eval_dataset=val_datasetOriginalV2,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Ensure to include training metrics at the end of training\n",
        "train_results = trainer.predict(train_datasetOriginalV2)\n",
        "\n",
        "\n",
        "predictions = train_results.predictions\n",
        "labels = train_results.label_ids\n",
        "\n",
        "# Compute metrics using the compute_metrics function\n",
        "train_metrics = compute_metrics((predictions, labels))\n",
        "\n",
        "wandb.log({f\"train_{key}\": value for key, value in train_metrics.items()})\n",
        "\n",
        "# Output the metrics at the end of training needed for loos tredns\n",
        "print(\"Training Metrics:\")\n",
        "for key, value in train_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Compute evaluation metrics on the test set\n",
        "test_results = trainer.evaluate(test_datasetOriginalV2, metric_key_prefix=\"test\")\n",
        "wandb.log(test_results)\n",
        "\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "Y7GKtpUfEyc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_bert_base_model\"\n"
      ],
      "metadata": {
        "id": "YTLTJnmQOwZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Trends"
      ],
      "metadata": {
        "id": "kBEZoey3TeCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compare Train, Validation, and Test Metrics reported during training above\n",
        "val_metrics = {\n",
        "    \"accuracy\": 0.97222,\n",
        "    \"f1\": 0.96703,\n",
        "    \"precision\": 0.97778,\n",
        "    \"recall\": 0.95652,\n",
        "    \"loss\": 0.11905,\n",
        "    \"runtime\": 0.6407,\n",
        "    \"samples_per_second\": 168.572,\n",
        "    \"steps_per_second\": 10.926,\n",
        "}\n",
        "train_metrics = {\n",
        "    \"accuracy\": 0.99423,\n",
        "    \"f1\": 0.99323,\n",
        "    \"precision\": 0.99189,\n",
        "    \"recall\": 0.99458,\n",
        "    \"loss\": 0.0405,\n",
        "    \"global_step\": 165,\n",
        "    \"grad_norm\": 0.07208,\n",
        "    \"learning_rate\": 0.0,\n",
        "    \"train_loss\": 0.18936,\n",
        "    \"runtime\": 90.4431,\n",
        "    \"samples_per_second\": 28.758,\n",
        "    \"steps_per_second\": 1.824,\n",
        "}\n",
        "test_metrics = {\n",
        "    \"accuracy\": 0.92661,\n",
        "    \"f1\": 0.90698,\n",
        "    \"precision\": 0.975,\n",
        "    \"recall\": 0.84783,\n",
        "    \"loss\": 0.28281,\n",
        "    \"runtime\": 0.4874,\n",
        "    \"samples_per_second\": 223.643,\n",
        "    \"steps_per_second\": 14.362,\n",
        "}\n",
        "\n",
        "print(\"Train Metrics:\", train_metrics)\n",
        "print(\"Validation Metrics:\", val_metrics)\n",
        "print(\"Test Metrics:\", test_metrics)\n",
        "\n",
        "epochs = [1, 2, 3]\n",
        "train_losses = [0.241200, 0.169700, 0.040500]\n",
        "val_losses = [0.169273, 0.122286, 0.119051]\n",
        "\n",
        "tl = plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
        "vl = plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Trends\")\n",
        "plt.legend()\n",
        "plt.savefig(\"Originalloss_trends\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Jp0yYmcvUnGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "tOBez5R6Tfu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "import wandb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_bert_base_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detecton\", entity=\"Roshna\", name=\"Bert_baseOripred\")\n",
        "\n",
        "# Tokenize again\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_data['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)  # probabilities\n",
        "\n",
        "test_predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "test_true_labels = test_data['Manual_Annotation'].values\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n",
        "\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                              columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                              index=[\"Actual NotInsensitive\", \"Actual Insensitive\"])\n",
        "\n",
        "print(\"Confusion Matrix - Test Set:\")\n",
        "print(conf_matrix_df)\n",
        "\n",
        "# Analyze false positives and false negatives\n",
        "false_positives = test_data[(test_true_labels == 0) & (test_predictions == 1)].copy()\n",
        "false_negatives = test_data[(test_true_labels == 1) & (test_predictions == 0)].copy()\n",
        "\n",
        "print(\"False Positives:\")\n",
        "print(false_positives[['Sentence', 'Manual_Annotation']])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "print(false_negatives[['Sentence', 'Manual_Annotation']])\n",
        "\n",
        "\n",
        "test_data = test_data.copy()\n",
        "test_data['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_data['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "\n",
        "# Assign confidence values to misclassified examples\n",
        "false_positives['Confidence_Positive'] = test_data.loc[false_positives.index, 'Confidence_Positive'].values\n",
        "false_negatives['Confidence_Negative'] = test_data.loc[false_negatives.index, 'Confidence_Negative'].values\n",
        "\n",
        "print(\"False Positives with Confidence:\")\n",
        "print(false_positives[['Sentence', 'Manual_Annotation', 'Confidence_Positive']])\n",
        "\n",
        "print(\"\\nFalse Negatives with Confidence:\")\n",
        "print(false_negatives[['Sentence', 'Manual_Annotation', 'Confidence_Negative']])\n"
      ],
      "metadata": {
        "id": "Kb5tnW6-TjPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP 1: Test augmented test set on model"
      ],
      "metadata": {
        "id": "IPQ1xdu2MjzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/AugTest_dataV2.csv\")\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_bert_base_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detection\", entity=\"Roshna\", name=\"Bert_baseAugpred\")\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_data['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)\n",
        "test_predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "test_true_labels = test_data['Label'].values\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "class_report = classification_report(test_true_labels, test_predictions, output_dict=True)\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n",
        "\n",
        "# Extract Precision, Recall, and F1-score - macro average gives equal importance to both classes in binary classification\n",
        "precision = class_report[\"macro avg\"][\"precision\"]\n",
        "recall = class_report[\"macro avg\"][\"recall\"]\n",
        "f1 = class_report[\"macro avg\"][\"f1-score\"]\n",
        "wandb.log({\n",
        "    \"test_accuracy\": accuracy,\n",
        "    \"test_precision\": precision,\n",
        "    \"test_recall\": recall,\n",
        "    \"test_f1\": f1\n",
        "})\n",
        "\n",
        "print(\"\\nTest Set Evaluation\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                              columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                              index=[\"Actual NotInsensitive\", \"Actual Insensitive\"])\n",
        "\n",
        "print(\"\\nConfusion Matrix - Test Set:\")\n",
        "print(conf_matrix_df)\n",
        "wandb.log({\"confusion_matrix\": wandb.Table(dataframe=conf_matrix_df)})\n",
        "\n",
        "\n",
        "false_positives = test_data[(test_true_labels == 0) & (test_predictions == 1)].copy()\n",
        "false_negatives = test_data[(test_true_labels == 1) & (test_predictions == 0)].copy()\n",
        "print(\"\\nFalse Positives:\")\n",
        "print(false_positives[['Sentence', 'Label']])\n",
        "print(\"\\nFalse Negatives:\")\n",
        "print(false_negatives[['Sentence', 'Label']])\n",
        "test_data['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_data['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "false_positives['Confidence_Positive'] = test_data.loc[false_positives.index, 'Confidence_Positive'].values\n",
        "false_negatives['Confidence_Negative'] = test_data.loc[false_negatives.index, 'Confidence_Negative'].values\n",
        "\n",
        "print(\"\\nFalse Positives with Confidence:\")\n",
        "print(false_positives[['Sentence', 'Label', 'Confidence_Positive']])\n",
        "print(\"\\nFalse Negatives with Confidence:\")\n",
        "print(false_negatives[['Sentence', 'Label', 'Confidence_Negative']])\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"NotInsensitive\", \"Insensitive\"],\n",
        "            yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"AugTest_confusion_matrix\")\n",
        "plt.show()\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "jjsHEUZeMso5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Load test data\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/AugTest_dataV2.csv\")\n",
        "\n",
        "# Load model\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_bert_base_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detection\", entity=\"Roshna\", name=\"Bert_baseAugpred\")\n",
        "\n",
        "# Tokenize input sentences\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_data['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "test_predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "test_true_labels = test_data['Label'].values\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "class_report = classification_report(test_true_labels, test_predictions, output_dict=True)\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n",
        "\n",
        "# Log to Weights & Biases\n",
        "wandb.log({\n",
        "    \"test_accuracy\": accuracy,\n",
        "    \"test_precision\": class_report[\"macro avg\"][\"precision\"],\n",
        "    \"test_recall\": class_report[\"macro avg\"][\"recall\"],\n",
        "    \"test_f1\": class_report[\"macro avg\"][\"f1-score\"]\n",
        "})\n",
        "\n",
        "# Extract false negatives (Actual: 1, Predicted: 0)\n",
        "false_negatives = test_data[(test_true_labels == 1) & (test_predictions == 0)].copy()\n",
        "test_data['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_data['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "\n",
        "# Add confidence scores to false negatives\n",
        "false_negatives['Confidence_Negative'] = test_data.loc[false_negatives.index, 'Confidence_Negative'].values\n",
        "\n",
        "# Save false negatives to CSV\n",
        "false_negatives.to_csv(\"false_negatives.csv\", index=False)\n",
        "print(\"\\nFalse negatives saved to false_negatives.csv\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"NotInsensitive\", \"Insensitive\"],\n",
        "            yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.savefig(\"AugTest_confusion_matrix\")\n",
        "plt.show()\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "CjXX-u0v8msm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXP 2: Test GPT test set on unaugmented model"
      ],
      "metadata": {
        "id": "rI9NaRXfY-ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Load pure GPT test set\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/ChatGPTtest_data.csv\")\n",
        "\n",
        "# Load the unaug model\n",
        "model_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_bert_base_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detection\", entity=\"Roshna\", name=\"Bert_basegptpred\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "inputs = tokenizer(list(test_data['Sentence']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    confidences = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "test_predictions = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "test_true_labels = test_data['Label'].values\n",
        "\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "class_report = classification_report(test_true_labels, test_predictions, output_dict=True)\n",
        "conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n",
        "\n",
        "# Extract Precision, Recall, and F1-score- macro average gives equal importance to both classes in binary classification\n",
        "precision = class_report[\"macro avg\"][\"precision\"]\n",
        "recall = class_report[\"macro avg\"][\"recall\"]\n",
        "f1 = class_report[\"macro avg\"][\"f1-score\"]\n",
        "\n",
        "wandb.log({\n",
        "    \"test_accuracy\": accuracy,\n",
        "    \"test_precision\": precision,\n",
        "    \"test_recall\": recall,\n",
        "    \"test_f1\": f1\n",
        "})\n",
        "\n",
        "\n",
        "print(\"\\nTest Set Evaluation\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
        "                              columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                              index=[\"Actual NotInsensitive\", \"Actual Insensitive\"])\n",
        "\n",
        "print(\"\\nConfusion Matrix - Test Set:\")\n",
        "print(conf_matrix_df)\n",
        "wandb.log({\"confusion_matrix\": wandb.Table(dataframe=conf_matrix_df)})\n",
        "false_positives = test_data[(test_true_labels == 0) & (test_predictions == 1)].copy()\n",
        "false_negatives = test_data[(test_true_labels == 1) & (test_predictions == 0)].copy()\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "print(false_positives[['Sentence', 'Label']])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "print(false_negatives[['Sentence', 'Label']])\n",
        "test_data['Confidence_Positive'] = confidences[:, 1].cpu().numpy()\n",
        "test_data['Confidence_Negative'] = confidences[:, 0].cpu().numpy()\n",
        "false_positives['Confidence_Positive'] = test_data.loc[false_positives.index, 'Confidence_Positive'].values\n",
        "false_negatives['Confidence_Negative'] = test_data.loc[false_negatives.index, 'Confidence_Negative'].values\n",
        "print(\"\\nFalse Positives with Confidence:\")\n",
        "print(false_positives[['Sentence', 'Label', 'Confidence_Positive']])\n",
        "print(\"\\nFalse Negatives with Confidence:\")\n",
        "print(false_negatives[['Sentence', 'Label', 'Confidence_Negative']])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"NotInsensitive\", \"Insensitive\"],\n",
        "            yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "\n",
        "plt.savefig(\"GPTtest_confusion_matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "Wi6V-6VwY90q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Large"
      ],
      "metadata": {
        "id": "4-t2mAlM_OJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/train_dataOriginal.csv\"\n",
        "val_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/val_dataOriginal.csv\"\n",
        "test_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\"\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv(train_path)\n",
        "val_data = pd.read_csv(val_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "\n",
        "def tokenize_data(data, tokenizer, max_length=109):\n",
        "    return tokenizer(\n",
        "        list(data['Sentence']),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length= max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_labels = list(train_data['Manual_Annotation'])\n",
        "val_labels = list(val_data['Manual_Annotation'])\n",
        "test_labels = list(test_data['Manual_Annotation'])\n",
        "\n",
        "train_encodings = tokenize_data(train_data, tokenizer)\n",
        "val_encodings = tokenize_data(val_data, tokenizer)\n",
        "test_encodings = tokenize_data(test_data, tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_encodings_input_idsV2.npy\", train_encodings['input_ids'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_encodings_attention_maskV2.npy\", train_encodings['attention_mask'].numpy())\n",
        "np.save(\"/content/drive/MyDrive/Honours MiscData(Roshna)/Original_train_labels.npyV2\", np.array(train_labels))\n"
      ],
      "metadata": {
        "id": "VBaFxKGU_Ngr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelLV2Original = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=2)"
      ],
      "metadata": {
        "id": "RJ7FfKvxALDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"./results\")\n",
        "print(f\"Default Learning Rate: {training_args.learning_rate}\")\n"
      ],
      "metadata": {
        "id": "TLfG_c13EceU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datasets for train, validation, and test sets\n",
        "train_datasetOriginalV2 = SentenceDataset(train_encodings, train_labels)\n",
        "val_datasetOriginalV2 = SentenceDataset(val_encodings, val_labels)\n",
        "test_datasetOriginalV2 = SentenceDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "zkHgX1gOGkFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, EvalPrediction\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.finish()\n",
        "wandb.init(project=\"Insensitive Lang Detection\", entity=\"Roshna\", name=\"Bert_Large_Original\")\n",
        "\n",
        "# Load BERT-Large model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\",\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_bert_large\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs_bert_large\",\n",
        "    report_to=[\"wandb\"],  # Log with WandB\n",
        "    run_name=\"Bert_Large_Original\",\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_datasetOriginalV2,\n",
        "    eval_dataset=val_datasetOriginalV2,\n",
        "    compute_metrics=compute_metrics,  # Using compute_metrics function\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Compute training metrics after training\n",
        "train_results = trainer.predict(train_datasetOriginalV2)\n",
        "predictions = train_results.predictions\n",
        "labels = train_results.label_ids\n",
        "\n",
        "# Compute and log metrics using compute_metrics\n",
        "train_metrics = compute_metrics((predictions, labels))\n",
        "wandb.log({f\"train_{key}\": value for key, value in train_metrics.items()})\n",
        "\n",
        "# Print final training metrics\n",
        "print(\"\\nTraining Metrics\")\n",
        "for key, value in train_metrics.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Compute and log evaluation metrics on the test set\n",
        "test_results = trainer.evaluate(test_datasetOriginalV2, metric_key_prefix=\"test\")\n",
        "wandb.log(test_results)\n",
        "\n",
        "# Print test metrics\n",
        "print(\"\\nTest Set Evaluation\")\n",
        "for key, value in test_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "3rIdSC83EsY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logisic Regression"
      ],
      "metadata": {
        "id": "KDBMLZS8cgu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/train_dataOriginal.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/val_dataOriginal.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\")\n",
        "\n",
        "X_train = train_df['Sentence'].tolist()\n",
        "y_train = train_df['Manual_Annotation'].tolist()\n",
        "\n",
        "X_val = val_df['Sentence'].tolist()\n",
        "y_val = val_df['Manual_Annotation'].tolist()\n",
        "\n",
        "X_test = test_df['Sentence'].tolist()\n",
        "y_test = test_df['Manual_Annotation'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "logistic_reg_model = LogisticRegression(max_iter=500, C=1.0)\n",
        "\n",
        "# 5-Fold Cross-Validation**\n",
        "cv_scores = cross_val_score(logistic_reg_model, X_train_tfidf, y_train, cv=5, scoring=\"accuracy\")\n",
        "\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean Cross-Validation Accuracy:\", np.mean(cv_scores))\n"
      ],
      "metadata": {
        "id": "coqUH-oKcsme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/train_dataOriginal.csv\")\n",
        "val_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/val_dataOriginal.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/Honours MiscData(Roshna)/test_dataOriginal.csv\")\n",
        "\n",
        "# Extract text and labels\n",
        "X_train, y_train = train_df['Sentence'], train_df['Manual_Annotation']\n",
        "X_val, y_val = val_df['Sentence'], val_df['Manual_Annotation']\n",
        "X_test, y_test = test_df['Sentence'], test_df['Manual_Annotation']\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression Model\n",
        "logistic_reg_model = LogisticRegression(max_iter=500, C=1.0)\n",
        "logistic_reg_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "def evaluate_model(model, X, y, dataset_name, dataset_df=None, show_conf_matrix=False):\n",
        "    \"\"\"Evaluates the model and prints precision, recall, F1-score, and accuracy.\"\"\"\n",
        "\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    precision = precision_score(y, predictions)\n",
        "    recall = recall_score(y, predictions)\n",
        "    f1 = f1_score(y, predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{dataset_name} Set Evaluation\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix & Misclassified Examples (Only for test set)\n",
        "    if show_conf_matrix and dataset_df is not None:\n",
        "        conf_matrix = confusion_matrix(y, predictions)\n",
        "        print(\"\\nConfusion Matrix - Test Set:\")\n",
        "        print(pd.DataFrame(conf_matrix,\n",
        "                           columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                           index=[\"Actual NotInsensitive\", \"Actual Insensitive\"]))\n",
        "\n",
        "        # Convert Pandas Series to NumPy for proper filtering\n",
        "        y_np = y.values\n",
        "        misclassified_df = dataset_df[y_np != predictions]\n",
        "\n",
        "        print(\"\\nMisclassified Examples:\")\n",
        "        print(misclassified_df[['Sentence', 'Manual_Annotation']])\n",
        "\n",
        "# Evaluate on Train, Validation, and Test sets\n",
        "evaluate_model(logistic_reg_model, X_train_tfidf, y_train, \"Train\")\n",
        "evaluate_model(logistic_reg_model, X_val_tfidf, y_val, \"Validation\")\n",
        "evaluate_model(logistic_reg_model, X_test_tfidf, y_test, \"Test\", dataset_df=test_df, show_conf_matrix=True)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fC2NcR8IYtDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "model_filename = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Orilogistic_regression_model.pkl\"\n",
        "vectorizer_filename = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Oritfidf_vectorizer.pkl\"\n",
        "\n",
        "joblib.dump(logistic_reg_model, model_filename)\n",
        "joblib.dump(vectorizer, vectorizer_filename)"
      ],
      "metadata": {
        "id": "eYOcvrqgdyTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_learning_curve(estimator, X, y, cv=5, scoring=\"neg_log_loss\"):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        "    )\n",
        "\n",
        "    # Convert negative log loss back to positive\n",
        "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_scores_mean, label=\"Training Loss\", marker='o')\n",
        "    plt.fill_between(\n",
        "        train_sizes,\n",
        "        train_scores_mean - train_scores_std,\n",
        "        train_scores_mean + train_scores_std,\n",
        "        alpha=0.1,\n",
        "    )\n",
        "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-Validation Loss\", marker='o')\n",
        "    plt.fill_between(\n",
        "        train_sizes,\n",
        "        test_scores_mean - test_scores_std,\n",
        "        test_scores_mean + test_scores_std,\n",
        "        alpha=0.1,\n",
        "    )\n",
        "    plt.title(\"Learning Curve (Log Loss)\")\n",
        "    plt.xlabel(\"Training Set Size\")\n",
        "    plt.ylabel(\"Log Loss\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.grid()\n",
        "\n",
        "# Use log loss instead of accuracy\n",
        "plot_learning_curve(\n",
        "    logistic_reg_model,\n",
        "    X_train_tfidf,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring=\"neg_log_loss\"\n",
        ")\n",
        "plt.savefig(\"Logistic_learning_curve_log_loss.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UoX00JURd97s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# File Paths\n",
        "model_filename = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Orilogistic_regression_model.pkl\"\n",
        "vectorizer_filename = \"/content/drive/MyDrive/Honours MiscData(Roshna)/Oritfidf_vectorizer.pkl\"\n",
        "new_test_data_path = \"/content/drive/MyDrive/Honours MiscData(Roshna)/AugTest_dataV2.csv\"\n",
        "\n",
        "# Load Saved Model & Vectorizer\n",
        "logistic_reg_model = joblib.load(model_filename)\n",
        "vectorizer = joblib.load(vectorizer_filename)\n",
        "\n",
        "# Load New Test Dataset\n",
        "new_test_df = pd.read_csv(new_test_data_path)\n",
        "\n",
        "# Extract Sentences and Labels\n",
        "X_new_test, y_new_test = new_test_df['Sentence'], new_test_df['Label']\n",
        "\n",
        "# Transform Using the Same TF-IDF Vectorizer\n",
        "X_new_test_tfidf = vectorizer.transform(X_new_test)\n",
        "\n",
        "# Function to Evaluate Model & Print Correct Metrics\n",
        "def evaluate_model(model, X, y, dataset_name, dataset_df=None, show_conf_matrix=False):\n",
        "    \"\"\"Evaluates the model and prints precision, recall, F1-score, and accuracy.\"\"\"\n",
        "\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Compute Metrics\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    precision = precision_score(y, predictions)\n",
        "    recall = recall_score(y, predictions)\n",
        "    f1 = f1_score(y, predictions)\n",
        "\n",
        "    # Print Results\n",
        "    print(f\"\\n{dataset_name} Set Evaluation\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix & Misclassified Examples\n",
        "    if show_conf_matrix and dataset_df is not None:\n",
        "        conf_matrix = confusion_matrix(y, predictions)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(pd.DataFrame(conf_matrix,\n",
        "                           columns=[\"Predicted NotInsensitive\", \"Predicted Insensitive\"],\n",
        "                           index=[\"Actual NotInsensitive\", \"Actual Insensitive\"]))\n",
        "\n",
        "        # Get Misclassified Examples\n",
        "        misclassified_df = dataset_df[y != predictions]\n",
        "\n",
        "        print(\"\\nMisclassified Examples:\")\n",
        "        print(misclassified_df[['Sentence', 'Label']])\n",
        "\n",
        "# Evaluate the New Test Set\n",
        "evaluate_model(logistic_reg_model, X_new_test_tfidf, y_new_test, \"New Test\", dataset_df=new_test_df, show_conf_matrix=True)\n"
      ],
      "metadata": {
        "id": "Vqw7-bZAn2rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the confusion matrix\n",
        "conf_matrix = np.array([[105, 3],\n",
        "                        [0, 107]])\n",
        "\n",
        "# Define class labels\n",
        "labels = [\"Not Insensitive\", \"Insensitive\"]\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(4, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"NotInsensitive\", \"Insensitive\"], yticklabels=[\"NotInsensitive\", \"Insensitive\"])\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.savefig(\"Augmatrix.png\")\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aWIVkVjTA3J2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}